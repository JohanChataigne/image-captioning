{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.6.0\n",
      "No GPU :(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as m\n",
    "from torch.utils.data import DataLoader\n",
    "from transforms import *\n",
    "from torchvision.transforms import Compose\n",
    "from torchsummary import summary\n",
    "from random_caption_dataset import RandomCaptionDataset\n",
    "from text_preprocessing import *\n",
    "from inference import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(\"GPU found :)\" if torch.cuda.is_available() else \"No GPU :(\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "EMBEDDING_SIZE = 512\n",
    "CONTEXT_SIZE = 4\n",
    "train_annotations_file = './flickr8k/annotations/annotations_image_id_train.csv'\n",
    "test_annotations_file = './flickr8k/annotations/annotations_image_id_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datas section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init text preprocessing class\n",
    "tp = TextPreprocessor(train_annotations_file, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random dataset size: 6000\n"
     ]
    }
   ],
   "source": [
    "transforms = Compose([Rescale(256), \n",
    "                      RandomCrop(IMAGE_SIZE), \n",
    "                      ToTensor(), \n",
    "                      Normalize(),\n",
    "                      AddDelimiters(),\n",
    "                      PadSentence(tp),\n",
    "                      OneHotEncode(tp)\n",
    "                     ])\n",
    "\n",
    "train_random_dataset = RandomCaptionDataset('./flickr8k/images/train/', train_annotations_file, transform=transforms)\n",
    "\n",
    "print(f'Random dataset size: {len(train_random_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "40\n",
      "(40, 8255)\n"
     ]
    }
   ],
   "source": [
    "caption = train_random_dataset[100]['caption']\n",
    "print(caption)\n",
    "print(tp.max_len)\n",
    "print(caption.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# Build data loader\n",
    "train_random_loader = DataLoader(train_random_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve pretrained model for features extraction\n",
    "base_cnn = m.resnet18(pretrained=True)\n",
    "#base_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the feature extraction layers of the model\n",
    "cnn = nn.Sequential(*(list(base_cnn.children())[:-1])).to(device, dtype=torch.float)\n",
    "#summary(cnn, (3, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tp.vocab_size\n",
    "print(vocab_size)\n",
    "\n",
    "# RNN with LSTM of  layer\n",
    "class LSTMCaptioning(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMCaptioning, self).__init__()\n",
    "        \n",
    "        # Random init the lstm state\n",
    "        self.h0 = torch.zeros((num_layers, batch_size, hidden_size)).to(device)\n",
    "        self.c0 = torch.zeros((num_layers, batch_size, hidden_size)).to(device)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, dropout=0.5)\n",
    "        self.hidden2out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, previous_state=None):\n",
    "         \n",
    "        if previous_state is None:\n",
    "            previous_state = (self.h0, self.c0)\n",
    "            \n",
    "        # Get hidden states for each t (out) , and latest one (h = (ht, ct))\n",
    "        lstm_out, (hn, cn) = self.lstm(x, previous_state)\n",
    "        \n",
    "        # Convert output of rnn to output targeted size\n",
    "        out = self.hidden2out(lstm_out.view(1, -1))\n",
    "        \n",
    "        # Compute probability distribution over all words for this t\n",
    "        pt = F.log_softmax(out, dim=1)\n",
    "                           \n",
    "        return (hn, cn), pt\n",
    "\n",
    "# Need to copy class here to load trained Ngram model    \n",
    "class NGram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGram, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embeds = self.embeddings(inputs).view(len(inputs), -1)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return log_probs\n",
    "\n",
    "\n",
    "# Not trained embedding layer to encode words to hidden space\n",
    "#embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE).to(device)\n",
    "\n",
    "# Load model for evaluation\n",
    "ngram_model = NGram(vocab_size, EMBEDDING_SIZE, CONTEXT_SIZE)\n",
    "ngram_model.load_state_dict(torch.load('./models/ngram_512_v1'))\n",
    "\n",
    "embedding = list(ngram_model.children())[0].to(device)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = EMBEDDING_SIZE\n",
    "hidden_size = 512\n",
    "num_layers = 3\n",
    "\n",
    "model = LSTMCaptioning(input_size, hidden_size, vocab_size, num_layers).to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epoch = 5\n",
    "step_count = len(train_random_loader)\n",
    "loss_function = nn.NLLLoss()\n",
    "losses = list()\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, sample in enumerate(train_random_loader):\n",
    "        \n",
    "        image = sample['image'].to(device, dtype=torch.float)\n",
    "        caption = sample['caption'].to(device, dtype=torch.long)\n",
    "        \n",
    "        # Reset grad\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Get the input image embedding \n",
    "        image_embedding = cnn(image).view(-1, batch_size, EMBEDDING_SIZE)\n",
    "        \n",
    "        \n",
    "        # Forward pass for t=-1: image\n",
    "        (hn, cn), probs = model(image_embedding)\n",
    "        \n",
    "        del image_embedding\n",
    "        del image\n",
    "        \n",
    "        target = tp.target_from_vect(caption[:, 0]).to(device)\n",
    "        \n",
    "        # Compute loss for 1st word prediction\n",
    "        loss = loss_function(probs, target)\n",
    "\n",
    "        \n",
    "        # Forward pass for t>=0: n - 1 first words of the sentence\n",
    "        for j, word in enumerate(caption[:, :-1][0]):\n",
    "\n",
    "            # Get index of the word in embedding matrix\n",
    "            idxs = torch.argmax(word)\n",
    "            \n",
    "            # Encode word to hidden space\n",
    "            word_embedding = embedding(idxs).view(1, batch_size, EMBEDDING_SIZE)\n",
    "            \n",
    "            # Feed the rnn\n",
    "            (hn, cn), probs = model(word_embedding, (hn, cn))\n",
    "            \n",
    "            target = tp.target_from_vect(caption[:, j+1]).to(device)\n",
    "            \n",
    "            # Add current word's loss\n",
    "            loss += loss_function(probs, target)\n",
    "            \n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Compute loss and backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "          \n",
    "        # Debug\n",
    "        if((i+1) % int(step_count/20) == 0):\n",
    "            print(\n",
    "                        f\"Epoch [{epoch + 1}/{num_epoch}]\"\n",
    "                        f\", step [{i + 1}/{step_count}]\"\n",
    "                        f\", loss: {loss.item():.4f}\"\n",
    "                    )\n",
    "            \n",
    "    losses.append(epoch_loss)\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            }, './models/model_random_init0_lstm3_checkpoint')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model \n",
    "#torch.save(model.state_dict(), './models/model_random_init0_lstm3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "epochs = list(range(1, num_epoch+1))\n",
    "mean_losses = list(map(lambda x: x/6000, losses))\n",
    "plt.plot(epochs, mean_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average loss')\n",
    "plt.grid()\n",
    "plt.savefig('Loss model random v2 training with lr=0.1.png')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for evaluation\n",
    "trained_model = LSTMCaptioning(input_size, hidden_size, vocab_size, num_layers)\n",
    "trained_model.load_state_dict(torch.load('./models/model_random_init0_lstm3'))\n",
    "trained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test loaders for datasets\n",
    "\n",
    "# Only preprocess images\n",
    "test_transforms = Compose([Rescale(256), \n",
    "                      RandomCrop(IMAGE_SIZE), \n",
    "                      ToTensor(), \n",
    "                      Normalize()])\n",
    "\n",
    "test_random_dataset_unformatted = RandomCaptionDataset('./flickr8k/images/test', test_annotations_file)\n",
    "train_random_dataset_unformatted = RandomCaptionDataset('./flickr8k/images/train', train_annotations_file)\n",
    "\n",
    "\n",
    "test_random_dataset = RandomCaptionDataset('./flickr8k/images/test', test_annotations_file, transform=test_transforms)\n",
    "\n",
    "test_random_loader = DataLoader(test_random_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = test_random_dataset[300]['image'].view(1, 3, IMAGE_SIZE, IMAGE_SIZE).to(device, dtype=torch.float)\n",
    "caption = test_random_dataset[300]['caption']\n",
    "captions = beam_search(cnn, embedding, trained_model, image, tp, EMBEDDING_SIZE, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "image = test_random_dataset_unformatted[100]['image']\n",
    "io.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "target = caption\n",
    "pred = captions[10]\n",
    "\n",
    "print(caption)\n",
    "print(pred)\n",
    "bleu = sacrebleu.sentence_bleu(pred, [target], smooth_method='exp')\n",
    "print(bleu.score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
