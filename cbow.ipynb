{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n",
      "GPU found :)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(\"GPU found :)\" if torch.cuda.is_available() else \"No GPU :(\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40460 entries, 0 to 40459\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   image_id  40460 non-null  object\n",
      " 1   caption   40460 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 632.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_vocab = pd.read_csv('./flickr8k/annotations/annotations_image_id.csv', sep=';')\n",
    "df_vocab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = list(df_vocab.iloc[:, 1])\n",
    "\n",
    "raw_text = raw_sentences[0]\n",
    "# Build raw_text\n",
    "for i in range(1, len(raw_sentences)):\n",
    "    raw_text += ' ' + raw_sentences[i]\n",
    "\n",
    "raw_text += ' <start> <stop>'\n",
    "    \n",
    "#print(raw_text)\n",
    "raw_text = raw_text.split()\n",
    "\n",
    "# Get vocabulary\n",
    "#vocab = set(raw_text)\n",
    "#vocab_size = len(vocab)\n",
    "#print(vocab_size)\n",
    "vocab = np.array(raw_text)\n",
    "vocab = np.unique(vocab) # start is at index 67 and stop is at index 68\n",
    "ohe = np.identity(vocab.shape[0])\n",
    "\n",
    "def same_word(word1,word2):\n",
    "    bool_arr = (word1 == word2)\n",
    "    for b in bool_arr:\n",
    "        if not b:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def word_to_vect(word):\n",
    "    word_ind = np.searchsorted(vocab, word)\n",
    "    return ohe[word_ind]\n",
    "\n",
    "def caption_to_vect(caption):\n",
    "    '''\n",
    "    Parameters : \n",
    "        caption : a string of a caption, starting with <start> and ending with <stop>\n",
    "    Output :\n",
    "        a vector of shape (9631,nb_of_words) representing the caption\n",
    "    '''\n",
    "    c_list = caption.split()\n",
    "    c_list = np.array(c_list)\n",
    "    c_vect = np.zeros((len(c_list),vocab.shape[0]))\n",
    "    for k in range(len(c_list)):\n",
    "        c_vect[k] = np.array(word_to_vect(c_list[k]))\n",
    "    return c_vect\n",
    "\n",
    "def vect_to_caption(vect):\n",
    "    '''\n",
    "    Parameters : \n",
    "        vect : a np array of shape (9631,nb_of_words) that represents a caption starting with <start> ending with <stop>\n",
    "    Output :\n",
    "        a string caption\n",
    "    '''\n",
    "    caption = \"\"\n",
    "    started = same_word(vect[0],word_to_vect('<start>'))\n",
    "    if not started:\n",
    "        raise ValueError\n",
    "    for k in range(1,vect.shape[0]-1):\n",
    "        wordx = np.argmax(vect[k])\n",
    "        caption += vocab[wordx] + ' '\n",
    "    stopped = same_word(vect[-1],word_to_vect('<stop>'))\n",
    "    if not stopped:\n",
    "        raise ValueError\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['A', 'child', 'a', 'pink'], 'in'), (['child', 'in', 'pink', 'dress'], 'a'), (['in', 'a', 'dress', 'is'], 'pink')]\n"
     ]
    }
   ],
   "source": [
    "# Size of the context of one word, i.e. words on the left and words on the right we keep as context\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Map each word to an index\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Build the data to train the model\n",
    "data = []\n",
    "\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    idx = list(range(i - CONTEXT_SIZE, i)) + list(range(i + 1, i + CONTEXT_SIZE + 1))\n",
    "    context = [raw_text[k] for k in idx]\n",
    "    target = raw_text[i]\n",
    "    \n",
    "    data.append((context, target))\n",
    "\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, context_size, embedding_dim, vocab_size):\n",
    "        \n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * 2 * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-246ae172d818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstep_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTEXT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "num_epoch = 5\n",
    "step_count = len(data)\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(CONTEXT_SIZE, EMBEDDING_DIM, vocab_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for i, sample in enumerate(data):\n",
    "\n",
    "        context, target = sample\n",
    "        # Prepare the inputs to be passed to the model\n",
    "        context_idxs = make_context_vector(context, word_to_ix)\n",
    "\n",
    "        # Reset grad\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Run forward and get log probabilities over the word that matches the context\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long).to(device))\n",
    "\n",
    "        # Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if((i+1) % 1000 == 0):\n",
    "            print(\n",
    "                        f\"Epoch [{epoch + 1}/{num_epoch}]\"\n",
    "                        f\", step [{i + 1}/{step_count}]\"\n",
    "                        f\", loss: {loss.item():.4f}\"\n",
    "                        f\", total loss: {total_loss:.4f}\"\n",
    "                    )\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model \n",
    "torch.save(model.state_dict(), './models/word2vec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model for test\n",
    "model = CBOW(CONTEXT_SIZE, EMBEDDING_DIM, vocab_size).to(device)\n",
    "model.load_state_dict(torch.load('./models/word2vec.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
