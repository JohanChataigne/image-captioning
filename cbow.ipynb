{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.0\n",
      "No GPU :(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(\"GPU found :)\" if torch.cuda.is_available() else \"No GPU :(\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40460 entries, 0 to 40459\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   image_id  40460 non-null  object\n",
      " 1   caption   40460 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 632.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_vocab = pd.read_csv('./flickr8k/annotations/annotations_image_id.csv', sep=';')\n",
    "df_vocab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9631\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = list(df_vocab.iloc[:, 1])\n",
    "\n",
    "raw_text = raw_sentences[0]\n",
    "# Build raw_text\n",
    "for i in range(1, len(raw_sentences)):\n",
    "    raw_text += ' ' + raw_sentences[i]\n",
    "\n",
    "raw_text += ' <start> <stop>'\n",
    "    \n",
    "#print(raw_text)\n",
    "raw_text = raw_text.split()\n",
    "\n",
    "# Get vocabulary\n",
    "#vocab = set(raw_text)\n",
    "#vocab_size = len(vocab)\n",
    "#print(vocab_size)\n",
    "vocab = np.array(raw_text)\n",
    "vocab = np.unique(vocab) # start is at index 67 and stop is at index 68\n",
    "print(vocab.shape[0])\n",
    "ohe = np.identity(vocab.shape[0])\n",
    "\n",
    "def same_word(word1,word2):\n",
    "    bool_arr = (word1 == word2)\n",
    "    for b in bool_arr:\n",
    "        if not b:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def word_to_vect(word):\n",
    "    word_ind = np.searchsorted(vocab, word)\n",
    "    return ohe[word_ind]\n",
    "\n",
    "def caption_to_vect(caption):\n",
    "    '''\n",
    "    Parameters : \n",
    "        caption : a string of a caption, starting with <start> and ending with <stop>\n",
    "    Output :\n",
    "        a vector of shape (9631,nb_of_words) representing the caption\n",
    "    '''\n",
    "    c_list = caption.split()\n",
    "    c_list = np.array(c_list)\n",
    "    c_vect = np.zeros((len(c_list),vocab.shape[0]))\n",
    "    for k in range(len(c_list)):\n",
    "        print(word_to_vect(c_list[k]))\n",
    "        c_vect[k] = np.array(word_to_vect(c_list[k]))\n",
    "    return c_vect\n",
    "\n",
    "def vect_to_caption(vect):\n",
    "    '''\n",
    "    Parameters : \n",
    "        vect : a np array of shape (9631,nb_of_words) that represents a caption starting with <start> ending with <stop>\n",
    "    Output :\n",
    "        a string caption\n",
    "    '''\n",
    "    caption = \"\"\n",
    "    started = same_word(vect[0],word_to_vect('<start>'))\n",
    "    if not started:\n",
    "        raise ValueError\n",
    "    for k in range(1,vect.shape[0]-1):\n",
    "        wordx = np.argmax(vect[k])\n",
    "        caption += vocab[wordx] + ' '\n",
    "    stopped = same_word(vect[-1],word_to_vect('<stop>'))\n",
    "    if not stopped:\n",
    "        raise ValueError\n",
    "    return caption\n",
    "        \n",
    "#wordtest = np.zeros((6,vocab.shape[0]))\n",
    "#wordtest[0][67] = 1#<start>\n",
    "#wordtest[1][310] = 1\n",
    "#wordtest[2][4857] = 1\n",
    "#wordtest[3][240] = 1\n",
    "#wordtest[4][4687] = 1\n",
    "#wordtest[5][68] = 1 #<stop>\n",
    "#print(vect_to_caption(wordtest))\n",
    "#print(word_to_vect('Canada'))\n",
    "#vecttest = '<start> Crowd hurdle Canada herding <stop>'\n",
    "#print(np.where((wordtest == caption_to_vect(vecttest)) == False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['A', 'child', 'a', 'pink'], 'in'), (['child', 'in', 'pink', 'dress'], 'a'), (['in', 'a', 'dress', 'is'], 'pink')]\n",
      "[(0, (['A', 'child', 'a', 'pink'], 'in')), (1, (['child', 'in', 'pink', 'dress'], 'a')), (2, (['in', 'a', 'dress', 'is'], 'pink'))]\n"
     ]
    }
   ],
   "source": [
    "# Size of the context of one word, i.e. words on the left and words on the right we keep as context\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Map each word to an index\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Build the data to train the model\n",
    "data = []\n",
    "\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    idx = list(range(i - CONTEXT_SIZE, i)) + list(range(i + 1, i + CONTEXT_SIZE + 1))\n",
    "    context = [raw_text[k] for k in idx]\n",
    "    target = raw_text[i]\n",
    "    \n",
    "    data.append((context, target))\n",
    "\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, context_size, embedding_dim, vocab_size):\n",
    "        \n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * 2 * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], step [1000/476702], loss: 8.9400, total loss: 8816.3221\n",
      "Epoch [1/5], step [2000/476702], loss: 0.5576, total loss: 16779.3860\n",
      "Epoch [1/5], step [3000/476702], loss: 4.9743, total loss: 23815.3299\n",
      "Epoch [1/5], step [4000/476702], loss: 2.8110, total loss: 30178.1256\n",
      "Epoch [1/5], step [5000/476702], loss: 9.2805, total loss: 36445.2720\n",
      "Epoch [1/5], step [6000/476702], loss: 7.0085, total loss: 42226.7163\n",
      "Epoch [1/5], step [7000/476702], loss: 0.4405, total loss: 47824.1943\n",
      "Epoch [1/5], step [8000/476702], loss: 6.1613, total loss: 53576.7170\n",
      "Epoch [1/5], step [9000/476702], loss: 4.5952, total loss: 59313.2254\n",
      "Epoch [1/5], step [10000/476702], loss: 8.4296, total loss: 64603.4270\n",
      "Epoch [1/5], step [11000/476702], loss: 2.1138, total loss: 69429.6583\n",
      "Epoch [1/5], step [12000/476702], loss: 0.0878, total loss: 74385.3465\n",
      "Epoch [1/5], step [13000/476702], loss: 1.9168, total loss: 79567.2594\n",
      "Epoch [1/5], step [14000/476702], loss: 1.3770, total loss: 84586.6596\n",
      "Epoch [1/5], step [15000/476702], loss: 0.1220, total loss: 89253.4267\n",
      "Epoch [1/5], step [16000/476702], loss: 8.5839, total loss: 93960.3048\n",
      "Epoch [1/5], step [17000/476702], loss: 4.5868, total loss: 98939.6976\n",
      "Epoch [1/5], step [18000/476702], loss: 1.2932, total loss: 103706.7240\n",
      "Epoch [1/5], step [19000/476702], loss: 7.0057, total loss: 108594.0926\n",
      "Epoch [1/5], step [20000/476702], loss: 10.6065, total loss: 113170.6185\n",
      "Epoch [1/5], step [21000/476702], loss: 1.1730, total loss: 117555.9027\n",
      "Epoch [1/5], step [22000/476702], loss: 4.8045, total loss: 121997.6562\n",
      "Epoch [1/5], step [23000/476702], loss: 3.3913, total loss: 126656.8622\n",
      "Epoch [1/5], step [24000/476702], loss: 10.1924, total loss: 131054.8051\n",
      "Epoch [1/5], step [25000/476702], loss: 10.7379, total loss: 135596.6984\n",
      "Epoch [1/5], step [26000/476702], loss: 7.4113, total loss: 140218.6890\n",
      "Epoch [1/5], step [27000/476702], loss: 6.3574, total loss: 144405.7912\n",
      "Epoch [1/5], step [28000/476702], loss: 9.0671, total loss: 148721.0014\n",
      "Epoch [1/5], step [29000/476702], loss: 6.1019, total loss: 152902.4293\n",
      "Epoch [1/5], step [30000/476702], loss: 1.3706, total loss: 157410.8244\n",
      "Epoch [1/5], step [31000/476702], loss: 1.6223, total loss: 161702.3074\n",
      "Epoch [1/5], step [32000/476702], loss: 11.6961, total loss: 165853.7905\n",
      "Epoch [1/5], step [33000/476702], loss: 2.6635, total loss: 170419.9590\n",
      "Epoch [1/5], step [34000/476702], loss: 11.8686, total loss: 174492.2512\n",
      "Epoch [1/5], step [35000/476702], loss: 2.6606, total loss: 178558.5470\n",
      "Epoch [1/5], step [36000/476702], loss: 2.1415, total loss: 182850.0354\n",
      "Epoch [1/5], step [37000/476702], loss: 2.5542, total loss: 187009.2321\n",
      "Epoch [1/5], step [38000/476702], loss: 1.1106, total loss: 191096.4338\n",
      "Epoch [1/5], step [39000/476702], loss: 0.7311, total loss: 195228.0221\n",
      "Epoch [1/5], step [40000/476702], loss: 3.5342, total loss: 199102.8051\n",
      "Epoch [1/5], step [41000/476702], loss: 0.8277, total loss: 202722.4092\n",
      "Epoch [1/5], step [42000/476702], loss: 3.8053, total loss: 206568.5066\n",
      "Epoch [1/5], step [43000/476702], loss: 5.2772, total loss: 210599.3972\n",
      "Epoch [1/5], step [44000/476702], loss: 0.5589, total loss: 214822.3572\n",
      "Epoch [1/5], step [45000/476702], loss: 0.2703, total loss: 219271.2777\n",
      "Epoch [1/5], step [46000/476702], loss: 4.9967, total loss: 223181.6808\n",
      "Epoch [1/5], step [47000/476702], loss: 2.6601, total loss: 226811.8617\n",
      "Epoch [1/5], step [48000/476702], loss: 3.4240, total loss: 230579.4167\n",
      "Epoch [1/5], step [49000/476702], loss: 0.8170, total loss: 235071.7645\n",
      "Epoch [1/5], step [50000/476702], loss: 0.4941, total loss: 239542.5143\n",
      "Epoch [1/5], step [51000/476702], loss: 1.0194, total loss: 243856.5656\n",
      "Epoch [1/5], step [52000/476702], loss: 0.1363, total loss: 247642.2368\n",
      "Epoch [1/5], step [53000/476702], loss: 1.7120, total loss: 251605.3637\n",
      "Epoch [1/5], step [54000/476702], loss: 1.9934, total loss: 255310.4523\n",
      "Epoch [1/5], step [55000/476702], loss: 2.8169, total loss: 259106.3596\n",
      "Epoch [1/5], step [56000/476702], loss: 2.7673, total loss: 263311.6763\n",
      "Epoch [1/5], step [57000/476702], loss: 0.1604, total loss: 266834.6667\n",
      "Epoch [1/5], step [58000/476702], loss: 0.0315, total loss: 271043.1692\n",
      "Epoch [1/5], step [59000/476702], loss: 0.0808, total loss: 274886.1549\n",
      "Epoch [1/5], step [60000/476702], loss: 5.2368, total loss: 278665.1892\n",
      "Epoch [1/5], step [61000/476702], loss: 0.7970, total loss: 282290.0319\n",
      "Epoch [1/5], step [62000/476702], loss: 0.6425, total loss: 285807.1541\n",
      "Epoch [1/5], step [63000/476702], loss: 0.4176, total loss: 289553.0762\n",
      "Epoch [1/5], step [64000/476702], loss: 0.1883, total loss: 293361.7015\n",
      "Epoch [1/5], step [65000/476702], loss: 0.5941, total loss: 297153.1350\n",
      "Epoch [1/5], step [66000/476702], loss: 0.2164, total loss: 300537.6863\n",
      "Epoch [1/5], step [67000/476702], loss: 3.4237, total loss: 304241.6901\n",
      "Epoch [1/5], step [68000/476702], loss: 0.8824, total loss: 307803.7775\n",
      "Epoch [1/5], step [69000/476702], loss: 2.1274, total loss: 311863.2235\n",
      "Epoch [1/5], step [70000/476702], loss: 9.9939, total loss: 315739.9347\n",
      "Epoch [1/5], step [71000/476702], loss: 15.2832, total loss: 319658.5453\n",
      "Epoch [1/5], step [72000/476702], loss: 3.2754, total loss: 323420.5149\n",
      "Epoch [1/5], step [73000/476702], loss: 1.8295, total loss: 327302.2191\n",
      "Epoch [1/5], step [74000/476702], loss: 0.0784, total loss: 331040.0834\n",
      "Epoch [1/5], step [75000/476702], loss: 0.1139, total loss: 334403.4010\n",
      "Epoch [1/5], step [76000/476702], loss: 0.9832, total loss: 338011.6751\n",
      "Epoch [1/5], step [77000/476702], loss: 0.1048, total loss: 341995.2140\n",
      "Epoch [1/5], step [78000/476702], loss: 5.1389, total loss: 345574.9546\n",
      "Epoch [1/5], step [79000/476702], loss: 11.6949, total loss: 349387.3314\n",
      "Epoch [1/5], step [80000/476702], loss: 9.7000, total loss: 353105.4335\n",
      "Epoch [1/5], step [81000/476702], loss: 1.6325, total loss: 356826.7749\n",
      "Epoch [1/5], step [82000/476702], loss: 0.2004, total loss: 360355.1512\n",
      "Epoch [1/5], step [83000/476702], loss: 3.2109, total loss: 364042.3308\n",
      "Epoch [1/5], step [84000/476702], loss: 3.2593, total loss: 367675.7780\n",
      "Epoch [1/5], step [85000/476702], loss: 3.1569, total loss: 371167.9354\n",
      "Epoch [1/5], step [86000/476702], loss: 0.0191, total loss: 374769.0843\n",
      "Epoch [1/5], step [87000/476702], loss: 8.1266, total loss: 378609.7590\n",
      "Epoch [1/5], step [88000/476702], loss: 2.4562, total loss: 381823.4588\n",
      "Epoch [1/5], step [89000/476702], loss: 1.6075, total loss: 384706.5655\n",
      "Epoch [1/5], step [90000/476702], loss: 6.9670, total loss: 388193.2334\n",
      "Epoch [1/5], step [91000/476702], loss: 8.9456, total loss: 391490.5940\n",
      "Epoch [1/5], step [92000/476702], loss: 5.8636, total loss: 394840.4600\n",
      "Epoch [1/5], step [93000/476702], loss: 0.0464, total loss: 398155.5428\n",
      "Epoch [1/5], step [94000/476702], loss: 6.9481, total loss: 401581.0871\n",
      "Epoch [1/5], step [95000/476702], loss: 7.1572, total loss: 405226.7294\n",
      "Epoch [1/5], step [96000/476702], loss: 4.1381, total loss: 408584.6997\n",
      "Epoch [1/5], step [97000/476702], loss: 7.4796, total loss: 412333.6962\n",
      "Epoch [1/5], step [98000/476702], loss: 3.6541, total loss: 415846.7584\n",
      "Epoch [1/5], step [99000/476702], loss: 1.6349, total loss: 419105.7904\n",
      "Epoch [1/5], step [100000/476702], loss: 3.5137, total loss: 422686.3703\n",
      "Epoch [1/5], step [101000/476702], loss: 0.7294, total loss: 425983.4567\n",
      "Epoch [1/5], step [102000/476702], loss: 3.5570, total loss: 429570.9189\n",
      "Epoch [1/5], step [103000/476702], loss: 3.3835, total loss: 433079.7465\n",
      "Epoch [1/5], step [104000/476702], loss: 4.3174, total loss: 436423.3886\n",
      "Epoch [1/5], step [105000/476702], loss: 1.8920, total loss: 440287.9824\n",
      "Epoch [1/5], step [106000/476702], loss: 1.9332, total loss: 444496.4742\n",
      "Epoch [1/5], step [107000/476702], loss: 9.8505, total loss: 448163.7688\n",
      "Epoch [1/5], step [108000/476702], loss: 0.0878, total loss: 451578.0697\n",
      "Epoch [1/5], step [109000/476702], loss: 2.5522, total loss: 454713.4612\n",
      "Epoch [1/5], step [110000/476702], loss: 1.8077, total loss: 458047.2792\n",
      "Epoch [1/5], step [111000/476702], loss: 0.0982, total loss: 461313.5024\n",
      "Epoch [1/5], step [112000/476702], loss: 5.1482, total loss: 464684.5792\n",
      "Epoch [1/5], step [113000/476702], loss: 0.7137, total loss: 468033.1100\n",
      "Epoch [1/5], step [114000/476702], loss: 2.0528, total loss: 471308.4490\n",
      "Epoch [1/5], step [115000/476702], loss: 2.3348, total loss: 474898.9089\n",
      "Epoch [1/5], step [116000/476702], loss: 1.9632, total loss: 478534.7320\n",
      "Epoch [1/5], step [117000/476702], loss: 1.5972, total loss: 481812.7449\n",
      "Epoch [1/5], step [118000/476702], loss: 0.3066, total loss: 485028.4877\n",
      "Epoch [1/5], step [119000/476702], loss: 2.7369, total loss: 488325.2945\n",
      "Epoch [1/5], step [120000/476702], loss: 0.0008, total loss: 492284.8162\n",
      "Epoch [1/5], step [121000/476702], loss: 10.7001, total loss: 495394.9684\n",
      "Epoch [1/5], step [122000/476702], loss: 0.4108, total loss: 498720.6605\n",
      "Epoch [1/5], step [123000/476702], loss: 10.7628, total loss: 502280.1812\n",
      "Epoch [1/5], step [124000/476702], loss: 7.3195, total loss: 505785.7733\n",
      "Epoch [1/5], step [125000/476702], loss: 3.0492, total loss: 508909.4876\n",
      "Epoch [1/5], step [126000/476702], loss: 4.4792, total loss: 511940.3678\n",
      "Epoch [1/5], step [127000/476702], loss: 0.0176, total loss: 515395.5628\n",
      "Epoch [1/5], step [128000/476702], loss: 3.8434, total loss: 519244.3390\n",
      "Epoch [1/5], step [129000/476702], loss: 12.2658, total loss: 522752.5405\n",
      "Epoch [1/5], step [130000/476702], loss: 3.5923, total loss: 526209.2121\n",
      "Epoch [1/5], step [131000/476702], loss: 0.8626, total loss: 529705.1879\n",
      "Epoch [1/5], step [132000/476702], loss: 0.6039, total loss: 532908.2678\n",
      "Epoch [1/5], step [133000/476702], loss: 4.6297, total loss: 535943.4092\n",
      "Epoch [1/5], step [134000/476702], loss: 1.0462, total loss: 539346.2075\n",
      "Epoch [1/5], step [135000/476702], loss: 1.3580, total loss: 542659.4391\n",
      "Epoch [1/5], step [136000/476702], loss: 0.2823, total loss: 545595.4298\n",
      "Epoch [1/5], step [137000/476702], loss: 11.8635, total loss: 548945.3195\n",
      "Epoch [1/5], step [138000/476702], loss: 9.8903, total loss: 552235.8491\n",
      "Epoch [1/5], step [139000/476702], loss: 2.1796, total loss: 555352.3245\n",
      "Epoch [1/5], step [140000/476702], loss: 4.8216, total loss: 558613.7245\n",
      "Epoch [1/5], step [141000/476702], loss: 4.3181, total loss: 561619.6038\n",
      "Epoch [1/5], step [142000/476702], loss: 5.8577, total loss: 564775.2189\n",
      "Epoch [1/5], step [143000/476702], loss: 0.3208, total loss: 568259.7541\n",
      "Epoch [1/5], step [144000/476702], loss: 1.9186, total loss: 571684.4752\n",
      "Epoch [1/5], step [145000/476702], loss: 0.5691, total loss: 575094.9370\n",
      "Epoch [1/5], step [146000/476702], loss: 0.2468, total loss: 578448.9951\n",
      "Epoch [1/5], step [147000/476702], loss: 3.0414, total loss: 581702.5442\n",
      "Epoch [1/5], step [148000/476702], loss: 2.9564, total loss: 585276.4498\n",
      "Epoch [1/5], step [149000/476702], loss: 0.1405, total loss: 588394.9249\n",
      "Epoch [1/5], step [150000/476702], loss: 1.8083, total loss: 591433.4187\n",
      "Epoch [1/5], step [151000/476702], loss: 5.6356, total loss: 595039.1734\n",
      "Epoch [1/5], step [152000/476702], loss: 1.5295, total loss: 598627.8830\n",
      "Epoch [1/5], step [153000/476702], loss: 0.0248, total loss: 602227.4976\n",
      "Epoch [1/5], step [154000/476702], loss: 6.2382, total loss: 605775.0429\n",
      "Epoch [1/5], step [155000/476702], loss: 2.2627, total loss: 609194.1222\n",
      "Epoch [1/5], step [156000/476702], loss: 0.6351, total loss: 612289.2363\n",
      "Epoch [1/5], step [157000/476702], loss: 1.5926, total loss: 615704.9166\n",
      "Epoch [1/5], step [158000/476702], loss: 1.2915, total loss: 619343.4863\n",
      "Epoch [1/5], step [159000/476702], loss: 1.1734, total loss: 622926.7518\n",
      "Epoch [1/5], step [160000/476702], loss: 2.1798, total loss: 626425.5380\n",
      "Epoch [1/5], step [161000/476702], loss: 0.0198, total loss: 629768.6703\n",
      "Epoch [1/5], step [162000/476702], loss: 0.0159, total loss: 633070.5685\n",
      "Epoch [1/5], step [163000/476702], loss: 12.3395, total loss: 636629.2102\n",
      "Epoch [1/5], step [164000/476702], loss: 0.2132, total loss: 640265.1247\n",
      "Epoch [1/5], step [165000/476702], loss: 4.4971, total loss: 643491.7858\n",
      "Epoch [1/5], step [166000/476702], loss: 0.8733, total loss: 646471.8457\n",
      "Epoch [1/5], step [167000/476702], loss: 0.0263, total loss: 649960.9047\n",
      "Epoch [1/5], step [168000/476702], loss: 6.0919, total loss: 653432.3518\n",
      "Epoch [1/5], step [169000/476702], loss: 2.3124, total loss: 657022.8125\n",
      "Epoch [1/5], step [170000/476702], loss: 2.1503, total loss: 660368.0622\n",
      "Epoch [1/5], step [171000/476702], loss: 6.5888, total loss: 663392.7603\n",
      "Epoch [1/5], step [172000/476702], loss: 0.2763, total loss: 666984.3551\n",
      "Epoch [1/5], step [173000/476702], loss: 5.2554, total loss: 670336.3901\n",
      "Epoch [1/5], step [174000/476702], loss: 1.1095, total loss: 673755.5434\n",
      "Epoch [1/5], step [175000/476702], loss: 0.0572, total loss: 676814.5058\n",
      "Epoch [1/5], step [176000/476702], loss: 3.4289, total loss: 680020.7519\n",
      "Epoch [1/5], step [177000/476702], loss: 4.2262, total loss: 683227.4130\n",
      "Epoch [1/5], step [178000/476702], loss: 0.1171, total loss: 686507.3706\n",
      "Epoch [1/5], step [179000/476702], loss: 1.7862, total loss: 690192.5754\n",
      "Epoch [1/5], step [180000/476702], loss: 5.8768, total loss: 693458.5447\n",
      "Epoch [1/5], step [181000/476702], loss: 3.3882, total loss: 696842.3890\n",
      "Epoch [1/5], step [182000/476702], loss: 0.2239, total loss: 700150.2739\n",
      "Epoch [1/5], step [183000/476702], loss: 10.7214, total loss: 703391.1468\n",
      "Epoch [1/5], step [184000/476702], loss: 6.4394, total loss: 706426.2204\n",
      "Epoch [1/5], step [185000/476702], loss: 0.0339, total loss: 709425.8535\n",
      "Epoch [1/5], step [186000/476702], loss: 6.1431, total loss: 712576.4856\n",
      "Epoch [1/5], step [187000/476702], loss: 3.5204, total loss: 715786.2339\n",
      "Epoch [1/5], step [188000/476702], loss: 4.1013, total loss: 719371.7309\n",
      "Epoch [1/5], step [189000/476702], loss: 5.3122, total loss: 722833.7910\n",
      "Epoch [1/5], step [190000/476702], loss: 5.7039, total loss: 725763.5509\n",
      "Epoch [1/5], step [191000/476702], loss: 7.8020, total loss: 729419.8376\n",
      "Epoch [1/5], step [192000/476702], loss: 5.4751, total loss: 732856.0110\n",
      "Epoch [1/5], step [193000/476702], loss: 0.8956, total loss: 736306.0089\n",
      "Epoch [1/5], step [194000/476702], loss: 10.6113, total loss: 739344.6184\n",
      "Epoch [1/5], step [195000/476702], loss: 3.9621, total loss: 742645.8929\n",
      "Epoch [1/5], step [196000/476702], loss: 5.1517, total loss: 745984.1326\n",
      "Epoch [1/5], step [197000/476702], loss: 0.4049, total loss: 749058.6571\n",
      "Epoch [1/5], step [198000/476702], loss: 1.3273, total loss: 752688.5641\n",
      "Epoch [1/5], step [199000/476702], loss: 4.9409, total loss: 756269.6418\n",
      "Epoch [1/5], step [200000/476702], loss: 7.3901, total loss: 759907.0551\n",
      "Epoch [1/5], step [201000/476702], loss: 1.4815, total loss: 763481.0943\n",
      "Epoch [1/5], step [202000/476702], loss: 0.3850, total loss: 766884.1316\n",
      "Epoch [1/5], step [203000/476702], loss: 4.5897, total loss: 770582.5749\n",
      "Epoch [1/5], step [204000/476702], loss: 0.7274, total loss: 773859.0135\n",
      "Epoch [1/5], step [205000/476702], loss: 7.2532, total loss: 777324.3254\n",
      "Epoch [1/5], step [206000/476702], loss: 0.2829, total loss: 780389.6742\n",
      "Epoch [1/5], step [207000/476702], loss: 10.2366, total loss: 783738.0432\n",
      "Epoch [1/5], step [208000/476702], loss: 1.2590, total loss: 787296.2299\n",
      "Epoch [1/5], step [209000/476702], loss: 5.5481, total loss: 790674.3884\n",
      "Epoch [1/5], step [210000/476702], loss: 0.5430, total loss: 794156.0045\n",
      "Epoch [1/5], step [211000/476702], loss: 0.7353, total loss: 797600.8345\n",
      "Epoch [1/5], step [212000/476702], loss: 11.9501, total loss: 801022.0848\n",
      "Epoch [1/5], step [213000/476702], loss: 1.2077, total loss: 804190.1855\n",
      "Epoch [1/5], step [214000/476702], loss: 2.6473, total loss: 807666.1496\n",
      "Epoch [1/5], step [215000/476702], loss: 1.5988, total loss: 811148.1903\n",
      "Epoch [1/5], step [216000/476702], loss: 3.7690, total loss: 815026.4514\n",
      "Epoch [1/5], step [217000/476702], loss: 0.0383, total loss: 819013.8952\n",
      "Epoch [1/5], step [218000/476702], loss: 1.7261, total loss: 822840.3046\n",
      "Epoch [1/5], step [219000/476702], loss: 0.4598, total loss: 826724.1324\n",
      "Epoch [1/5], step [220000/476702], loss: 0.2954, total loss: 830261.7109\n",
      "Epoch [1/5], step [221000/476702], loss: 1.8663, total loss: 833375.1391\n",
      "Epoch [1/5], step [222000/476702], loss: 0.0349, total loss: 836682.2716\n",
      "Epoch [1/5], step [223000/476702], loss: 0.4631, total loss: 839834.8643\n",
      "Epoch [1/5], step [224000/476702], loss: 1.5476, total loss: 842960.9299\n",
      "Epoch [1/5], step [225000/476702], loss: 0.0706, total loss: 846318.9357\n",
      "Epoch [1/5], step [226000/476702], loss: 0.0467, total loss: 849629.8011\n",
      "Epoch [1/5], step [227000/476702], loss: 0.0193, total loss: 853033.5682\n",
      "Epoch [1/5], step [228000/476702], loss: 7.4140, total loss: 856875.3430\n",
      "Epoch [1/5], step [229000/476702], loss: 4.4119, total loss: 860428.6258\n",
      "Epoch [1/5], step [230000/476702], loss: 0.2830, total loss: 863829.2157\n",
      "Epoch [1/5], step [231000/476702], loss: 4.0967, total loss: 867038.5995\n",
      "Epoch [1/5], step [232000/476702], loss: 1.8212, total loss: 870015.2614\n",
      "Epoch [1/5], step [233000/476702], loss: 0.0303, total loss: 873337.7822\n",
      "Epoch [1/5], step [234000/476702], loss: 1.5898, total loss: 876547.6154\n",
      "Epoch [1/5], step [235000/476702], loss: 0.2969, total loss: 880044.7742\n",
      "Epoch [1/5], step [236000/476702], loss: 0.0150, total loss: 883373.4782\n",
      "Epoch [1/5], step [237000/476702], loss: 2.9901, total loss: 887168.7427\n",
      "Epoch [1/5], step [238000/476702], loss: 9.8437, total loss: 890968.2646\n",
      "Epoch [1/5], step [239000/476702], loss: 1.2659, total loss: 894423.4435\n",
      "Epoch [1/5], step [240000/476702], loss: 1.4629, total loss: 897868.6086\n",
      "Epoch [1/5], step [241000/476702], loss: 2.7711, total loss: 900926.9831\n",
      "Epoch [1/5], step [242000/476702], loss: 0.4322, total loss: 904676.1553\n",
      "Epoch [1/5], step [243000/476702], loss: 0.0037, total loss: 907904.4325\n",
      "Epoch [1/5], step [244000/476702], loss: 0.0118, total loss: 910875.9655\n",
      "Epoch [1/5], step [245000/476702], loss: 1.1662, total loss: 914510.0500\n",
      "Epoch [1/5], step [246000/476702], loss: 13.3758, total loss: 917893.5008\n",
      "Epoch [1/5], step [247000/476702], loss: 6.4960, total loss: 921210.4531\n",
      "Epoch [1/5], step [248000/476702], loss: 5.0298, total loss: 924244.9834\n",
      "Epoch [1/5], step [249000/476702], loss: 5.2550, total loss: 927550.8401\n",
      "Epoch [1/5], step [250000/476702], loss: 7.1184, total loss: 930553.2130\n",
      "Epoch [1/5], step [251000/476702], loss: 6.8808, total loss: 933608.9427\n",
      "Epoch [1/5], step [252000/476702], loss: 3.4444, total loss: 936682.9217\n",
      "Epoch [1/5], step [253000/476702], loss: 5.0001, total loss: 940062.7427\n",
      "Epoch [1/5], step [254000/476702], loss: 8.5720, total loss: 943354.9321\n",
      "Epoch [1/5], step [255000/476702], loss: 2.7283, total loss: 946597.7511\n",
      "Epoch [1/5], step [256000/476702], loss: 0.0613, total loss: 949835.0312\n",
      "Epoch [1/5], step [257000/476702], loss: 0.5247, total loss: 953096.9947\n",
      "Epoch [1/5], step [258000/476702], loss: 10.1815, total loss: 956467.9106\n",
      "Epoch [1/5], step [259000/476702], loss: 5.6014, total loss: 960068.5700\n",
      "Epoch [1/5], step [260000/476702], loss: 3.7700, total loss: 963360.0127\n",
      "Epoch [1/5], step [261000/476702], loss: 2.1995, total loss: 966523.2330\n",
      "Epoch [1/5], step [262000/476702], loss: 2.2163, total loss: 969640.8411\n",
      "Epoch [1/5], step [263000/476702], loss: 0.2620, total loss: 973221.3613\n",
      "Epoch [1/5], step [264000/476702], loss: 0.0026, total loss: 976333.0388\n",
      "Epoch [1/5], step [265000/476702], loss: 0.1478, total loss: 979902.5000\n",
      "Epoch [1/5], step [266000/476702], loss: 0.0697, total loss: 983675.6420\n",
      "Epoch [1/5], step [267000/476702], loss: 0.6214, total loss: 987336.7099\n",
      "Epoch [1/5], step [268000/476702], loss: 3.0365, total loss: 990583.6668\n",
      "Epoch [1/5], step [269000/476702], loss: 8.6802, total loss: 994018.1847\n",
      "Epoch [1/5], step [270000/476702], loss: 0.0309, total loss: 997395.4590\n",
      "Epoch [1/5], step [271000/476702], loss: 6.2462, total loss: 1000598.5496\n",
      "Epoch [1/5], step [272000/476702], loss: 2.9014, total loss: 1003753.1822\n",
      "Epoch [1/5], step [273000/476702], loss: 0.0925, total loss: 1007349.7403\n",
      "Epoch [1/5], step [274000/476702], loss: 0.0020, total loss: 1010417.5383\n",
      "Epoch [1/5], step [275000/476702], loss: 10.8899, total loss: 1013736.6784\n",
      "Epoch [1/5], step [276000/476702], loss: 0.8829, total loss: 1016820.2926\n",
      "Epoch [1/5], step [277000/476702], loss: 11.9895, total loss: 1020408.4956\n",
      "Epoch [1/5], step [278000/476702], loss: 3.4624, total loss: 1023768.3978\n",
      "Epoch [1/5], step [279000/476702], loss: 3.9596, total loss: 1026972.1568\n",
      "Epoch [1/5], step [280000/476702], loss: 12.4136, total loss: 1030446.6905\n",
      "Epoch [1/5], step [281000/476702], loss: 0.5394, total loss: 1033808.0966\n",
      "Epoch [1/5], step [282000/476702], loss: 4.6406, total loss: 1037036.8328\n",
      "Epoch [1/5], step [283000/476702], loss: 5.3014, total loss: 1040354.2654\n",
      "Epoch [1/5], step [284000/476702], loss: 1.5974, total loss: 1043580.1406\n",
      "Epoch [1/5], step [285000/476702], loss: 1.9599, total loss: 1046875.1482\n",
      "Epoch [1/5], step [286000/476702], loss: 3.9883, total loss: 1050411.8993\n",
      "Epoch [1/5], step [287000/476702], loss: 0.3871, total loss: 1053350.4315\n",
      "Epoch [1/5], step [288000/476702], loss: 1.2605, total loss: 1056912.0804\n",
      "Epoch [1/5], step [289000/476702], loss: 0.0378, total loss: 1059947.2421\n",
      "Epoch [1/5], step [290000/476702], loss: 1.3766, total loss: 1063525.5517\n",
      "Epoch [1/5], step [291000/476702], loss: 2.7249, total loss: 1066821.5983\n",
      "Epoch [1/5], step [292000/476702], loss: 0.1041, total loss: 1070455.0675\n",
      "Epoch [1/5], step [293000/476702], loss: 0.0182, total loss: 1073589.0772\n",
      "Epoch [1/5], step [294000/476702], loss: 7.9735, total loss: 1076757.1164\n",
      "Epoch [1/5], step [295000/476702], loss: 0.1183, total loss: 1080177.6542\n",
      "Epoch [1/5], step [296000/476702], loss: 0.0836, total loss: 1083243.0657\n",
      "Epoch [1/5], step [297000/476702], loss: 0.6136, total loss: 1086458.1947\n",
      "Epoch [1/5], step [298000/476702], loss: 1.0820, total loss: 1089550.1540\n",
      "Epoch [1/5], step [299000/476702], loss: 2.2140, total loss: 1092642.8405\n",
      "Epoch [1/5], step [300000/476702], loss: 0.2693, total loss: 1095747.5948\n",
      "Epoch [1/5], step [301000/476702], loss: 1.8603, total loss: 1098959.7582\n",
      "Epoch [1/5], step [302000/476702], loss: 6.9516, total loss: 1102289.6236\n",
      "Epoch [1/5], step [303000/476702], loss: 0.2042, total loss: 1105344.8437\n",
      "Epoch [1/5], step [304000/476702], loss: 0.4053, total loss: 1108453.0712\n",
      "Epoch [1/5], step [305000/476702], loss: 0.9943, total loss: 1111639.2589\n",
      "Epoch [1/5], step [306000/476702], loss: 1.9530, total loss: 1114815.5690\n",
      "Epoch [1/5], step [307000/476702], loss: 0.0581, total loss: 1118179.1365\n",
      "Epoch [1/5], step [308000/476702], loss: 1.4198, total loss: 1121626.8675\n",
      "Epoch [1/5], step [309000/476702], loss: 3.5196, total loss: 1124541.9998\n",
      "Epoch [1/5], step [310000/476702], loss: 10.1487, total loss: 1127824.2505\n",
      "Epoch [1/5], step [311000/476702], loss: 7.2216, total loss: 1130601.7233\n",
      "Epoch [1/5], step [312000/476702], loss: 8.1248, total loss: 1134167.7365\n",
      "Epoch [1/5], step [313000/476702], loss: 9.0297, total loss: 1137451.7785\n",
      "Epoch [1/5], step [314000/476702], loss: 3.3431, total loss: 1140412.7209\n",
      "Epoch [1/5], step [315000/476702], loss: 1.9683, total loss: 1143651.8053\n",
      "Epoch [1/5], step [316000/476702], loss: 1.9059, total loss: 1147001.5348\n",
      "Epoch [1/5], step [317000/476702], loss: 0.1010, total loss: 1149942.8582\n",
      "Epoch [1/5], step [318000/476702], loss: 0.0647, total loss: 1153032.3376\n",
      "Epoch [1/5], step [319000/476702], loss: 5.3020, total loss: 1155875.2772\n",
      "Epoch [1/5], step [320000/476702], loss: 4.8585, total loss: 1159236.3167\n",
      "Epoch [1/5], step [321000/476702], loss: 2.3554, total loss: 1162214.6138\n",
      "Epoch [1/5], step [322000/476702], loss: 0.0375, total loss: 1165329.2775\n",
      "Epoch [1/5], step [323000/476702], loss: 2.0587, total loss: 1168533.1108\n",
      "Epoch [1/5], step [324000/476702], loss: 1.9221, total loss: 1171795.0511\n",
      "Epoch [1/5], step [325000/476702], loss: 2.9157, total loss: 1174894.8148\n",
      "Epoch [1/5], step [326000/476702], loss: 0.3701, total loss: 1178152.8784\n",
      "Epoch [1/5], step [327000/476702], loss: 4.4421, total loss: 1181504.1962\n",
      "Epoch [1/5], step [328000/476702], loss: 0.0693, total loss: 1184451.6355\n",
      "Epoch [1/5], step [329000/476702], loss: 6.9756, total loss: 1187739.9065\n",
      "Epoch [1/5], step [330000/476702], loss: 4.8188, total loss: 1190889.9800\n",
      "Epoch [1/5], step [331000/476702], loss: 1.2102, total loss: 1194162.3391\n",
      "Epoch [1/5], step [332000/476702], loss: 0.7997, total loss: 1197199.8665\n",
      "Epoch [1/5], step [333000/476702], loss: 4.8371, total loss: 1200330.3676\n",
      "Epoch [1/5], step [334000/476702], loss: 6.4423, total loss: 1203742.6691\n",
      "Epoch [1/5], step [335000/476702], loss: 8.3315, total loss: 1206801.1598\n",
      "Epoch [1/5], step [336000/476702], loss: 0.4320, total loss: 1210176.1042\n",
      "Epoch [1/5], step [337000/476702], loss: 1.6774, total loss: 1213044.3242\n",
      "Epoch [1/5], step [338000/476702], loss: 0.4816, total loss: 1215825.0424\n",
      "Epoch [1/5], step [339000/476702], loss: 0.1053, total loss: 1219468.6564\n",
      "Epoch [1/5], step [340000/476702], loss: 0.7921, total loss: 1222293.2036\n",
      "Epoch [1/5], step [341000/476702], loss: 4.0445, total loss: 1225727.5414\n",
      "Epoch [1/5], step [342000/476702], loss: 0.8983, total loss: 1229386.2478\n",
      "Epoch [1/5], step [343000/476702], loss: 0.0005, total loss: 1232422.4679\n",
      "Epoch [1/5], step [344000/476702], loss: 10.2546, total loss: 1235255.6454\n",
      "Epoch [1/5], step [345000/476702], loss: 6.7983, total loss: 1238661.8233\n",
      "Epoch [1/5], step [346000/476702], loss: 6.0717, total loss: 1241712.8510\n",
      "Epoch [1/5], step [347000/476702], loss: 0.1806, total loss: 1245043.5098\n",
      "Epoch [1/5], step [348000/476702], loss: 0.6116, total loss: 1248100.3355\n",
      "Epoch [1/5], step [349000/476702], loss: 8.5551, total loss: 1251363.2528\n",
      "Epoch [1/5], step [350000/476702], loss: 2.2874, total loss: 1254752.9061\n",
      "Epoch [1/5], step [351000/476702], loss: 0.0443, total loss: 1258137.5830\n",
      "Epoch [1/5], step [352000/476702], loss: 9.8365, total loss: 1261420.2639\n",
      "Epoch [1/5], step [353000/476702], loss: 7.6644, total loss: 1264733.5147\n",
      "Epoch [1/5], step [354000/476702], loss: 1.3939, total loss: 1268230.7923\n",
      "Epoch [1/5], step [355000/476702], loss: 3.1171, total loss: 1271199.9007\n",
      "Epoch [1/5], step [356000/476702], loss: 0.5469, total loss: 1274400.3963\n",
      "Epoch [1/5], step [357000/476702], loss: 0.4429, total loss: 1277514.9109\n",
      "Epoch [1/5], step [358000/476702], loss: 1.0895, total loss: 1280475.1416\n",
      "Epoch [1/5], step [359000/476702], loss: 0.7526, total loss: 1283322.2452\n",
      "Epoch [1/5], step [360000/476702], loss: 0.0112, total loss: 1286656.8825\n",
      "Epoch [1/5], step [361000/476702], loss: 2.2957, total loss: 1290000.2678\n",
      "Epoch [1/5], step [362000/476702], loss: 3.4734, total loss: 1293197.4858\n",
      "Epoch [1/5], step [363000/476702], loss: 1.0091, total loss: 1296010.8000\n",
      "Epoch [1/5], step [364000/476702], loss: 0.1274, total loss: 1298960.6574\n",
      "Epoch [1/5], step [365000/476702], loss: 0.4845, total loss: 1302011.9193\n",
      "Epoch [1/5], step [366000/476702], loss: 5.9705, total loss: 1305101.6212\n",
      "Epoch [1/5], step [367000/476702], loss: 0.9717, total loss: 1308404.1629\n",
      "Epoch [1/5], step [368000/476702], loss: 2.0912, total loss: 1311438.6425\n",
      "Epoch [1/5], step [369000/476702], loss: 6.0969, total loss: 1314537.2037\n",
      "Epoch [1/5], step [370000/476702], loss: 3.5061, total loss: 1317906.2485\n",
      "Epoch [1/5], step [371000/476702], loss: 5.5170, total loss: 1321226.2351\n",
      "Epoch [1/5], step [372000/476702], loss: 4.1078, total loss: 1324605.4747\n",
      "Epoch [1/5], step [373000/476702], loss: 0.0211, total loss: 1327411.6746\n",
      "Epoch [1/5], step [374000/476702], loss: 12.4073, total loss: 1330541.1621\n",
      "Epoch [1/5], step [375000/476702], loss: 1.3548, total loss: 1333710.3977\n",
      "Epoch [1/5], step [376000/476702], loss: 0.0101, total loss: 1337178.5295\n",
      "Epoch [1/5], step [377000/476702], loss: 8.0313, total loss: 1340660.0867\n",
      "Epoch [1/5], step [378000/476702], loss: 7.6767, total loss: 1344269.6597\n",
      "Epoch [1/5], step [379000/476702], loss: 2.7096, total loss: 1347675.0617\n",
      "Epoch [1/5], step [380000/476702], loss: 0.0452, total loss: 1350884.1450\n",
      "Epoch [1/5], step [381000/476702], loss: 11.0910, total loss: 1354343.6102\n",
      "Epoch [1/5], step [382000/476702], loss: 2.1504, total loss: 1357512.3504\n",
      "Epoch [1/5], step [383000/476702], loss: 0.0845, total loss: 1360336.0037\n",
      "Epoch [1/5], step [384000/476702], loss: 1.4412, total loss: 1363606.6326\n",
      "Epoch [1/5], step [385000/476702], loss: 0.0275, total loss: 1366764.8401\n",
      "Epoch [1/5], step [386000/476702], loss: 6.8276, total loss: 1370065.1033\n",
      "Epoch [1/5], step [387000/476702], loss: 4.9479, total loss: 1372973.1955\n",
      "Epoch [1/5], step [388000/476702], loss: 4.7085, total loss: 1375677.1828\n",
      "Epoch [1/5], step [389000/476702], loss: 0.0854, total loss: 1378760.0434\n",
      "Epoch [1/5], step [390000/476702], loss: 0.1886, total loss: 1382172.6636\n",
      "Epoch [1/5], step [391000/476702], loss: 5.1954, total loss: 1385270.6246\n",
      "Epoch [1/5], step [392000/476702], loss: 0.0034, total loss: 1388407.3138\n",
      "Epoch [1/5], step [393000/476702], loss: 0.0056, total loss: 1391618.3182\n",
      "Epoch [1/5], step [394000/476702], loss: 0.0141, total loss: 1394819.5409\n",
      "Epoch [1/5], step [395000/476702], loss: 4.4287, total loss: 1397884.7392\n",
      "Epoch [1/5], step [396000/476702], loss: 1.8943, total loss: 1400961.9681\n",
      "Epoch [1/5], step [397000/476702], loss: 5.2374, total loss: 1404049.7577\n",
      "Epoch [1/5], step [398000/476702], loss: 2.0712, total loss: 1407271.3982\n",
      "Epoch [1/5], step [399000/476702], loss: 3.1812, total loss: 1410383.3089\n",
      "Epoch [1/5], step [400000/476702], loss: 0.1327, total loss: 1413302.2867\n",
      "Epoch [1/5], step [401000/476702], loss: 7.2172, total loss: 1416818.3104\n",
      "Epoch [1/5], step [402000/476702], loss: 1.7346, total loss: 1419905.4676\n",
      "Epoch [1/5], step [403000/476702], loss: 0.7800, total loss: 1423433.0576\n",
      "Epoch [1/5], step [404000/476702], loss: 0.1337, total loss: 1426948.5200\n",
      "Epoch [1/5], step [405000/476702], loss: 5.3425, total loss: 1429995.6150\n",
      "Epoch [1/5], step [406000/476702], loss: 4.9612, total loss: 1433350.7363\n",
      "Epoch [1/5], step [407000/476702], loss: 0.1014, total loss: 1436511.4161\n",
      "Epoch [1/5], step [408000/476702], loss: 0.1434, total loss: 1439984.7541\n",
      "Epoch [1/5], step [409000/476702], loss: 2.3096, total loss: 1443148.4699\n",
      "Epoch [1/5], step [410000/476702], loss: 8.3928, total loss: 1446182.0949\n",
      "Epoch [1/5], step [411000/476702], loss: 3.1120, total loss: 1449111.2515\n",
      "Epoch [1/5], step [412000/476702], loss: 2.1209, total loss: 1452342.6984\n",
      "Epoch [1/5], step [413000/476702], loss: 3.9005, total loss: 1455354.2829\n",
      "Epoch [1/5], step [414000/476702], loss: 9.9431, total loss: 1458411.6599\n",
      "Epoch [1/5], step [415000/476702], loss: 1.4233, total loss: 1461565.8095\n",
      "Epoch [1/5], step [416000/476702], loss: 0.1722, total loss: 1464844.1885\n",
      "Epoch [1/5], step [417000/476702], loss: 0.8301, total loss: 1467833.8465\n",
      "Epoch [1/5], step [418000/476702], loss: 0.6290, total loss: 1471242.5326\n",
      "Epoch [1/5], step [419000/476702], loss: 7.0942, total loss: 1473932.6013\n",
      "Epoch [1/5], step [420000/476702], loss: 4.2085, total loss: 1477077.1197\n",
      "Epoch [1/5], step [421000/476702], loss: 0.0407, total loss: 1480238.3275\n",
      "Epoch [1/5], step [422000/476702], loss: 0.9931, total loss: 1483032.0513\n",
      "Epoch [1/5], step [423000/476702], loss: 0.0047, total loss: 1485876.1624\n",
      "Epoch [1/5], step [424000/476702], loss: 0.1145, total loss: 1489037.6822\n",
      "Epoch [1/5], step [425000/476702], loss: 8.9153, total loss: 1492084.5228\n",
      "Epoch [1/5], step [426000/476702], loss: 0.2221, total loss: 1494981.4739\n",
      "Epoch [1/5], step [427000/476702], loss: 0.4726, total loss: 1497834.4550\n",
      "Epoch [1/5], step [428000/476702], loss: 2.2955, total loss: 1500534.5927\n",
      "Epoch [1/5], step [429000/476702], loss: 0.2165, total loss: 1503568.7255\n",
      "Epoch [1/5], step [430000/476702], loss: 5.8330, total loss: 1506560.4759\n",
      "Epoch [1/5], step [431000/476702], loss: 1.0033, total loss: 1509390.2280\n",
      "Epoch [1/5], step [432000/476702], loss: 9.7808, total loss: 1512025.8641\n",
      "Epoch [1/5], step [433000/476702], loss: 0.2399, total loss: 1514728.1125\n",
      "Epoch [1/5], step [434000/476702], loss: 0.0007, total loss: 1517422.5394\n",
      "Epoch [1/5], step [435000/476702], loss: 4.4241, total loss: 1520427.0160\n",
      "Epoch [1/5], step [436000/476702], loss: 0.1307, total loss: 1523732.7061\n",
      "Epoch [1/5], step [437000/476702], loss: 1.4469, total loss: 1526638.8495\n",
      "Epoch [1/5], step [438000/476702], loss: 2.7110, total loss: 1529708.0874\n",
      "Epoch [1/5], step [439000/476702], loss: 7.3600, total loss: 1532609.5899\n",
      "Epoch [1/5], step [440000/476702], loss: 5.4241, total loss: 1535386.3048\n",
      "Epoch [1/5], step [441000/476702], loss: 14.3379, total loss: 1538279.1935\n",
      "Epoch [1/5], step [442000/476702], loss: 0.0107, total loss: 1541418.9482\n",
      "Epoch [1/5], step [443000/476702], loss: 1.8300, total loss: 1544560.0435\n",
      "Epoch [1/5], step [444000/476702], loss: 0.0360, total loss: 1547303.4854\n",
      "Epoch [1/5], step [445000/476702], loss: 3.4345, total loss: 1550477.4196\n",
      "Epoch [1/5], step [446000/476702], loss: 9.3252, total loss: 1553367.0784\n",
      "Epoch [1/5], step [447000/476702], loss: 5.9262, total loss: 1556592.3560\n",
      "Epoch [1/5], step [448000/476702], loss: 0.2042, total loss: 1559348.9393\n",
      "Epoch [1/5], step [449000/476702], loss: 2.4669, total loss: 1562775.7694\n",
      "Epoch [1/5], step [450000/476702], loss: 2.5796, total loss: 1565708.7348\n",
      "Epoch [1/5], step [451000/476702], loss: 1.3401, total loss: 1568769.9086\n",
      "Epoch [1/5], step [452000/476702], loss: 0.0908, total loss: 1572098.6691\n",
      "Epoch [1/5], step [453000/476702], loss: 2.2764, total loss: 1574980.3522\n",
      "Epoch [1/5], step [454000/476702], loss: 12.3838, total loss: 1577813.7617\n",
      "Epoch [1/5], step [455000/476702], loss: 5.1872, total loss: 1581028.3522\n",
      "Epoch [1/5], step [456000/476702], loss: 0.0296, total loss: 1583971.4003\n",
      "Epoch [1/5], step [457000/476702], loss: 0.5848, total loss: 1587007.7030\n",
      "Epoch [1/5], step [458000/476702], loss: 1.4853, total loss: 1589700.2158\n",
      "Epoch [1/5], step [459000/476702], loss: 0.1076, total loss: 1593023.1570\n",
      "Epoch [1/5], step [460000/476702], loss: 9.0121, total loss: 1595989.3530\n",
      "Epoch [1/5], step [461000/476702], loss: 0.2096, total loss: 1598830.1118\n",
      "Epoch [1/5], step [462000/476702], loss: 5.6110, total loss: 1601719.1815\n",
      "Epoch [1/5], step [463000/476702], loss: 1.3421, total loss: 1604534.9357\n",
      "Epoch [1/5], step [464000/476702], loss: 0.4180, total loss: 1607317.9792\n",
      "Epoch [1/5], step [465000/476702], loss: 3.7506, total loss: 1610203.5607\n",
      "Epoch [1/5], step [466000/476702], loss: 0.0872, total loss: 1613008.1055\n",
      "Epoch [1/5], step [467000/476702], loss: 4.2292, total loss: 1616211.0864\n",
      "Epoch [1/5], step [468000/476702], loss: 6.6475, total loss: 1619164.6590\n",
      "Epoch [1/5], step [469000/476702], loss: 0.3353, total loss: 1622021.3460\n",
      "Epoch [1/5], step [470000/476702], loss: 2.1166, total loss: 1624803.9038\n",
      "Epoch [1/5], step [471000/476702], loss: 0.2223, total loss: 1628024.6730\n",
      "Epoch [1/5], step [472000/476702], loss: 5.8019, total loss: 1630974.5261\n",
      "Epoch [1/5], step [473000/476702], loss: 0.4991, total loss: 1634037.6773\n",
      "Epoch [1/5], step [474000/476702], loss: 5.8158, total loss: 1636947.6858\n",
      "Epoch [1/5], step [475000/476702], loss: 6.6074, total loss: 1640372.5352\n",
      "Epoch [1/5], step [476000/476702], loss: 6.1740, total loss: 1643520.6127\n",
      "Epoch [2/5], step [1000/476702], loss: 1.7857, total loss: 2486.1335\n",
      "Epoch [2/5], step [2000/476702], loss: 0.4578, total loss: 5553.1880\n",
      "Epoch [2/5], step [3000/476702], loss: 1.9614, total loss: 8393.5134\n",
      "Epoch [2/5], step [4000/476702], loss: 1.9944, total loss: 11058.2063\n",
      "Epoch [2/5], step [5000/476702], loss: 4.1228, total loss: 14076.5545\n",
      "Epoch [2/5], step [6000/476702], loss: 1.1120, total loss: 16998.0830\n",
      "Epoch [2/5], step [7000/476702], loss: 0.7439, total loss: 19649.8903\n",
      "Epoch [2/5], step [8000/476702], loss: 2.1779, total loss: 22550.0867\n",
      "Epoch [2/5], step [9000/476702], loss: 1.1211, total loss: 25687.1855\n",
      "Epoch [2/5], step [10000/476702], loss: 2.3315, total loss: 28361.0616\n",
      "Epoch [2/5], step [11000/476702], loss: 1.6590, total loss: 30969.4484\n",
      "Epoch [2/5], step [12000/476702], loss: 0.0793, total loss: 33654.6545\n",
      "Epoch [2/5], step [13000/476702], loss: 0.0431, total loss: 36642.4297\n",
      "Epoch [2/5], step [14000/476702], loss: 1.4824, total loss: 39516.2076\n",
      "Epoch [2/5], step [15000/476702], loss: 0.0818, total loss: 42420.7933\n",
      "Epoch [2/5], step [16000/476702], loss: 0.9768, total loss: 45211.7025\n",
      "Epoch [2/5], step [17000/476702], loss: 2.4850, total loss: 48218.1606\n",
      "Epoch [2/5], step [18000/476702], loss: 1.7135, total loss: 51200.6970\n",
      "Epoch [2/5], step [19000/476702], loss: 5.1225, total loss: 54388.0945\n",
      "Epoch [2/5], step [20000/476702], loss: 3.3927, total loss: 57313.5867\n",
      "Epoch [2/5], step [21000/476702], loss: 0.2096, total loss: 60072.2262\n",
      "Epoch [2/5], step [22000/476702], loss: 3.1963, total loss: 62961.4812\n",
      "Epoch [2/5], step [23000/476702], loss: 1.3222, total loss: 65902.0984\n",
      "Epoch [2/5], step [24000/476702], loss: 3.0870, total loss: 68857.4439\n",
      "Epoch [2/5], step [25000/476702], loss: 8.7664, total loss: 71864.7481\n",
      "Epoch [2/5], step [26000/476702], loss: 3.3771, total loss: 75091.1234\n",
      "Epoch [2/5], step [27000/476702], loss: 6.2534, total loss: 77774.3080\n",
      "Epoch [2/5], step [28000/476702], loss: 10.7829, total loss: 80612.6104\n",
      "Epoch [2/5], step [29000/476702], loss: 4.7932, total loss: 83392.2359\n",
      "Epoch [2/5], step [30000/476702], loss: 0.0095, total loss: 86365.3488\n",
      "Epoch [2/5], step [31000/476702], loss: 0.4686, total loss: 89359.0764\n",
      "Epoch [2/5], step [32000/476702], loss: 12.3054, total loss: 92240.1676\n",
      "Epoch [2/5], step [33000/476702], loss: 3.1243, total loss: 95621.0999\n",
      "Epoch [2/5], step [34000/476702], loss: 4.8214, total loss: 98354.5489\n",
      "Epoch [2/5], step [35000/476702], loss: 0.2462, total loss: 101155.7196\n",
      "Epoch [2/5], step [36000/476702], loss: 1.0114, total loss: 104345.8567\n",
      "Epoch [2/5], step [37000/476702], loss: 2.3530, total loss: 107384.8743\n",
      "Epoch [2/5], step [38000/476702], loss: 0.2456, total loss: 110355.0244\n",
      "Epoch [2/5], step [39000/476702], loss: 0.0796, total loss: 113171.3698\n",
      "Epoch [2/5], step [40000/476702], loss: 3.2480, total loss: 115939.9987\n",
      "Epoch [2/5], step [41000/476702], loss: 0.4824, total loss: 118495.0265\n",
      "Epoch [2/5], step [42000/476702], loss: 2.6930, total loss: 121310.4993\n",
      "Epoch [2/5], step [43000/476702], loss: 3.3630, total loss: 124407.5181\n",
      "Epoch [2/5], step [44000/476702], loss: 0.3109, total loss: 127288.8719\n",
      "Epoch [2/5], step [45000/476702], loss: 0.1229, total loss: 130559.7314\n",
      "Epoch [2/5], step [46000/476702], loss: 4.6967, total loss: 133258.9211\n",
      "Epoch [2/5], step [47000/476702], loss: 1.5957, total loss: 135964.8258\n",
      "Epoch [2/5], step [48000/476702], loss: 3.3691, total loss: 138812.3287\n",
      "Epoch [2/5], step [49000/476702], loss: 0.2977, total loss: 142175.8611\n",
      "Epoch [2/5], step [50000/476702], loss: 0.1321, total loss: 145545.8511\n",
      "Epoch [2/5], step [51000/476702], loss: 0.6532, total loss: 148783.7313\n",
      "Epoch [2/5], step [52000/476702], loss: 0.0898, total loss: 151578.9479\n",
      "Epoch [2/5], step [53000/476702], loss: 0.5023, total loss: 154680.2036\n",
      "Epoch [2/5], step [54000/476702], loss: 2.0810, total loss: 157340.1795\n",
      "Epoch [2/5], step [55000/476702], loss: 2.4569, total loss: 160087.6134\n",
      "Epoch [2/5], step [56000/476702], loss: 2.7657, total loss: 163220.7937\n",
      "Epoch [2/5], step [57000/476702], loss: 0.5440, total loss: 165778.5812\n",
      "Epoch [2/5], step [58000/476702], loss: 0.0452, total loss: 169084.5472\n",
      "Epoch [2/5], step [59000/476702], loss: 0.2005, total loss: 172034.6880\n",
      "Epoch [2/5], step [60000/476702], loss: 3.8779, total loss: 174907.3912\n",
      "Epoch [2/5], step [61000/476702], loss: 1.2642, total loss: 177652.9104\n",
      "Epoch [2/5], step [62000/476702], loss: 0.2743, total loss: 180359.4670\n",
      "Epoch [2/5], step [63000/476702], loss: 0.7344, total loss: 183306.5031\n",
      "Epoch [2/5], step [64000/476702], loss: 0.1635, total loss: 186239.0843\n",
      "Epoch [2/5], step [65000/476702], loss: 0.2756, total loss: 189135.6986\n",
      "Epoch [2/5], step [66000/476702], loss: 0.1663, total loss: 191804.0855\n",
      "Epoch [2/5], step [67000/476702], loss: 4.2460, total loss: 194632.7645\n",
      "Epoch [2/5], step [68000/476702], loss: 1.0054, total loss: 197363.6604\n",
      "Epoch [2/5], step [69000/476702], loss: 0.4112, total loss: 200455.8141\n",
      "Epoch [2/5], step [70000/476702], loss: 6.7906, total loss: 203549.6594\n",
      "Epoch [2/5], step [71000/476702], loss: 13.9101, total loss: 206555.7019\n",
      "Epoch [2/5], step [72000/476702], loss: 3.3510, total loss: 209483.5675\n",
      "Epoch [2/5], step [73000/476702], loss: 0.4426, total loss: 212432.8926\n",
      "Epoch [2/5], step [74000/476702], loss: 0.3554, total loss: 215290.2982\n",
      "Epoch [2/5], step [75000/476702], loss: 0.1321, total loss: 217947.6297\n",
      "Epoch [2/5], step [76000/476702], loss: 0.5847, total loss: 220667.6362\n",
      "Epoch [2/5], step [77000/476702], loss: 0.0614, total loss: 223760.7407\n",
      "Epoch [2/5], step [78000/476702], loss: 4.2777, total loss: 226583.4481\n",
      "Epoch [2/5], step [79000/476702], loss: 10.6187, total loss: 229528.2581\n",
      "Epoch [2/5], step [80000/476702], loss: 9.3894, total loss: 232557.6340\n",
      "Epoch [2/5], step [81000/476702], loss: 1.8671, total loss: 235449.5040\n",
      "Epoch [2/5], step [82000/476702], loss: 0.2867, total loss: 238234.9990\n",
      "Epoch [2/5], step [83000/476702], loss: 2.0414, total loss: 241122.7586\n",
      "Epoch [2/5], step [84000/476702], loss: 2.3152, total loss: 243855.1676\n",
      "Epoch [2/5], step [85000/476702], loss: 2.4635, total loss: 246588.7086\n",
      "Epoch [2/5], step [86000/476702], loss: 0.0277, total loss: 249376.2582\n",
      "Epoch [2/5], step [87000/476702], loss: 6.0026, total loss: 252383.9342\n",
      "Epoch [2/5], step [88000/476702], loss: 2.9543, total loss: 254797.5719\n",
      "Epoch [2/5], step [89000/476702], loss: 0.4055, total loss: 257022.6163\n",
      "Epoch [2/5], step [90000/476702], loss: 5.3686, total loss: 259730.3717\n",
      "Epoch [2/5], step [91000/476702], loss: 6.5867, total loss: 262325.7116\n",
      "Epoch [2/5], step [92000/476702], loss: 0.5647, total loss: 265060.6543\n",
      "Epoch [2/5], step [93000/476702], loss: 0.0243, total loss: 267593.4085\n",
      "Epoch [2/5], step [94000/476702], loss: 5.4970, total loss: 270378.6654\n",
      "Epoch [2/5], step [95000/476702], loss: 5.3790, total loss: 273309.8973\n",
      "Epoch [2/5], step [96000/476702], loss: 3.5146, total loss: 275949.4879\n",
      "Epoch [2/5], step [97000/476702], loss: 8.0088, total loss: 279011.6630\n",
      "Epoch [2/5], step [98000/476702], loss: 2.8403, total loss: 281846.9057\n",
      "Epoch [2/5], step [99000/476702], loss: 0.8745, total loss: 284427.4881\n",
      "Epoch [2/5], step [100000/476702], loss: 4.1707, total loss: 287266.9957\n",
      "Epoch [2/5], step [101000/476702], loss: 0.3871, total loss: 290001.7972\n",
      "Epoch [2/5], step [102000/476702], loss: 3.7458, total loss: 292953.1715\n",
      "Epoch [2/5], step [103000/476702], loss: 2.2726, total loss: 295846.0187\n",
      "Epoch [2/5], step [104000/476702], loss: 3.3554, total loss: 298510.8574\n",
      "Epoch [2/5], step [105000/476702], loss: 1.5873, total loss: 301571.7088\n",
      "Epoch [2/5], step [106000/476702], loss: 0.9875, total loss: 304980.6650\n",
      "Epoch [2/5], step [107000/476702], loss: 10.7074, total loss: 307906.4943\n",
      "Epoch [2/5], step [108000/476702], loss: 0.0224, total loss: 310649.0894\n",
      "Epoch [2/5], step [109000/476702], loss: 1.3363, total loss: 313132.5192\n",
      "Epoch [2/5], step [110000/476702], loss: 0.7576, total loss: 315897.4185\n",
      "Epoch [2/5], step [111000/476702], loss: 0.1040, total loss: 318451.9977\n",
      "Epoch [2/5], step [112000/476702], loss: 2.9371, total loss: 321092.7973\n",
      "Epoch [2/5], step [113000/476702], loss: 0.5304, total loss: 323802.1539\n",
      "Epoch [2/5], step [114000/476702], loss: 1.4335, total loss: 326459.0323\n",
      "Epoch [2/5], step [115000/476702], loss: 0.8219, total loss: 329248.0954\n",
      "Epoch [2/5], step [116000/476702], loss: 1.0637, total loss: 332242.7115\n",
      "Epoch [2/5], step [117000/476702], loss: 1.1666, total loss: 334951.8115\n",
      "Epoch [2/5], step [118000/476702], loss: 0.1807, total loss: 337484.2754\n",
      "Epoch [2/5], step [119000/476702], loss: 1.1721, total loss: 340082.6160\n",
      "Epoch [2/5], step [120000/476702], loss: 0.0003, total loss: 343335.3432\n",
      "Epoch [2/5], step [121000/476702], loss: 11.3101, total loss: 345869.9385\n",
      "Epoch [2/5], step [122000/476702], loss: 0.3494, total loss: 348632.1988\n",
      "Epoch [2/5], step [123000/476702], loss: 10.4492, total loss: 351491.0362\n",
      "Epoch [2/5], step [124000/476702], loss: 5.8897, total loss: 354387.4751\n",
      "Epoch [2/5], step [125000/476702], loss: 3.6521, total loss: 356931.4085\n",
      "Epoch [2/5], step [126000/476702], loss: 3.6998, total loss: 359425.4080\n",
      "Epoch [2/5], step [127000/476702], loss: 0.0070, total loss: 362302.4893\n",
      "Epoch [2/5], step [128000/476702], loss: 3.9223, total loss: 365485.5163\n",
      "Epoch [2/5], step [129000/476702], loss: 12.0376, total loss: 368371.2754\n",
      "Epoch [2/5], step [130000/476702], loss: 2.7341, total loss: 371075.8433\n",
      "Epoch [2/5], step [131000/476702], loss: 0.7875, total loss: 374009.7361\n",
      "Epoch [2/5], step [132000/476702], loss: 0.8191, total loss: 376559.9616\n",
      "Epoch [2/5], step [133000/476702], loss: 2.2476, total loss: 379162.4750\n",
      "Epoch [2/5], step [134000/476702], loss: 0.2229, total loss: 381912.3081\n",
      "Epoch [2/5], step [135000/476702], loss: 1.2164, total loss: 384652.9533\n",
      "Epoch [2/5], step [136000/476702], loss: 0.3561, total loss: 387102.9386\n",
      "Epoch [2/5], step [137000/476702], loss: 8.8256, total loss: 389862.2490\n",
      "Epoch [2/5], step [138000/476702], loss: 8.8026, total loss: 392574.3486\n",
      "Epoch [2/5], step [139000/476702], loss: 1.7783, total loss: 395129.0844\n",
      "Epoch [2/5], step [140000/476702], loss: 4.3903, total loss: 397720.3248\n",
      "Epoch [2/5], step [141000/476702], loss: 3.1413, total loss: 400140.3396\n",
      "Epoch [2/5], step [142000/476702], loss: 4.0459, total loss: 402696.9091\n",
      "Epoch [2/5], step [143000/476702], loss: 0.3415, total loss: 405549.4837\n",
      "Epoch [2/5], step [144000/476702], loss: 1.9340, total loss: 408368.8508\n",
      "Epoch [2/5], step [145000/476702], loss: 0.2474, total loss: 411249.9119\n",
      "Epoch [2/5], step [146000/476702], loss: 0.1442, total loss: 414057.2055\n",
      "Epoch [2/5], step [147000/476702], loss: 3.1647, total loss: 416751.7788\n",
      "Epoch [2/5], step [148000/476702], loss: 3.2845, total loss: 419670.1176\n",
      "Epoch [2/5], step [149000/476702], loss: 0.1611, total loss: 422311.9151\n",
      "Epoch [2/5], step [150000/476702], loss: 1.6842, total loss: 424758.8356\n",
      "Epoch [2/5], step [151000/476702], loss: 7.3791, total loss: 427814.5277\n",
      "Epoch [2/5], step [152000/476702], loss: 1.3459, total loss: 430785.8475\n",
      "Epoch [2/5], step [153000/476702], loss: 0.0193, total loss: 433893.2188\n",
      "Epoch [2/5], step [154000/476702], loss: 7.5063, total loss: 436872.0162\n",
      "Epoch [2/5], step [155000/476702], loss: 3.0462, total loss: 439777.0734\n",
      "Epoch [2/5], step [156000/476702], loss: 0.5491, total loss: 442344.9058\n",
      "Epoch [2/5], step [157000/476702], loss: 2.0979, total loss: 445187.3211\n",
      "Epoch [2/5], step [158000/476702], loss: 1.0043, total loss: 448286.9471\n",
      "Epoch [2/5], step [159000/476702], loss: 0.8230, total loss: 451318.6635\n",
      "Epoch [2/5], step [160000/476702], loss: 1.8419, total loss: 454136.6822\n",
      "Epoch [2/5], step [161000/476702], loss: 0.0072, total loss: 457018.1614\n",
      "Epoch [2/5], step [162000/476702], loss: 0.0172, total loss: 459715.9636\n",
      "Epoch [2/5], step [163000/476702], loss: 11.7151, total loss: 462747.0306\n",
      "Epoch [2/5], step [164000/476702], loss: 0.1799, total loss: 465759.0883\n",
      "Epoch [2/5], step [165000/476702], loss: 4.0932, total loss: 468378.7362\n",
      "Epoch [2/5], step [166000/476702], loss: 0.9436, total loss: 470772.6808\n",
      "Epoch [2/5], step [167000/476702], loss: 0.0255, total loss: 473645.0931\n",
      "Epoch [2/5], step [168000/476702], loss: 3.7567, total loss: 476590.3262\n",
      "Epoch [2/5], step [169000/476702], loss: 1.2351, total loss: 479571.0375\n",
      "Epoch [2/5], step [170000/476702], loss: 2.3804, total loss: 482358.6915\n",
      "Epoch [2/5], step [171000/476702], loss: 0.9112, total loss: 484918.2689\n",
      "Epoch [2/5], step [172000/476702], loss: 0.0795, total loss: 487896.3227\n",
      "Epoch [2/5], step [173000/476702], loss: 4.7151, total loss: 490707.4697\n",
      "Epoch [2/5], step [174000/476702], loss: 0.8191, total loss: 493615.2590\n",
      "Epoch [2/5], step [175000/476702], loss: 0.0219, total loss: 496069.3441\n",
      "Epoch [2/5], step [176000/476702], loss: 3.0631, total loss: 498748.0511\n",
      "Epoch [2/5], step [177000/476702], loss: 2.5842, total loss: 501522.2203\n",
      "Epoch [2/5], step [178000/476702], loss: 0.1679, total loss: 504309.1152\n",
      "Epoch [2/5], step [179000/476702], loss: 2.5905, total loss: 507378.1677\n",
      "Epoch [2/5], step [180000/476702], loss: 5.0836, total loss: 510111.9868\n",
      "Epoch [2/5], step [181000/476702], loss: 2.6093, total loss: 513109.6724\n",
      "Epoch [2/5], step [182000/476702], loss: 0.2020, total loss: 515943.0102\n",
      "Epoch [2/5], step [183000/476702], loss: 11.3822, total loss: 518647.8331\n",
      "Epoch [2/5], step [184000/476702], loss: 5.3336, total loss: 521243.2895\n",
      "Epoch [2/5], step [185000/476702], loss: 0.0737, total loss: 523777.7088\n",
      "Epoch [2/5], step [186000/476702], loss: 4.1221, total loss: 526377.2531\n",
      "Epoch [2/5], step [187000/476702], loss: 2.8628, total loss: 529037.1812\n",
      "Epoch [2/5], step [188000/476702], loss: 2.3952, total loss: 532067.8662\n",
      "Epoch [2/5], step [189000/476702], loss: 6.2644, total loss: 535055.3551\n",
      "Epoch [2/5], step [190000/476702], loss: 4.9331, total loss: 537455.0811\n",
      "Epoch [2/5], step [191000/476702], loss: 6.5913, total loss: 540577.0592\n",
      "Epoch [2/5], step [192000/476702], loss: 3.2703, total loss: 543467.3881\n",
      "Epoch [2/5], step [193000/476702], loss: 0.5411, total loss: 546335.0256\n",
      "Epoch [2/5], step [194000/476702], loss: 11.2065, total loss: 548883.7742\n",
      "Epoch [2/5], step [195000/476702], loss: 3.9285, total loss: 551649.5164\n",
      "Epoch [2/5], step [196000/476702], loss: 4.8388, total loss: 554432.7231\n",
      "Epoch [2/5], step [197000/476702], loss: 0.5362, total loss: 556992.7758\n",
      "Epoch [2/5], step [198000/476702], loss: 0.3666, total loss: 560121.7309\n",
      "Epoch [2/5], step [199000/476702], loss: 3.5276, total loss: 563140.4971\n",
      "Epoch [2/5], step [200000/476702], loss: 6.8918, total loss: 566170.7144\n",
      "Epoch [2/5], step [201000/476702], loss: 0.9401, total loss: 569232.7537\n",
      "Epoch [2/5], step [202000/476702], loss: 0.3293, total loss: 572114.3877\n",
      "Epoch [2/5], step [203000/476702], loss: 4.7032, total loss: 575256.4396\n",
      "Epoch [2/5], step [204000/476702], loss: 0.2841, total loss: 578019.2147\n",
      "Epoch [2/5], step [205000/476702], loss: 5.6585, total loss: 580931.5616\n",
      "Epoch [2/5], step [206000/476702], loss: 0.4826, total loss: 583504.8893\n",
      "Epoch [2/5], step [207000/476702], loss: 6.2931, total loss: 586361.7060\n",
      "Epoch [2/5], step [208000/476702], loss: 0.8012, total loss: 589331.7901\n",
      "Epoch [2/5], step [209000/476702], loss: 3.0630, total loss: 592137.3726\n",
      "Epoch [2/5], step [210000/476702], loss: 0.7037, total loss: 595102.2700\n",
      "Epoch [2/5], step [211000/476702], loss: 0.7396, total loss: 598067.7112\n",
      "Epoch [2/5], step [212000/476702], loss: 7.7519, total loss: 600919.0728\n",
      "Epoch [2/5], step [213000/476702], loss: 0.9264, total loss: 603521.3502\n",
      "Epoch [2/5], step [214000/476702], loss: 2.3831, total loss: 606457.8597\n",
      "Epoch [2/5], step [215000/476702], loss: 0.5238, total loss: 609373.0850\n",
      "Epoch [2/5], step [216000/476702], loss: 2.1620, total loss: 612726.1993\n",
      "Epoch [2/5], step [217000/476702], loss: 0.0046, total loss: 616206.1557\n",
      "Epoch [2/5], step [218000/476702], loss: 1.0700, total loss: 619540.7250\n",
      "Epoch [2/5], step [219000/476702], loss: 0.4676, total loss: 622918.5216\n",
      "Epoch [2/5], step [220000/476702], loss: 0.1863, total loss: 625999.0440\n",
      "Epoch [2/5], step [221000/476702], loss: 1.3251, total loss: 628626.7860\n",
      "Epoch [2/5], step [222000/476702], loss: 0.0213, total loss: 631448.7738\n",
      "Epoch [2/5], step [223000/476702], loss: 0.4873, total loss: 634139.9611\n",
      "Epoch [2/5], step [224000/476702], loss: 1.1993, total loss: 636807.6748\n",
      "Epoch [2/5], step [225000/476702], loss: 0.0577, total loss: 639604.2731\n",
      "Epoch [2/5], step [226000/476702], loss: 0.0596, total loss: 642381.3465\n",
      "Epoch [2/5], step [227000/476702], loss: 0.0089, total loss: 645250.0625\n",
      "Epoch [2/5], step [228000/476702], loss: 4.4949, total loss: 648552.0994\n",
      "Epoch [2/5], step [229000/476702], loss: 4.8620, total loss: 651613.0689\n",
      "Epoch [2/5], step [230000/476702], loss: 0.0827, total loss: 654620.0499\n",
      "Epoch [2/5], step [231000/476702], loss: 4.2901, total loss: 657357.4632\n",
      "Epoch [2/5], step [232000/476702], loss: 0.5001, total loss: 659914.5463\n",
      "Epoch [2/5], step [233000/476702], loss: 0.0232, total loss: 662723.8978\n",
      "Epoch [2/5], step [234000/476702], loss: 1.3002, total loss: 665483.9275\n",
      "Epoch [2/5], step [235000/476702], loss: 0.3402, total loss: 668553.3525\n",
      "Epoch [2/5], step [236000/476702], loss: 0.0061, total loss: 671474.2935\n",
      "Epoch [2/5], step [237000/476702], loss: 2.9975, total loss: 674820.3728\n",
      "Epoch [2/5], step [238000/476702], loss: 6.5673, total loss: 678079.8236\n",
      "Epoch [2/5], step [239000/476702], loss: 1.1511, total loss: 681069.6843\n",
      "Epoch [2/5], step [240000/476702], loss: 1.5363, total loss: 684099.7092\n",
      "Epoch [2/5], step [241000/476702], loss: 2.6948, total loss: 686747.7538\n",
      "Epoch [2/5], step [242000/476702], loss: 0.5314, total loss: 689982.2963\n",
      "Epoch [2/5], step [243000/476702], loss: 0.0052, total loss: 692766.7258\n",
      "Epoch [2/5], step [244000/476702], loss: 0.0143, total loss: 695336.7430\n",
      "Epoch [2/5], step [245000/476702], loss: 1.2750, total loss: 698435.8715\n",
      "Epoch [2/5], step [246000/476702], loss: 11.6882, total loss: 701323.2393\n",
      "Epoch [2/5], step [247000/476702], loss: 6.0604, total loss: 704222.0888\n",
      "Epoch [2/5], step [248000/476702], loss: 2.4916, total loss: 706851.2411\n",
      "Epoch [2/5], step [249000/476702], loss: 5.2583, total loss: 709769.9903\n",
      "Epoch [2/5], step [250000/476702], loss: 5.9482, total loss: 712391.4300\n",
      "Epoch [2/5], step [251000/476702], loss: 6.1103, total loss: 715039.8072\n",
      "Epoch [2/5], step [252000/476702], loss: 3.0432, total loss: 717602.5516\n",
      "Epoch [2/5], step [253000/476702], loss: 4.9355, total loss: 720584.3421\n",
      "Epoch [2/5], step [254000/476702], loss: 5.9946, total loss: 723342.9978\n",
      "Epoch [2/5], step [255000/476702], loss: 1.6657, total loss: 726154.2215\n",
      "Epoch [2/5], step [256000/476702], loss: 0.0354, total loss: 728959.9793\n",
      "Epoch [2/5], step [257000/476702], loss: 0.5249, total loss: 731823.9483\n",
      "Epoch [2/5], step [258000/476702], loss: 8.3422, total loss: 734738.4808\n",
      "Epoch [2/5], step [259000/476702], loss: 3.2031, total loss: 737906.9551\n",
      "Epoch [2/5], step [260000/476702], loss: 3.6349, total loss: 740762.7165\n",
      "Epoch [2/5], step [261000/476702], loss: 2.0669, total loss: 743498.5151\n",
      "Epoch [2/5], step [262000/476702], loss: 1.7767, total loss: 746218.0074\n",
      "Epoch [2/5], step [263000/476702], loss: 0.0996, total loss: 749392.3898\n",
      "Epoch [2/5], step [264000/476702], loss: 0.0008, total loss: 752117.5863\n",
      "Epoch [2/5], step [265000/476702], loss: 0.1246, total loss: 755271.8943\n",
      "Epoch [2/5], step [266000/476702], loss: 0.0594, total loss: 758591.0964\n",
      "Epoch [2/5], step [267000/476702], loss: 0.6869, total loss: 761773.8699\n",
      "Epoch [2/5], step [268000/476702], loss: 2.7188, total loss: 764623.6600\n",
      "Epoch [2/5], step [269000/476702], loss: 6.4654, total loss: 767654.0525\n",
      "Epoch [2/5], step [270000/476702], loss: 0.0260, total loss: 770591.7417\n",
      "Epoch [2/5], step [271000/476702], loss: 5.6735, total loss: 773399.2666\n",
      "Epoch [2/5], step [272000/476702], loss: 2.6614, total loss: 776159.8661\n",
      "Epoch [2/5], step [273000/476702], loss: 0.0550, total loss: 779292.1960\n",
      "Epoch [2/5], step [274000/476702], loss: 0.0027, total loss: 781933.2039\n",
      "Epoch [2/5], step [275000/476702], loss: 11.3400, total loss: 784847.3841\n",
      "Epoch [2/5], step [276000/476702], loss: 0.9474, total loss: 787550.9064\n",
      "Epoch [2/5], step [277000/476702], loss: 10.9766, total loss: 790739.5524\n",
      "Epoch [2/5], step [278000/476702], loss: 2.4920, total loss: 793693.6764\n",
      "Epoch [2/5], step [279000/476702], loss: 3.3304, total loss: 796477.7317\n",
      "Epoch [2/5], step [280000/476702], loss: 12.4218, total loss: 799569.4355\n",
      "Epoch [2/5], step [281000/476702], loss: 0.1905, total loss: 802610.5457\n",
      "Epoch [2/5], step [282000/476702], loss: 4.6232, total loss: 805418.0061\n",
      "Epoch [2/5], step [283000/476702], loss: 5.1931, total loss: 808320.2682\n",
      "Epoch [2/5], step [284000/476702], loss: 0.6367, total loss: 811233.0081\n",
      "Epoch [2/5], step [285000/476702], loss: 0.7049, total loss: 814074.6654\n",
      "Epoch [2/5], step [286000/476702], loss: 3.7199, total loss: 817144.2148\n",
      "Epoch [2/5], step [287000/476702], loss: 0.3659, total loss: 819715.8744\n",
      "Epoch [2/5], step [288000/476702], loss: 0.8030, total loss: 822916.0670\n",
      "Epoch [2/5], step [289000/476702], loss: 0.0132, total loss: 825507.5282\n",
      "Epoch [2/5], step [290000/476702], loss: 1.2710, total loss: 828662.9491\n",
      "Epoch [2/5], step [291000/476702], loss: 1.8913, total loss: 831547.5543\n",
      "Epoch [2/5], step [292000/476702], loss: 0.0774, total loss: 834774.5572\n",
      "Epoch [2/5], step [293000/476702], loss: 0.0344, total loss: 837486.1191\n",
      "Epoch [2/5], step [294000/476702], loss: 6.6488, total loss: 840285.6004\n",
      "Epoch [2/5], step [295000/476702], loss: 0.0394, total loss: 843310.2931\n",
      "Epoch [2/5], step [296000/476702], loss: 0.0748, total loss: 845949.4295\n",
      "Epoch [2/5], step [297000/476702], loss: 0.6442, total loss: 848733.7444\n",
      "Epoch [2/5], step [298000/476702], loss: 1.1271, total loss: 851456.4934\n",
      "Epoch [2/5], step [299000/476702], loss: 1.9314, total loss: 854180.6421\n",
      "Epoch [2/5], step [300000/476702], loss: 0.3445, total loss: 856976.4805\n",
      "Epoch [2/5], step [301000/476702], loss: 1.3837, total loss: 859868.2514\n",
      "Epoch [2/5], step [302000/476702], loss: 4.3614, total loss: 862819.9434\n",
      "Epoch [2/5], step [303000/476702], loss: 0.2454, total loss: 865521.1556\n",
      "Epoch [2/5], step [304000/476702], loss: 0.4220, total loss: 868254.2548\n",
      "Epoch [2/5], step [305000/476702], loss: 0.9533, total loss: 871005.6686\n",
      "Epoch [2/5], step [306000/476702], loss: 0.8285, total loss: 873727.3559\n",
      "Epoch [2/5], step [307000/476702], loss: 0.0405, total loss: 876714.6018\n",
      "Epoch [2/5], step [308000/476702], loss: 0.3532, total loss: 879730.1489\n",
      "Epoch [2/5], step [309000/476702], loss: 2.8974, total loss: 882278.7171\n",
      "Epoch [2/5], step [310000/476702], loss: 8.4578, total loss: 885222.7738\n",
      "Epoch [2/5], step [311000/476702], loss: 6.1155, total loss: 887699.3345\n",
      "Epoch [2/5], step [312000/476702], loss: 5.4807, total loss: 890906.3641\n",
      "Epoch [2/5], step [313000/476702], loss: 7.6544, total loss: 893799.1477\n",
      "Epoch [2/5], step [314000/476702], loss: 3.3221, total loss: 896379.9036\n",
      "Epoch [2/5], step [315000/476702], loss: 1.8460, total loss: 899230.9068\n",
      "Epoch [2/5], step [316000/476702], loss: 1.3413, total loss: 902225.4769\n",
      "Epoch [2/5], step [317000/476702], loss: 0.0358, total loss: 904841.7411\n",
      "Epoch [2/5], step [318000/476702], loss: 0.0737, total loss: 907553.3322\n",
      "Epoch [2/5], step [319000/476702], loss: 3.0854, total loss: 910081.5118\n",
      "Epoch [2/5], step [320000/476702], loss: 6.8633, total loss: 913055.7894\n",
      "Epoch [2/5], step [321000/476702], loss: 2.1887, total loss: 915699.5512\n",
      "Epoch [2/5], step [322000/476702], loss: 0.0198, total loss: 918439.0522\n",
      "Epoch [2/5], step [323000/476702], loss: 2.7860, total loss: 921317.1044\n",
      "Epoch [2/5], step [324000/476702], loss: 1.6645, total loss: 924185.0544\n",
      "Epoch [2/5], step [325000/476702], loss: 3.0686, total loss: 926967.2557\n",
      "Epoch [2/5], step [326000/476702], loss: 0.3222, total loss: 929844.3450\n",
      "Epoch [2/5], step [327000/476702], loss: 4.3999, total loss: 932813.3179\n",
      "Epoch [2/5], step [328000/476702], loss: 0.0286, total loss: 935456.2527\n",
      "Epoch [2/5], step [329000/476702], loss: 5.0278, total loss: 938376.3895\n",
      "Epoch [2/5], step [330000/476702], loss: 3.9420, total loss: 941198.2501\n",
      "Epoch [2/5], step [331000/476702], loss: 1.5479, total loss: 944112.3173\n",
      "Epoch [2/5], step [332000/476702], loss: 0.7623, total loss: 946849.0011\n",
      "Epoch [2/5], step [333000/476702], loss: 2.6146, total loss: 949591.3926\n",
      "Epoch [2/5], step [334000/476702], loss: 6.0658, total loss: 952695.4725\n",
      "Epoch [2/5], step [335000/476702], loss: 6.6578, total loss: 955443.1548\n",
      "Epoch [2/5], step [336000/476702], loss: 0.4132, total loss: 958437.6802\n",
      "Epoch [2/5], step [337000/476702], loss: 1.1268, total loss: 960952.6351\n",
      "Epoch [2/5], step [338000/476702], loss: 0.1880, total loss: 963385.2521\n",
      "Epoch [2/5], step [339000/476702], loss: 0.0207, total loss: 966638.1047\n",
      "Epoch [2/5], step [340000/476702], loss: 1.0444, total loss: 969139.3261\n",
      "Epoch [2/5], step [341000/476702], loss: 4.3470, total loss: 972238.3286\n",
      "Epoch [2/5], step [342000/476702], loss: 0.7956, total loss: 975516.3949\n",
      "Epoch [2/5], step [343000/476702], loss: 0.0003, total loss: 978232.5326\n",
      "Epoch [2/5], step [344000/476702], loss: 9.3939, total loss: 980764.8986\n",
      "Epoch [2/5], step [345000/476702], loss: 5.5616, total loss: 983776.7261\n",
      "Epoch [2/5], step [346000/476702], loss: 3.5669, total loss: 986493.1652\n",
      "Epoch [2/5], step [347000/476702], loss: 0.1732, total loss: 989413.3724\n",
      "Epoch [2/5], step [348000/476702], loss: 0.6910, total loss: 992141.6068\n",
      "Epoch [2/5], step [349000/476702], loss: 9.4916, total loss: 995047.4724\n",
      "Epoch [2/5], step [350000/476702], loss: 1.7548, total loss: 998113.8097\n",
      "Epoch [2/5], step [351000/476702], loss: 0.0219, total loss: 1001106.6590\n",
      "Epoch [2/5], step [352000/476702], loss: 7.8934, total loss: 1004058.1305\n",
      "Epoch [2/5], step [353000/476702], loss: 8.1960, total loss: 1007008.1224\n",
      "Epoch [2/5], step [354000/476702], loss: 1.5430, total loss: 1010143.5243\n",
      "Epoch [2/5], step [355000/476702], loss: 2.4930, total loss: 1012822.0101\n",
      "Epoch [2/5], step [356000/476702], loss: 0.4027, total loss: 1015630.9766\n",
      "Epoch [2/5], step [357000/476702], loss: 0.1832, total loss: 1018438.6820\n",
      "Epoch [2/5], step [358000/476702], loss: 0.9530, total loss: 1021059.9558\n",
      "Epoch [2/5], step [359000/476702], loss: 0.7719, total loss: 1023612.1698\n",
      "Epoch [2/5], step [360000/476702], loss: 0.0397, total loss: 1026680.2937\n",
      "Epoch [2/5], step [361000/476702], loss: 2.4323, total loss: 1029690.6683\n",
      "Epoch [2/5], step [362000/476702], loss: 3.2745, total loss: 1032580.2441\n",
      "Epoch [2/5], step [363000/476702], loss: 0.6200, total loss: 1035053.7059\n",
      "Epoch [2/5], step [364000/476702], loss: 0.1847, total loss: 1037687.4957\n",
      "Epoch [2/5], step [365000/476702], loss: 0.3036, total loss: 1040438.2048\n",
      "Epoch [2/5], step [366000/476702], loss: 5.5839, total loss: 1043219.2800\n",
      "Epoch [2/5], step [367000/476702], loss: 0.4030, total loss: 1046200.8958\n",
      "Epoch [2/5], step [368000/476702], loss: 2.0971, total loss: 1048917.8573\n",
      "Epoch [2/5], step [369000/476702], loss: 4.6657, total loss: 1051728.2114\n",
      "Epoch [2/5], step [370000/476702], loss: 3.6305, total loss: 1054774.1948\n",
      "Epoch [2/5], step [371000/476702], loss: 5.2280, total loss: 1057695.4827\n",
      "Epoch [2/5], step [372000/476702], loss: 2.7978, total loss: 1060675.2315\n",
      "Epoch [2/5], step [373000/476702], loss: 0.0144, total loss: 1063140.5221\n",
      "Epoch [2/5], step [374000/476702], loss: 10.6934, total loss: 1065969.6553\n",
      "Epoch [2/5], step [375000/476702], loss: 1.3497, total loss: 1068814.9885\n",
      "Epoch [2/5], step [376000/476702], loss: 0.0099, total loss: 1071905.1544\n",
      "Epoch [2/5], step [377000/476702], loss: 7.1840, total loss: 1075018.5552\n",
      "Epoch [2/5], step [378000/476702], loss: 8.4782, total loss: 1078232.5217\n",
      "Epoch [2/5], step [379000/476702], loss: 2.1526, total loss: 1081317.8632\n",
      "Epoch [2/5], step [380000/476702], loss: 0.0436, total loss: 1084248.0970\n",
      "Epoch [2/5], step [381000/476702], loss: 10.8968, total loss: 1087329.9650\n",
      "Epoch [2/5], step [382000/476702], loss: 2.0751, total loss: 1090194.4381\n",
      "Epoch [2/5], step [383000/476702], loss: 0.1131, total loss: 1092699.0876\n",
      "Epoch [2/5], step [384000/476702], loss: 1.5321, total loss: 1095660.2763\n",
      "Epoch [2/5], step [385000/476702], loss: 0.0306, total loss: 1098522.2174\n",
      "Epoch [2/5], step [386000/476702], loss: 6.3888, total loss: 1101458.5335\n",
      "Epoch [2/5], step [387000/476702], loss: 4.4498, total loss: 1104067.6360\n",
      "Epoch [2/5], step [388000/476702], loss: 4.3653, total loss: 1106506.4868\n",
      "Epoch [2/5], step [389000/476702], loss: 0.0850, total loss: 1109308.5026\n",
      "Epoch [2/5], step [390000/476702], loss: 0.1181, total loss: 1112370.9357\n",
      "Epoch [2/5], step [391000/476702], loss: 5.1998, total loss: 1115191.2994\n",
      "Epoch [2/5], step [392000/476702], loss: 0.0019, total loss: 1118034.8778\n",
      "Epoch [2/5], step [393000/476702], loss: 0.0043, total loss: 1120895.8396\n",
      "Epoch [2/5], step [394000/476702], loss: 0.0134, total loss: 1123723.1445\n",
      "Epoch [2/5], step [395000/476702], loss: 4.4923, total loss: 1126489.5482\n",
      "Epoch [2/5], step [396000/476702], loss: 0.5998, total loss: 1129277.0413\n",
      "Epoch [2/5], step [397000/476702], loss: 4.0455, total loss: 1132043.9874\n",
      "Epoch [2/5], step [398000/476702], loss: 1.4520, total loss: 1134909.9609\n",
      "Epoch [2/5], step [399000/476702], loss: 3.1515, total loss: 1137744.9753\n",
      "Epoch [2/5], step [400000/476702], loss: 0.1820, total loss: 1140349.4951\n",
      "Epoch [2/5], step [401000/476702], loss: 6.5520, total loss: 1143520.6358\n",
      "Epoch [2/5], step [402000/476702], loss: 1.9149, total loss: 1146300.3966\n",
      "Epoch [2/5], step [403000/476702], loss: 0.4216, total loss: 1149459.9249\n",
      "Epoch [2/5], step [404000/476702], loss: 0.0860, total loss: 1152632.1215\n",
      "Epoch [2/5], step [405000/476702], loss: 4.9346, total loss: 1155393.5581\n",
      "Epoch [2/5], step [406000/476702], loss: 4.2710, total loss: 1158417.9518\n",
      "Epoch [2/5], step [407000/476702], loss: 0.0419, total loss: 1161298.9808\n",
      "Epoch [2/5], step [408000/476702], loss: 0.1149, total loss: 1164371.1809\n",
      "Epoch [2/5], step [409000/476702], loss: 1.9819, total loss: 1167244.3277\n",
      "Epoch [2/5], step [410000/476702], loss: 7.3603, total loss: 1170016.2062\n",
      "Epoch [2/5], step [411000/476702], loss: 2.6022, total loss: 1172675.3513\n",
      "Epoch [2/5], step [412000/476702], loss: 1.6338, total loss: 1175583.4242\n",
      "Epoch [2/5], step [413000/476702], loss: 3.1571, total loss: 1178250.5758\n",
      "Epoch [2/5], step [414000/476702], loss: 9.6387, total loss: 1180998.7844\n",
      "Epoch [2/5], step [415000/476702], loss: 0.7437, total loss: 1183800.5640\n",
      "Epoch [2/5], step [416000/476702], loss: 0.0651, total loss: 1186781.9462\n",
      "Epoch [2/5], step [417000/476702], loss: 0.7330, total loss: 1189489.1895\n",
      "Epoch [2/5], step [418000/476702], loss: 0.7292, total loss: 1192542.3981\n",
      "Epoch [2/5], step [419000/476702], loss: 7.1171, total loss: 1194966.8044\n",
      "Epoch [2/5], step [420000/476702], loss: 3.1909, total loss: 1197855.9637\n",
      "Epoch [2/5], step [421000/476702], loss: 0.0422, total loss: 1200694.5317\n",
      "Epoch [2/5], step [422000/476702], loss: 1.2918, total loss: 1203180.0971\n",
      "Epoch [2/5], step [423000/476702], loss: 0.0039, total loss: 1205770.2138\n",
      "Epoch [2/5], step [424000/476702], loss: 0.1047, total loss: 1208620.1582\n",
      "Epoch [2/5], step [425000/476702], loss: 8.5650, total loss: 1211385.7038\n",
      "Epoch [2/5], step [426000/476702], loss: 0.1789, total loss: 1214070.9036\n",
      "Epoch [2/5], step [427000/476702], loss: 0.4595, total loss: 1216673.9683\n",
      "Epoch [2/5], step [428000/476702], loss: 2.2325, total loss: 1219120.4872\n",
      "Epoch [2/5], step [429000/476702], loss: 0.2127, total loss: 1221892.1609\n",
      "Epoch [2/5], step [430000/476702], loss: 3.4007, total loss: 1224607.2400\n",
      "Epoch [2/5], step [431000/476702], loss: 0.9239, total loss: 1227179.7999\n",
      "Epoch [2/5], step [432000/476702], loss: 7.7302, total loss: 1229534.7501\n",
      "Epoch [2/5], step [433000/476702], loss: 0.1656, total loss: 1231982.5271\n",
      "Epoch [2/5], step [434000/476702], loss: 0.0014, total loss: 1234364.7739\n",
      "Epoch [2/5], step [435000/476702], loss: 3.9943, total loss: 1237053.7647\n",
      "Epoch [2/5], step [436000/476702], loss: 0.0944, total loss: 1240047.8967\n",
      "Epoch [2/5], step [437000/476702], loss: 1.5468, total loss: 1242637.5884\n",
      "Epoch [2/5], step [438000/476702], loss: 2.3802, total loss: 1245368.2089\n",
      "Epoch [2/5], step [439000/476702], loss: 8.3639, total loss: 1248023.4809\n",
      "Epoch [2/5], step [440000/476702], loss: 4.7460, total loss: 1250525.4016\n",
      "Epoch [2/5], step [441000/476702], loss: 14.6016, total loss: 1253136.4143\n",
      "Epoch [2/5], step [442000/476702], loss: 0.0052, total loss: 1255962.7880\n",
      "Epoch [2/5], step [443000/476702], loss: 1.6832, total loss: 1258833.9783\n",
      "Epoch [2/5], step [444000/476702], loss: 0.0547, total loss: 1261323.1424\n",
      "Epoch [2/5], step [445000/476702], loss: 3.5425, total loss: 1264218.5864\n",
      "Epoch [2/5], step [446000/476702], loss: 9.0319, total loss: 1266780.5280\n",
      "Epoch [2/5], step [447000/476702], loss: 5.5631, total loss: 1269742.2240\n",
      "Epoch [2/5], step [448000/476702], loss: 0.2307, total loss: 1272184.3743\n",
      "Epoch [2/5], step [449000/476702], loss: 1.9030, total loss: 1275295.4899\n",
      "Epoch [2/5], step [450000/476702], loss: 2.3803, total loss: 1277929.5715\n",
      "Epoch [2/5], step [451000/476702], loss: 1.5680, total loss: 1280715.4784\n",
      "Epoch [2/5], step [452000/476702], loss: 0.0960, total loss: 1283729.2673\n",
      "Epoch [2/5], step [453000/476702], loss: 2.3471, total loss: 1286335.8485\n",
      "Epoch [2/5], step [454000/476702], loss: 12.0263, total loss: 1288905.0499\n",
      "Epoch [2/5], step [455000/476702], loss: 4.5890, total loss: 1291833.2265\n",
      "Epoch [2/5], step [456000/476702], loss: 0.0267, total loss: 1294550.4842\n",
      "Epoch [2/5], step [457000/476702], loss: 0.5999, total loss: 1297228.0498\n",
      "Epoch [2/5], step [458000/476702], loss: 1.4050, total loss: 1299630.4318\n",
      "Epoch [2/5], step [459000/476702], loss: 0.1811, total loss: 1302657.9460\n",
      "Epoch [2/5], step [460000/476702], loss: 9.1189, total loss: 1305348.5527\n",
      "Epoch [2/5], step [461000/476702], loss: 0.1801, total loss: 1307942.3426\n",
      "Epoch [2/5], step [462000/476702], loss: 5.9895, total loss: 1310553.5151\n",
      "Epoch [2/5], step [463000/476702], loss: 1.4458, total loss: 1313055.5421\n",
      "Epoch [2/5], step [464000/476702], loss: 0.5184, total loss: 1315623.1842\n",
      "Epoch [2/5], step [465000/476702], loss: 4.2454, total loss: 1318253.3063\n",
      "Epoch [2/5], step [466000/476702], loss: 0.0741, total loss: 1320837.6406\n",
      "Epoch [2/5], step [467000/476702], loss: 3.7474, total loss: 1323744.5241\n",
      "Epoch [2/5], step [468000/476702], loss: 5.3986, total loss: 1326380.8325\n",
      "Epoch [2/5], step [469000/476702], loss: 0.2238, total loss: 1328978.5189\n",
      "Epoch [2/5], step [470000/476702], loss: 1.6175, total loss: 1331515.0099\n",
      "Epoch [2/5], step [471000/476702], loss: 0.1866, total loss: 1334464.6464\n",
      "Epoch [2/5], step [472000/476702], loss: 4.9907, total loss: 1337137.9471\n",
      "Epoch [2/5], step [473000/476702], loss: 0.3872, total loss: 1339976.2243\n",
      "Epoch [2/5], step [474000/476702], loss: 5.8039, total loss: 1342637.4761\n",
      "Epoch [2/5], step [475000/476702], loss: 6.8074, total loss: 1345752.0205\n",
      "Epoch [2/5], step [476000/476702], loss: 5.1463, total loss: 1348603.9625\n",
      "Epoch [3/5], step [1000/476702], loss: 1.9830, total loss: 2287.0210\n",
      "Epoch [3/5], step [2000/476702], loss: 0.4493, total loss: 5132.9888\n",
      "Epoch [3/5], step [3000/476702], loss: 1.8380, total loss: 7738.9940\n",
      "Epoch [3/5], step [4000/476702], loss: 2.1647, total loss: 10170.0568\n",
      "Epoch [3/5], step [5000/476702], loss: 2.6277, total loss: 12952.2539\n",
      "Epoch [3/5], step [6000/476702], loss: 1.3445, total loss: 15649.6038\n",
      "Epoch [3/5], step [7000/476702], loss: 0.9070, total loss: 18124.7129\n",
      "Epoch [3/5], step [8000/476702], loss: 1.8952, total loss: 20811.9301\n",
      "Epoch [3/5], step [9000/476702], loss: 1.4397, total loss: 23695.8542\n",
      "Epoch [3/5], step [10000/476702], loss: 1.1599, total loss: 26188.8724\n",
      "Epoch [3/5], step [11000/476702], loss: 1.7641, total loss: 28613.6703\n",
      "Epoch [3/5], step [12000/476702], loss: 0.0800, total loss: 31070.4707\n",
      "Epoch [3/5], step [13000/476702], loss: 0.0171, total loss: 33799.6758\n",
      "Epoch [3/5], step [14000/476702], loss: 1.5250, total loss: 36437.8003\n",
      "Epoch [3/5], step [15000/476702], loss: 0.0517, total loss: 39145.2322\n",
      "Epoch [3/5], step [16000/476702], loss: 0.6972, total loss: 41673.8727\n",
      "Epoch [3/5], step [17000/476702], loss: 2.4380, total loss: 44422.8346\n",
      "Epoch [3/5], step [18000/476702], loss: 1.6146, total loss: 47105.0717\n",
      "Epoch [3/5], step [19000/476702], loss: 5.7087, total loss: 50071.5703\n",
      "Epoch [3/5], step [20000/476702], loss: 3.6713, total loss: 52766.9082\n",
      "Epoch [3/5], step [21000/476702], loss: 0.1394, total loss: 55264.3438\n",
      "Epoch [3/5], step [22000/476702], loss: 3.1297, total loss: 57948.0533\n",
      "Epoch [3/5], step [23000/476702], loss: 1.6415, total loss: 60690.2556\n",
      "Epoch [3/5], step [24000/476702], loss: 3.2655, total loss: 63386.9394\n",
      "Epoch [3/5], step [25000/476702], loss: 8.9045, total loss: 66150.2368\n",
      "Epoch [3/5], step [26000/476702], loss: 2.7066, total loss: 69126.6883\n",
      "Epoch [3/5], step [27000/476702], loss: 6.2997, total loss: 71586.6502\n",
      "Epoch [3/5], step [28000/476702], loss: 10.8707, total loss: 74180.6093\n",
      "Epoch [3/5], step [29000/476702], loss: 5.1435, total loss: 76733.0811\n",
      "Epoch [3/5], step [30000/476702], loss: 0.0031, total loss: 79470.5774\n",
      "Epoch [3/5], step [31000/476702], loss: 0.4556, total loss: 82194.3042\n",
      "Epoch [3/5], step [32000/476702], loss: 12.5477, total loss: 84816.7182\n",
      "Epoch [3/5], step [33000/476702], loss: 3.3181, total loss: 87946.9341\n",
      "Epoch [3/5], step [34000/476702], loss: 5.4126, total loss: 90486.2739\n",
      "Epoch [3/5], step [35000/476702], loss: 0.1379, total loss: 93085.9570\n",
      "Epoch [3/5], step [36000/476702], loss: 0.8967, total loss: 96055.9339\n",
      "Epoch [3/5], step [37000/476702], loss: 2.0506, total loss: 98885.5295\n",
      "Epoch [3/5], step [38000/476702], loss: 0.1786, total loss: 101643.1150\n",
      "Epoch [3/5], step [39000/476702], loss: 0.0680, total loss: 104180.2611\n",
      "Epoch [3/5], step [40000/476702], loss: 3.6234, total loss: 106789.2328\n",
      "Epoch [3/5], step [41000/476702], loss: 0.2819, total loss: 109169.2722\n",
      "Epoch [3/5], step [42000/476702], loss: 2.4244, total loss: 111793.3180\n",
      "Epoch [3/5], step [43000/476702], loss: 3.5926, total loss: 114663.7846\n",
      "Epoch [3/5], step [44000/476702], loss: 0.1966, total loss: 117334.9459\n",
      "Epoch [3/5], step [45000/476702], loss: 0.1079, total loss: 120372.5169\n",
      "Epoch [3/5], step [46000/476702], loss: 4.2288, total loss: 122832.7657\n",
      "Epoch [3/5], step [47000/476702], loss: 1.3138, total loss: 125322.7347\n",
      "Epoch [3/5], step [48000/476702], loss: 3.2457, total loss: 127950.3485\n",
      "Epoch [3/5], step [49000/476702], loss: 0.0814, total loss: 131019.5768\n",
      "Epoch [3/5], step [50000/476702], loss: 0.1153, total loss: 134154.0077\n",
      "Epoch [3/5], step [51000/476702], loss: 0.6136, total loss: 137156.6675\n",
      "Epoch [3/5], step [52000/476702], loss: 0.1291, total loss: 139766.3743\n",
      "Epoch [3/5], step [53000/476702], loss: 0.5574, total loss: 142654.8531\n",
      "Epoch [3/5], step [54000/476702], loss: 2.3520, total loss: 145121.2667\n",
      "Epoch [3/5], step [55000/476702], loss: 2.3768, total loss: 147629.1356\n",
      "Epoch [3/5], step [56000/476702], loss: 3.0301, total loss: 150528.2891\n",
      "Epoch [3/5], step [57000/476702], loss: 0.5093, total loss: 152897.8800\n",
      "Epoch [3/5], step [58000/476702], loss: 0.0431, total loss: 155984.6212\n",
      "Epoch [3/5], step [59000/476702], loss: 0.2142, total loss: 158747.5634\n",
      "Epoch [3/5], step [60000/476702], loss: 3.3641, total loss: 161396.8962\n",
      "Epoch [3/5], step [61000/476702], loss: 1.3091, total loss: 163967.1685\n",
      "Epoch [3/5], step [62000/476702], loss: 0.2688, total loss: 166478.9170\n",
      "Epoch [3/5], step [63000/476702], loss: 0.4169, total loss: 169254.4164\n",
      "Epoch [3/5], step [64000/476702], loss: 0.1257, total loss: 171981.5544\n",
      "Epoch [3/5], step [65000/476702], loss: 0.2439, total loss: 174664.4904\n",
      "Epoch [3/5], step [66000/476702], loss: 0.1240, total loss: 177165.8603\n",
      "Epoch [3/5], step [67000/476702], loss: 4.2232, total loss: 179774.4564\n",
      "Epoch [3/5], step [68000/476702], loss: 1.1138, total loss: 182311.4801\n",
      "Epoch [3/5], step [69000/476702], loss: 0.3064, total loss: 185138.0900\n",
      "Epoch [3/5], step [70000/476702], loss: 6.7044, total loss: 188021.5153\n",
      "Epoch [3/5], step [71000/476702], loss: 13.5445, total loss: 190835.7360\n",
      "Epoch [3/5], step [72000/476702], loss: 3.3677, total loss: 193564.5761\n",
      "Epoch [3/5], step [73000/476702], loss: 0.4338, total loss: 196285.5556\n",
      "Epoch [3/5], step [74000/476702], loss: 0.5672, total loss: 198950.4203\n",
      "Epoch [3/5], step [75000/476702], loss: 0.1340, total loss: 201462.0701\n",
      "Epoch [3/5], step [76000/476702], loss: 0.5269, total loss: 203959.6297\n",
      "Epoch [3/5], step [77000/476702], loss: 0.0453, total loss: 206846.3966\n",
      "Epoch [3/5], step [78000/476702], loss: 3.0495, total loss: 209459.5291\n",
      "Epoch [3/5], step [79000/476702], loss: 10.3228, total loss: 212192.3306\n",
      "Epoch [3/5], step [80000/476702], loss: 10.0605, total loss: 215015.0801\n",
      "Epoch [3/5], step [81000/476702], loss: 1.9080, total loss: 217683.8092\n",
      "Epoch [3/5], step [82000/476702], loss: 0.2623, total loss: 220272.9003\n",
      "Epoch [3/5], step [83000/476702], loss: 1.8975, total loss: 222943.0005\n",
      "Epoch [3/5], step [84000/476702], loss: 1.7437, total loss: 225439.7091\n",
      "Epoch [3/5], step [85000/476702], loss: 2.8996, total loss: 227990.9739\n",
      "Epoch [3/5], step [86000/476702], loss: 0.0116, total loss: 230581.4673\n",
      "Epoch [3/5], step [87000/476702], loss: 5.6006, total loss: 233335.0188\n",
      "Epoch [3/5], step [88000/476702], loss: 2.7231, total loss: 235557.5642\n",
      "Epoch [3/5], step [89000/476702], loss: 0.3772, total loss: 237602.5572\n",
      "Epoch [3/5], step [90000/476702], loss: 5.2138, total loss: 240126.5164\n",
      "Epoch [3/5], step [91000/476702], loss: 6.2771, total loss: 242511.1728\n",
      "Epoch [3/5], step [92000/476702], loss: 0.1756, total loss: 245075.5278\n",
      "Epoch [3/5], step [93000/476702], loss: 0.0228, total loss: 247431.8416\n",
      "Epoch [3/5], step [94000/476702], loss: 4.9540, total loss: 250026.9986\n",
      "Epoch [3/5], step [95000/476702], loss: 4.4980, total loss: 252751.9467\n",
      "Epoch [3/5], step [96000/476702], loss: 3.4224, total loss: 255184.4865\n",
      "Epoch [3/5], step [97000/476702], loss: 7.6472, total loss: 258013.7344\n",
      "Epoch [3/5], step [98000/476702], loss: 2.7927, total loss: 260644.6330\n",
      "Epoch [3/5], step [99000/476702], loss: 1.0908, total loss: 263022.3205\n",
      "Epoch [3/5], step [100000/476702], loss: 4.3155, total loss: 265630.4681\n",
      "Epoch [3/5], step [101000/476702], loss: 0.3439, total loss: 268205.8814\n",
      "Epoch [3/5], step [102000/476702], loss: 3.3963, total loss: 270944.6007\n",
      "Epoch [3/5], step [103000/476702], loss: 2.3490, total loss: 273645.7922\n",
      "Epoch [3/5], step [104000/476702], loss: 3.1655, total loss: 276130.0746\n",
      "Epoch [3/5], step [105000/476702], loss: 1.0992, total loss: 278966.4688\n",
      "Epoch [3/5], step [106000/476702], loss: 0.7933, total loss: 282046.9066\n",
      "Epoch [3/5], step [107000/476702], loss: 10.8644, total loss: 284675.9474\n",
      "Epoch [3/5], step [108000/476702], loss: 0.0135, total loss: 287201.8729\n",
      "Epoch [3/5], step [109000/476702], loss: 1.4821, total loss: 289533.3825\n",
      "Epoch [3/5], step [110000/476702], loss: 0.8559, total loss: 292133.2603\n",
      "Epoch [3/5], step [111000/476702], loss: 0.1136, total loss: 294476.0228\n",
      "Epoch [3/5], step [112000/476702], loss: 3.4213, total loss: 296921.6112\n",
      "Epoch [3/5], step [113000/476702], loss: 0.5021, total loss: 299441.7322\n",
      "Epoch [3/5], step [114000/476702], loss: 1.3124, total loss: 301946.6570\n",
      "Epoch [3/5], step [115000/476702], loss: 0.7462, total loss: 304506.9954\n",
      "Epoch [3/5], step [116000/476702], loss: 0.9244, total loss: 307337.1473\n",
      "Epoch [3/5], step [117000/476702], loss: 0.7804, total loss: 309853.8983\n",
      "Epoch [3/5], step [118000/476702], loss: 0.0906, total loss: 312167.2643\n",
      "Epoch [3/5], step [119000/476702], loss: 0.8918, total loss: 314551.0518\n",
      "Epoch [3/5], step [120000/476702], loss: 0.0003, total loss: 317538.5564\n",
      "Epoch [3/5], step [121000/476702], loss: 10.8252, total loss: 319898.1523\n",
      "Epoch [3/5], step [122000/476702], loss: 0.2912, total loss: 322446.3998\n",
      "Epoch [3/5], step [123000/476702], loss: 9.5607, total loss: 325089.5390\n",
      "Epoch [3/5], step [124000/476702], loss: 5.5801, total loss: 327749.1198\n",
      "Epoch [3/5], step [125000/476702], loss: 4.0104, total loss: 330113.6676\n",
      "Epoch [3/5], step [126000/476702], loss: 3.9040, total loss: 332450.4300\n",
      "Epoch [3/5], step [127000/476702], loss: 0.0063, total loss: 335130.9065\n",
      "Epoch [3/5], step [128000/476702], loss: 4.2475, total loss: 338086.3433\n",
      "Epoch [3/5], step [129000/476702], loss: 11.1858, total loss: 340747.6300\n",
      "Epoch [3/5], step [130000/476702], loss: 2.2858, total loss: 343217.2619\n",
      "Epoch [3/5], step [131000/476702], loss: 0.8482, total loss: 345957.3562\n",
      "Epoch [3/5], step [132000/476702], loss: 0.6218, total loss: 348309.4688\n",
      "Epoch [3/5], step [133000/476702], loss: 2.3269, total loss: 350751.6632\n",
      "Epoch [3/5], step [134000/476702], loss: 0.0964, total loss: 353259.7300\n",
      "Epoch [3/5], step [135000/476702], loss: 1.0625, total loss: 355807.7592\n",
      "Epoch [3/5], step [136000/476702], loss: 0.3269, total loss: 358103.2625\n",
      "Epoch [3/5], step [137000/476702], loss: 8.2524, total loss: 360667.1617\n",
      "Epoch [3/5], step [138000/476702], loss: 6.8767, total loss: 363171.4604\n",
      "Epoch [3/5], step [139000/476702], loss: 1.7759, total loss: 365514.6606\n",
      "Epoch [3/5], step [140000/476702], loss: 4.5460, total loss: 367877.6250\n",
      "Epoch [3/5], step [141000/476702], loss: 3.2682, total loss: 370111.8944\n",
      "Epoch [3/5], step [142000/476702], loss: 4.1542, total loss: 372470.7171\n",
      "Epoch [3/5], step [143000/476702], loss: 0.3342, total loss: 375106.2329\n",
      "Epoch [3/5], step [144000/476702], loss: 1.7293, total loss: 377720.5912\n",
      "Epoch [3/5], step [145000/476702], loss: 0.3066, total loss: 380400.6337\n",
      "Epoch [3/5], step [146000/476702], loss: 0.1283, total loss: 383036.4581\n",
      "Epoch [3/5], step [147000/476702], loss: 3.0005, total loss: 385548.5324\n",
      "Epoch [3/5], step [148000/476702], loss: 3.7505, total loss: 388241.3227\n",
      "Epoch [3/5], step [149000/476702], loss: 0.1957, total loss: 390699.4493\n",
      "Epoch [3/5], step [150000/476702], loss: 1.6920, total loss: 392975.3088\n",
      "Epoch [3/5], step [151000/476702], loss: 8.4380, total loss: 395837.1999\n",
      "Epoch [3/5], step [152000/476702], loss: 1.3585, total loss: 398609.4416\n",
      "Epoch [3/5], step [153000/476702], loss: 0.0064, total loss: 401523.4219\n",
      "Epoch [3/5], step [154000/476702], loss: 7.5891, total loss: 404322.7977\n",
      "Epoch [3/5], step [155000/476702], loss: 3.3753, total loss: 407043.8211\n",
      "Epoch [3/5], step [156000/476702], loss: 0.6574, total loss: 409463.5235\n",
      "Epoch [3/5], step [157000/476702], loss: 2.1241, total loss: 412125.5973\n",
      "Epoch [3/5], step [158000/476702], loss: 0.8814, total loss: 415026.2931\n",
      "Epoch [3/5], step [159000/476702], loss: 0.8975, total loss: 417858.6842\n",
      "Epoch [3/5], step [160000/476702], loss: 1.8736, total loss: 420410.5322\n",
      "Epoch [3/5], step [161000/476702], loss: 0.0046, total loss: 423114.1803\n",
      "Epoch [3/5], step [162000/476702], loss: 0.0137, total loss: 425602.4576\n",
      "Epoch [3/5], step [163000/476702], loss: 11.4865, total loss: 428437.1109\n",
      "Epoch [3/5], step [164000/476702], loss: 0.1585, total loss: 431243.7987\n",
      "Epoch [3/5], step [165000/476702], loss: 4.0409, total loss: 433644.3784\n",
      "Epoch [3/5], step [166000/476702], loss: 0.5847, total loss: 435857.1760\n",
      "Epoch [3/5], step [167000/476702], loss: 0.0305, total loss: 438494.4461\n",
      "Epoch [3/5], step [168000/476702], loss: 2.1769, total loss: 441243.7085\n",
      "Epoch [3/5], step [169000/476702], loss: 1.0065, total loss: 443988.0453\n",
      "Epoch [3/5], step [170000/476702], loss: 2.3266, total loss: 446549.1199\n",
      "Epoch [3/5], step [171000/476702], loss: 0.1945, total loss: 448957.0427\n",
      "Epoch [3/5], step [172000/476702], loss: 0.0711, total loss: 451698.2758\n",
      "Epoch [3/5], step [173000/476702], loss: 3.9088, total loss: 454312.6284\n",
      "Epoch [3/5], step [174000/476702], loss: 0.7701, total loss: 457011.6731\n",
      "Epoch [3/5], step [175000/476702], loss: 0.0179, total loss: 459278.4964\n",
      "Epoch [3/5], step [176000/476702], loss: 2.7884, total loss: 461749.9071\n",
      "Epoch [3/5], step [177000/476702], loss: 2.6278, total loss: 464363.2474\n",
      "Epoch [3/5], step [178000/476702], loss: 0.1514, total loss: 466978.9395\n",
      "Epoch [3/5], step [179000/476702], loss: 2.3027, total loss: 469820.1867\n",
      "Epoch [3/5], step [180000/476702], loss: 4.8193, total loss: 472358.5564\n",
      "Epoch [3/5], step [181000/476702], loss: 2.1431, total loss: 475187.7540\n",
      "Epoch [3/5], step [182000/476702], loss: 0.1654, total loss: 477834.0288\n",
      "Epoch [3/5], step [183000/476702], loss: 11.2496, total loss: 480312.5136\n",
      "Epoch [3/5], step [184000/476702], loss: 4.9810, total loss: 482734.6529\n",
      "Epoch [3/5], step [185000/476702], loss: 0.0866, total loss: 485113.6389\n",
      "Epoch [3/5], step [186000/476702], loss: 3.9489, total loss: 487513.1697\n",
      "Epoch [3/5], step [187000/476702], loss: 2.6525, total loss: 489957.0557\n",
      "Epoch [3/5], step [188000/476702], loss: 1.9762, total loss: 492730.8962\n",
      "Epoch [3/5], step [189000/476702], loss: 5.6884, total loss: 495528.6291\n",
      "Epoch [3/5], step [190000/476702], loss: 4.0079, total loss: 497730.8918\n",
      "Epoch [3/5], step [191000/476702], loss: 5.7688, total loss: 500642.1786\n",
      "Epoch [3/5], step [192000/476702], loss: 2.4666, total loss: 503320.1315\n",
      "Epoch [3/5], step [193000/476702], loss: 0.4238, total loss: 505967.9523\n",
      "Epoch [3/5], step [194000/476702], loss: 11.7627, total loss: 508325.0910\n",
      "Epoch [3/5], step [195000/476702], loss: 3.7912, total loss: 510908.2333\n",
      "Epoch [3/5], step [196000/476702], loss: 4.1537, total loss: 513501.0399\n",
      "Epoch [3/5], step [197000/476702], loss: 0.6027, total loss: 515863.2260\n",
      "Epoch [3/5], step [198000/476702], loss: 0.2398, total loss: 518790.4463\n",
      "Epoch [3/5], step [199000/476702], loss: 2.5933, total loss: 521586.4661\n",
      "Epoch [3/5], step [200000/476702], loss: 5.9794, total loss: 524362.6647\n",
      "Epoch [3/5], step [201000/476702], loss: 0.7712, total loss: 527227.7329\n",
      "Epoch [3/5], step [202000/476702], loss: 0.3086, total loss: 529901.8216\n",
      "Epoch [3/5], step [203000/476702], loss: 4.4648, total loss: 532828.1882\n",
      "Epoch [3/5], step [204000/476702], loss: 0.3001, total loss: 535395.6696\n",
      "Epoch [3/5], step [205000/476702], loss: 5.5228, total loss: 538106.5498\n",
      "Epoch [3/5], step [206000/476702], loss: 0.6338, total loss: 540490.6949\n",
      "Epoch [3/5], step [207000/476702], loss: 4.5182, total loss: 543157.6922\n",
      "Epoch [3/5], step [208000/476702], loss: 0.7220, total loss: 545897.8013\n",
      "Epoch [3/5], step [209000/476702], loss: 2.6108, total loss: 548512.2062\n",
      "Epoch [3/5], step [210000/476702], loss: 0.7707, total loss: 551247.3387\n",
      "Epoch [3/5], step [211000/476702], loss: 0.8115, total loss: 554025.7294\n",
      "Epoch [3/5], step [212000/476702], loss: 6.8572, total loss: 556656.5552\n",
      "Epoch [3/5], step [213000/476702], loss: 0.9189, total loss: 559045.6118\n",
      "Epoch [3/5], step [214000/476702], loss: 1.9748, total loss: 561787.2959\n",
      "Epoch [3/5], step [215000/476702], loss: 0.3671, total loss: 564504.6320\n",
      "Epoch [3/5], step [216000/476702], loss: 0.9791, total loss: 567648.4157\n",
      "Epoch [3/5], step [217000/476702], loss: 0.0022, total loss: 570906.0861\n",
      "Epoch [3/5], step [218000/476702], loss: 1.1635, total loss: 574006.8117\n",
      "Epoch [3/5], step [219000/476702], loss: 0.5093, total loss: 577152.7688\n",
      "Epoch [3/5], step [220000/476702], loss: 0.1212, total loss: 580043.5970\n",
      "Epoch [3/5], step [221000/476702], loss: 1.2681, total loss: 582458.4556\n",
      "Epoch [3/5], step [222000/476702], loss: 0.0132, total loss: 585089.8469\n",
      "Epoch [3/5], step [223000/476702], loss: 0.3215, total loss: 587601.6764\n",
      "Epoch [3/5], step [224000/476702], loss: 0.7873, total loss: 590082.6185\n",
      "Epoch [3/5], step [225000/476702], loss: 0.0614, total loss: 592667.4894\n",
      "Epoch [3/5], step [226000/476702], loss: 0.0547, total loss: 595236.4989\n",
      "Epoch [3/5], step [227000/476702], loss: 0.0108, total loss: 597898.0686\n",
      "Epoch [3/5], step [228000/476702], loss: 1.7415, total loss: 600962.0190\n",
      "Epoch [3/5], step [229000/476702], loss: 5.2586, total loss: 603806.1291\n",
      "Epoch [3/5], step [230000/476702], loss: 0.0611, total loss: 606660.4275\n",
      "Epoch [3/5], step [231000/476702], loss: 4.1066, total loss: 609208.3838\n",
      "Epoch [3/5], step [232000/476702], loss: 0.3010, total loss: 611604.1885\n",
      "Epoch [3/5], step [233000/476702], loss: 0.0329, total loss: 614207.6749\n",
      "Epoch [3/5], step [234000/476702], loss: 1.3679, total loss: 616766.5648\n",
      "Epoch [3/5], step [235000/476702], loss: 0.3789, total loss: 619646.0204\n",
      "Epoch [3/5], step [236000/476702], loss: 0.0043, total loss: 622396.2657\n",
      "Epoch [3/5], step [237000/476702], loss: 2.6898, total loss: 625530.8503\n",
      "Epoch [3/5], step [238000/476702], loss: 4.9615, total loss: 628561.1948\n",
      "Epoch [3/5], step [239000/476702], loss: 0.9011, total loss: 631353.8184\n",
      "Epoch [3/5], step [240000/476702], loss: 1.4078, total loss: 634211.9146\n",
      "Epoch [3/5], step [241000/476702], loss: 2.3422, total loss: 636687.8459\n",
      "Epoch [3/5], step [242000/476702], loss: 0.5314, total loss: 639706.9471\n",
      "Epoch [3/5], step [243000/476702], loss: 0.0037, total loss: 642318.7126\n",
      "Epoch [3/5], step [244000/476702], loss: 0.0115, total loss: 644729.4851\n",
      "Epoch [3/5], step [245000/476702], loss: 1.2252, total loss: 647594.5871\n",
      "Epoch [3/5], step [246000/476702], loss: 10.1027, total loss: 650279.8377\n",
      "Epoch [3/5], step [247000/476702], loss: 5.9440, total loss: 653002.6212\n",
      "Epoch [3/5], step [248000/476702], loss: 1.8215, total loss: 655462.0263\n",
      "Epoch [3/5], step [249000/476702], loss: 4.1466, total loss: 658207.5184\n",
      "Epoch [3/5], step [250000/476702], loss: 5.1576, total loss: 660675.4361\n",
      "Epoch [3/5], step [251000/476702], loss: 5.8915, total loss: 663146.2577\n",
      "Epoch [3/5], step [252000/476702], loss: 3.0344, total loss: 665516.3841\n",
      "Epoch [3/5], step [253000/476702], loss: 4.8218, total loss: 668324.7189\n",
      "Epoch [3/5], step [254000/476702], loss: 4.7156, total loss: 670873.1648\n",
      "Epoch [3/5], step [255000/476702], loss: 1.2409, total loss: 673479.6874\n",
      "Epoch [3/5], step [256000/476702], loss: 0.0201, total loss: 676106.7593\n",
      "Epoch [3/5], step [257000/476702], loss: 0.3974, total loss: 678796.8780\n",
      "Epoch [3/5], step [258000/476702], loss: 7.1086, total loss: 681513.9978\n",
      "Epoch [3/5], step [259000/476702], loss: 2.4970, total loss: 684471.5092\n",
      "Epoch [3/5], step [260000/476702], loss: 3.4485, total loss: 687165.0072\n",
      "Epoch [3/5], step [261000/476702], loss: 2.0366, total loss: 689724.6598\n",
      "Epoch [3/5], step [262000/476702], loss: 1.7523, total loss: 692280.1551\n",
      "Epoch [3/5], step [263000/476702], loss: 0.0527, total loss: 695260.3588\n",
      "Epoch [3/5], step [264000/476702], loss: 0.0005, total loss: 697806.6445\n",
      "Epoch [3/5], step [265000/476702], loss: 0.0964, total loss: 700761.0498\n",
      "Epoch [3/5], step [266000/476702], loss: 0.0368, total loss: 703852.7990\n",
      "Epoch [3/5], step [267000/476702], loss: 0.6825, total loss: 706794.3113\n",
      "Epoch [3/5], step [268000/476702], loss: 2.6215, total loss: 709468.5559\n",
      "Epoch [3/5], step [269000/476702], loss: 4.9092, total loss: 712309.2239\n",
      "Epoch [3/5], step [270000/476702], loss: 0.0188, total loss: 715005.5477\n",
      "Epoch [3/5], step [271000/476702], loss: 5.5553, total loss: 717634.4550\n",
      "Epoch [3/5], step [272000/476702], loss: 2.7333, total loss: 720231.1843\n",
      "Epoch [3/5], step [273000/476702], loss: 0.0363, total loss: 723144.2722\n",
      "Epoch [3/5], step [274000/476702], loss: 0.0020, total loss: 725632.9030\n",
      "Epoch [3/5], step [275000/476702], loss: 11.5133, total loss: 728362.9084\n",
      "Epoch [3/5], step [276000/476702], loss: 0.7115, total loss: 730893.6689\n",
      "Epoch [3/5], step [277000/476702], loss: 10.3919, total loss: 733889.0368\n",
      "Epoch [3/5], step [278000/476702], loss: 1.7299, total loss: 736639.6921\n",
      "Epoch [3/5], step [279000/476702], loss: 3.0879, total loss: 739228.0247\n",
      "Epoch [3/5], step [280000/476702], loss: 11.9426, total loss: 742130.9210\n",
      "Epoch [3/5], step [281000/476702], loss: 0.1484, total loss: 745002.8688\n",
      "Epoch [3/5], step [282000/476702], loss: 3.7301, total loss: 747613.8675\n",
      "Epoch [3/5], step [283000/476702], loss: 4.6100, total loss: 750315.5337\n",
      "Epoch [3/5], step [284000/476702], loss: 0.3352, total loss: 753045.5734\n",
      "Epoch [3/5], step [285000/476702], loss: 0.5175, total loss: 755663.3185\n",
      "Epoch [3/5], step [286000/476702], loss: 3.4490, total loss: 758516.8374\n",
      "Epoch [3/5], step [287000/476702], loss: 0.3608, total loss: 760923.4627\n",
      "Epoch [3/5], step [288000/476702], loss: 0.7665, total loss: 763965.6315\n",
      "Epoch [3/5], step [289000/476702], loss: 0.0050, total loss: 766370.0066\n",
      "Epoch [3/5], step [290000/476702], loss: 1.5925, total loss: 769327.3465\n",
      "Epoch [3/5], step [291000/476702], loss: 1.6519, total loss: 772009.5450\n",
      "Epoch [3/5], step [292000/476702], loss: 0.0695, total loss: 775012.5178\n",
      "Epoch [3/5], step [293000/476702], loss: 0.0468, total loss: 777534.2012\n",
      "Epoch [3/5], step [294000/476702], loss: 6.0274, total loss: 780150.6735\n",
      "Epoch [3/5], step [295000/476702], loss: 0.0348, total loss: 782974.6125\n",
      "Epoch [3/5], step [296000/476702], loss: 0.0778, total loss: 785434.5026\n",
      "Epoch [3/5], step [297000/476702], loss: 0.7345, total loss: 788040.9019\n",
      "Epoch [3/5], step [298000/476702], loss: 1.0492, total loss: 790607.5162\n",
      "Epoch [3/5], step [299000/476702], loss: 1.8917, total loss: 793147.1507\n",
      "Epoch [3/5], step [300000/476702], loss: 0.3006, total loss: 795791.1732\n",
      "Epoch [3/5], step [301000/476702], loss: 1.0806, total loss: 798520.6759\n",
      "Epoch [3/5], step [302000/476702], loss: 3.0092, total loss: 801312.8774\n",
      "Epoch [3/5], step [303000/476702], loss: 0.2312, total loss: 803860.1804\n",
      "Epoch [3/5], step [304000/476702], loss: 0.4426, total loss: 806419.8997\n",
      "Epoch [3/5], step [305000/476702], loss: 1.0271, total loss: 808976.6911\n",
      "Epoch [3/5], step [306000/476702], loss: 0.6813, total loss: 811501.7806\n",
      "Epoch [3/5], step [307000/476702], loss: 0.0308, total loss: 814314.0814\n",
      "Epoch [3/5], step [308000/476702], loss: 0.2407, total loss: 817159.7529\n",
      "Epoch [3/5], step [309000/476702], loss: 2.6983, total loss: 819548.5428\n",
      "Epoch [3/5], step [310000/476702], loss: 6.8062, total loss: 822317.2492\n",
      "Epoch [3/5], step [311000/476702], loss: 5.4933, total loss: 824657.1418\n",
      "Epoch [3/5], step [312000/476702], loss: 4.2348, total loss: 827681.8107\n",
      "Epoch [3/5], step [313000/476702], loss: 5.8961, total loss: 830393.4639\n",
      "Epoch [3/5], step [314000/476702], loss: 2.9970, total loss: 832808.6195\n",
      "Epoch [3/5], step [315000/476702], loss: 1.8688, total loss: 835460.1446\n",
      "Epoch [3/5], step [316000/476702], loss: 1.2611, total loss: 838279.6464\n",
      "Epoch [3/5], step [317000/476702], loss: 0.0223, total loss: 840748.7956\n",
      "Epoch [3/5], step [318000/476702], loss: 0.0712, total loss: 843288.2812\n",
      "Epoch [3/5], step [319000/476702], loss: 2.7257, total loss: 845657.1676\n",
      "Epoch [3/5], step [320000/476702], loss: 7.2782, total loss: 848452.4495\n",
      "Epoch [3/5], step [321000/476702], loss: 2.1171, total loss: 850941.6023\n",
      "Epoch [3/5], step [322000/476702], loss: 0.0222, total loss: 853511.7514\n",
      "Epoch [3/5], step [323000/476702], loss: 2.9911, total loss: 856214.9948\n",
      "Epoch [3/5], step [324000/476702], loss: 1.7090, total loss: 858889.3104\n",
      "Epoch [3/5], step [325000/476702], loss: 3.0508, total loss: 861525.4941\n",
      "Epoch [3/5], step [326000/476702], loss: 0.3850, total loss: 864226.1811\n",
      "Epoch [3/5], step [327000/476702], loss: 4.5020, total loss: 867003.1677\n",
      "Epoch [3/5], step [328000/476702], loss: 0.0183, total loss: 869511.2014\n",
      "Epoch [3/5], step [329000/476702], loss: 3.3788, total loss: 872254.7340\n",
      "Epoch [3/5], step [330000/476702], loss: 3.7941, total loss: 874920.1160\n",
      "Epoch [3/5], step [331000/476702], loss: 1.6799, total loss: 877661.6971\n",
      "Epoch [3/5], step [332000/476702], loss: 0.6987, total loss: 880248.8118\n",
      "Epoch [3/5], step [333000/476702], loss: 2.5062, total loss: 882797.1154\n",
      "Epoch [3/5], step [334000/476702], loss: 5.8922, total loss: 885752.2308\n",
      "Epoch [3/5], step [335000/476702], loss: 5.9720, total loss: 888332.6465\n",
      "Epoch [3/5], step [336000/476702], loss: 0.4718, total loss: 891132.2292\n",
      "Epoch [3/5], step [337000/476702], loss: 1.1381, total loss: 893488.4755\n",
      "Epoch [3/5], step [338000/476702], loss: 0.0907, total loss: 895756.0859\n",
      "Epoch [3/5], step [339000/476702], loss: 0.0172, total loss: 898802.1833\n",
      "Epoch [3/5], step [340000/476702], loss: 1.4433, total loss: 901171.1777\n",
      "Epoch [3/5], step [341000/476702], loss: 4.4971, total loss: 904097.7652\n",
      "Epoch [3/5], step [342000/476702], loss: 0.6570, total loss: 907166.7187\n",
      "Epoch [3/5], step [343000/476702], loss: 0.0003, total loss: 909737.3430\n",
      "Epoch [3/5], step [344000/476702], loss: 8.5339, total loss: 912134.2851\n",
      "Epoch [3/5], step [345000/476702], loss: 4.9123, total loss: 914969.9442\n",
      "Epoch [3/5], step [346000/476702], loss: 2.3832, total loss: 917523.3955\n",
      "Epoch [3/5], step [347000/476702], loss: 0.1515, total loss: 920261.5640\n",
      "Epoch [3/5], step [348000/476702], loss: 0.6766, total loss: 922827.3558\n",
      "Epoch [3/5], step [349000/476702], loss: 9.8727, total loss: 925553.9259\n",
      "Epoch [3/5], step [350000/476702], loss: 1.5800, total loss: 928450.5712\n",
      "Epoch [3/5], step [351000/476702], loss: 0.0122, total loss: 931237.3325\n",
      "Epoch [3/5], step [352000/476702], loss: 6.7778, total loss: 934015.8762\n",
      "Epoch [3/5], step [353000/476702], loss: 8.0241, total loss: 936799.7547\n",
      "Epoch [3/5], step [354000/476702], loss: 1.5591, total loss: 939733.2453\n",
      "Epoch [3/5], step [355000/476702], loss: 2.3022, total loss: 942266.5473\n",
      "Epoch [3/5], step [356000/476702], loss: 0.3614, total loss: 944880.3800\n",
      "Epoch [3/5], step [357000/476702], loss: 0.0557, total loss: 947539.1461\n",
      "Epoch [3/5], step [358000/476702], loss: 0.9116, total loss: 950005.1863\n",
      "Epoch [3/5], step [359000/476702], loss: 0.9769, total loss: 952402.6659\n",
      "Epoch [3/5], step [360000/476702], loss: 0.0278, total loss: 955329.0385\n",
      "Epoch [3/5], step [361000/476702], loss: 2.5108, total loss: 958158.5352\n",
      "Epoch [3/5], step [362000/476702], loss: 3.3147, total loss: 960883.1296\n",
      "Epoch [3/5], step [363000/476702], loss: 0.4606, total loss: 963187.0109\n",
      "Epoch [3/5], step [364000/476702], loss: 0.2035, total loss: 965674.3204\n",
      "Epoch [3/5], step [365000/476702], loss: 0.2796, total loss: 968280.6333\n",
      "Epoch [3/5], step [366000/476702], loss: 5.2562, total loss: 970878.3418\n",
      "Epoch [3/5], step [367000/476702], loss: 0.2984, total loss: 973700.5127\n",
      "Epoch [3/5], step [368000/476702], loss: 2.0940, total loss: 976258.4098\n",
      "Epoch [3/5], step [369000/476702], loss: 4.1634, total loss: 978928.8884\n",
      "Epoch [3/5], step [370000/476702], loss: 3.6080, total loss: 981802.9923\n",
      "Epoch [3/5], step [371000/476702], loss: 5.2569, total loss: 984539.0350\n",
      "Epoch [3/5], step [372000/476702], loss: 2.6986, total loss: 987315.2886\n",
      "Epoch [3/5], step [373000/476702], loss: 0.0159, total loss: 989621.5155\n",
      "Epoch [3/5], step [374000/476702], loss: 9.4611, total loss: 992307.1679\n",
      "Epoch [3/5], step [375000/476702], loss: 1.2497, total loss: 994982.7387\n",
      "Epoch [3/5], step [376000/476702], loss: 0.0132, total loss: 997870.6862\n",
      "Epoch [3/5], step [377000/476702], loss: 6.4882, total loss: 1000800.7989\n",
      "Epoch [3/5], step [378000/476702], loss: 9.3053, total loss: 1003797.9740\n",
      "Epoch [3/5], step [379000/476702], loss: 2.2899, total loss: 1006722.5946\n",
      "Epoch [3/5], step [380000/476702], loss: 0.0461, total loss: 1009491.1128\n",
      "Epoch [3/5], step [381000/476702], loss: 10.1141, total loss: 1012368.9275\n",
      "Epoch [3/5], step [382000/476702], loss: 1.7563, total loss: 1015077.8694\n",
      "Epoch [3/5], step [383000/476702], loss: 0.1539, total loss: 1017424.2687\n",
      "Epoch [3/5], step [384000/476702], loss: 1.8487, total loss: 1020230.4265\n",
      "Epoch [3/5], step [385000/476702], loss: 0.0332, total loss: 1022958.4364\n",
      "Epoch [3/5], step [386000/476702], loss: 6.0355, total loss: 1025694.3071\n",
      "Epoch [3/5], step [387000/476702], loss: 4.0721, total loss: 1028170.6800\n",
      "Epoch [3/5], step [388000/476702], loss: 4.1964, total loss: 1030469.6977\n",
      "Epoch [3/5], step [389000/476702], loss: 0.0871, total loss: 1033137.3628\n",
      "Epoch [3/5], step [390000/476702], loss: 0.0765, total loss: 1036021.8637\n",
      "Epoch [3/5], step [391000/476702], loss: 4.8127, total loss: 1038702.8948\n",
      "Epoch [3/5], step [392000/476702], loss: 0.0038, total loss: 1041389.8889\n",
      "Epoch [3/5], step [393000/476702], loss: 0.0029, total loss: 1044052.2069\n",
      "Epoch [3/5], step [394000/476702], loss: 0.0112, total loss: 1046700.7328\n",
      "Epoch [3/5], step [395000/476702], loss: 4.2954, total loss: 1049305.9775\n",
      "Epoch [3/5], step [396000/476702], loss: 0.3432, total loss: 1051944.3100\n",
      "Epoch [3/5], step [397000/476702], loss: 3.4794, total loss: 1054559.2612\n",
      "Epoch [3/5], step [398000/476702], loss: 1.3853, total loss: 1057229.9879\n",
      "Epoch [3/5], step [399000/476702], loss: 3.3461, total loss: 1059911.6593\n",
      "Epoch [3/5], step [400000/476702], loss: 0.2163, total loss: 1062356.5172\n",
      "Epoch [3/5], step [401000/476702], loss: 6.1172, total loss: 1065340.2180\n",
      "Epoch [3/5], step [402000/476702], loss: 1.9288, total loss: 1067952.5696\n",
      "Epoch [3/5], step [403000/476702], loss: 0.3906, total loss: 1070921.9591\n",
      "Epoch [3/5], step [404000/476702], loss: 0.1066, total loss: 1073905.0408\n",
      "Epoch [3/5], step [405000/476702], loss: 4.9828, total loss: 1076502.1304\n",
      "Epoch [3/5], step [406000/476702], loss: 3.7579, total loss: 1079356.0185\n",
      "Epoch [3/5], step [407000/476702], loss: 0.0177, total loss: 1082097.9233\n",
      "Epoch [3/5], step [408000/476702], loss: 0.0946, total loss: 1084975.6169\n",
      "Epoch [3/5], step [409000/476702], loss: 2.1174, total loss: 1087693.4676\n",
      "Epoch [3/5], step [410000/476702], loss: 6.4387, total loss: 1090329.7803\n",
      "Epoch [3/5], step [411000/476702], loss: 1.8847, total loss: 1092830.0717\n",
      "Epoch [3/5], step [412000/476702], loss: 1.1683, total loss: 1095566.7943\n",
      "Epoch [3/5], step [413000/476702], loss: 2.9210, total loss: 1098060.4519\n",
      "Epoch [3/5], step [414000/476702], loss: 9.2607, total loss: 1100624.2954\n",
      "Epoch [3/5], step [415000/476702], loss: 0.5735, total loss: 1103242.7086\n",
      "Epoch [3/5], step [416000/476702], loss: 0.0716, total loss: 1106051.8359\n",
      "Epoch [3/5], step [417000/476702], loss: 0.6289, total loss: 1108620.2425\n",
      "Epoch [3/5], step [418000/476702], loss: 0.7042, total loss: 1111475.8746\n",
      "Epoch [3/5], step [419000/476702], loss: 7.2387, total loss: 1113773.9638\n",
      "Epoch [3/5], step [420000/476702], loss: 2.6712, total loss: 1116515.0863\n",
      "Epoch [3/5], step [421000/476702], loss: 0.0388, total loss: 1119178.3761\n",
      "Epoch [3/5], step [422000/476702], loss: 1.6989, total loss: 1121507.9963\n",
      "Epoch [3/5], step [423000/476702], loss: 0.0056, total loss: 1123957.3046\n",
      "Epoch [3/5], step [424000/476702], loss: 0.0854, total loss: 1126644.4024\n",
      "Epoch [3/5], step [425000/476702], loss: 8.2794, total loss: 1129250.9812\n",
      "Epoch [3/5], step [426000/476702], loss: 0.1386, total loss: 1131823.5435\n",
      "Epoch [3/5], step [427000/476702], loss: 0.4374, total loss: 1134288.3178\n",
      "Epoch [3/5], step [428000/476702], loss: 2.0619, total loss: 1136602.8542\n",
      "Epoch [3/5], step [429000/476702], loss: 0.2344, total loss: 1139229.0089\n",
      "Epoch [3/5], step [430000/476702], loss: 2.4703, total loss: 1141792.1333\n",
      "Epoch [3/5], step [431000/476702], loss: 0.7488, total loss: 1144220.1528\n",
      "Epoch [3/5], step [432000/476702], loss: 5.2773, total loss: 1146428.6828\n",
      "Epoch [3/5], step [433000/476702], loss: 0.1366, total loss: 1148751.2307\n",
      "Epoch [3/5], step [434000/476702], loss: 0.0020, total loss: 1150973.2804\n",
      "Epoch [3/5], step [435000/476702], loss: 3.9529, total loss: 1153483.6849\n",
      "Epoch [3/5], step [436000/476702], loss: 0.0733, total loss: 1156286.7500\n",
      "Epoch [3/5], step [437000/476702], loss: 1.6165, total loss: 1158717.2453\n",
      "Epoch [3/5], step [438000/476702], loss: 2.4741, total loss: 1161267.3523\n",
      "Epoch [3/5], step [439000/476702], loss: 8.9991, total loss: 1163796.4885\n",
      "Epoch [3/5], step [440000/476702], loss: 4.2111, total loss: 1166154.4600\n",
      "Epoch [3/5], step [441000/476702], loss: 14.6123, total loss: 1168615.5003\n",
      "Epoch [3/5], step [442000/476702], loss: 0.0040, total loss: 1171272.0086\n",
      "Epoch [3/5], step [443000/476702], loss: 1.7866, total loss: 1174004.6312\n",
      "Epoch [3/5], step [444000/476702], loss: 0.0548, total loss: 1176360.4305\n",
      "Epoch [3/5], step [445000/476702], loss: 3.7355, total loss: 1179101.5306\n",
      "Epoch [3/5], step [446000/476702], loss: 8.4577, total loss: 1181488.3104\n",
      "Epoch [3/5], step [447000/476702], loss: 5.0923, total loss: 1184290.6530\n",
      "Epoch [3/5], step [448000/476702], loss: 0.2391, total loss: 1186579.5749\n",
      "Epoch [3/5], step [449000/476702], loss: 1.7526, total loss: 1189498.2481\n",
      "Epoch [3/5], step [450000/476702], loss: 2.2069, total loss: 1191975.6245\n",
      "Epoch [3/5], step [451000/476702], loss: 1.7785, total loss: 1194594.5621\n",
      "Epoch [3/5], step [452000/476702], loss: 0.1164, total loss: 1197450.2701\n",
      "Epoch [3/5], step [453000/476702], loss: 2.4886, total loss: 1199905.1221\n",
      "Epoch [3/5], step [454000/476702], loss: 11.4081, total loss: 1202319.6125\n",
      "Epoch [3/5], step [455000/476702], loss: 4.3197, total loss: 1205091.6926\n",
      "Epoch [3/5], step [456000/476702], loss: 0.0276, total loss: 1207690.0590\n",
      "Epoch [3/5], step [457000/476702], loss: 0.3909, total loss: 1210184.0156\n",
      "Epoch [3/5], step [458000/476702], loss: 1.7436, total loss: 1212441.6575\n",
      "Epoch [3/5], step [459000/476702], loss: 0.3849, total loss: 1215300.5755\n",
      "Epoch [3/5], step [460000/476702], loss: 9.0044, total loss: 1217826.7228\n",
      "Epoch [3/5], step [461000/476702], loss: 0.1333, total loss: 1220291.6320\n",
      "Epoch [3/5], step [462000/476702], loss: 6.1014, total loss: 1222757.4097\n",
      "Epoch [3/5], step [463000/476702], loss: 1.5107, total loss: 1225101.9688\n",
      "Epoch [3/5], step [464000/476702], loss: 0.5349, total loss: 1227538.1061\n",
      "Epoch [3/5], step [465000/476702], loss: 3.9571, total loss: 1230027.2106\n",
      "Epoch [3/5], step [466000/476702], loss: 0.0506, total loss: 1232481.6083\n",
      "Epoch [3/5], step [467000/476702], loss: 3.0922, total loss: 1235218.0520\n",
      "Epoch [3/5], step [468000/476702], loss: 4.8757, total loss: 1237680.9068\n",
      "Epoch [3/5], step [469000/476702], loss: 0.1980, total loss: 1240139.8909\n",
      "Epoch [3/5], step [470000/476702], loss: 1.5256, total loss: 1242532.1099\n",
      "Epoch [3/5], step [471000/476702], loss: 0.1200, total loss: 1245319.9152\n",
      "Epoch [3/5], step [472000/476702], loss: 4.1282, total loss: 1247848.9137\n",
      "Epoch [3/5], step [473000/476702], loss: 0.3253, total loss: 1250550.5438\n",
      "Epoch [3/5], step [474000/476702], loss: 5.8412, total loss: 1253079.2715\n",
      "Epoch [3/5], step [475000/476702], loss: 7.2225, total loss: 1256019.4966\n",
      "Epoch [3/5], step [476000/476702], loss: 4.3960, total loss: 1258695.0993\n",
      "Epoch [4/5], step [1000/476702], loss: 1.9691, total loss: 2182.9502\n",
      "Epoch [4/5], step [2000/476702], loss: 0.4404, total loss: 4889.5383\n",
      "Epoch [4/5], step [3000/476702], loss: 1.3849, total loss: 7354.5049\n",
      "Epoch [4/5], step [4000/476702], loss: 1.9241, total loss: 9648.8256\n",
      "Epoch [4/5], step [5000/476702], loss: 2.0846, total loss: 12297.4070\n",
      "Epoch [4/5], step [6000/476702], loss: 1.8392, total loss: 14863.3338\n",
      "Epoch [4/5], step [7000/476702], loss: 0.7394, total loss: 17237.1750\n",
      "Epoch [4/5], step [8000/476702], loss: 1.9136, total loss: 19811.5164\n",
      "Epoch [4/5], step [9000/476702], loss: 1.5491, total loss: 22552.0752\n",
      "Epoch [4/5], step [10000/476702], loss: 0.8387, total loss: 24933.7165\n",
      "Epoch [4/5], step [11000/476702], loss: 1.7514, total loss: 27240.9305\n",
      "Epoch [4/5], step [12000/476702], loss: 0.0885, total loss: 29579.0342\n",
      "Epoch [4/5], step [13000/476702], loss: 0.0151, total loss: 32152.3239\n",
      "Epoch [4/5], step [14000/476702], loss: 1.2319, total loss: 34652.5395\n",
      "Epoch [4/5], step [15000/476702], loss: 0.0560, total loss: 37249.7969\n",
      "Epoch [4/5], step [16000/476702], loss: 0.5977, total loss: 39629.7783\n",
      "Epoch [4/5], step [17000/476702], loss: 2.4781, total loss: 42228.7972\n",
      "Epoch [4/5], step [18000/476702], loss: 1.6187, total loss: 44729.3982\n",
      "Epoch [4/5], step [19000/476702], loss: 6.1677, total loss: 47540.6947\n",
      "Epoch [4/5], step [20000/476702], loss: 3.8817, total loss: 50082.6093\n",
      "Epoch [4/5], step [21000/476702], loss: 0.1506, total loss: 52430.0383\n",
      "Epoch [4/5], step [22000/476702], loss: 3.0556, total loss: 54994.3730\n",
      "Epoch [4/5], step [23000/476702], loss: 1.6064, total loss: 57615.0722\n",
      "Epoch [4/5], step [24000/476702], loss: 3.2918, total loss: 60165.0895\n",
      "Epoch [4/5], step [25000/476702], loss: 8.6662, total loss: 62791.7720\n",
      "Epoch [4/5], step [26000/476702], loss: 2.4708, total loss: 65614.2062\n",
      "Epoch [4/5], step [27000/476702], loss: 6.5658, total loss: 67941.9882\n",
      "Epoch [4/5], step [28000/476702], loss: 10.5115, total loss: 70389.6332\n",
      "Epoch [4/5], step [29000/476702], loss: 5.2716, total loss: 72803.3991\n",
      "Epoch [4/5], step [30000/476702], loss: 0.0021, total loss: 75407.4495\n",
      "Epoch [4/5], step [31000/476702], loss: 0.4613, total loss: 77962.6667\n",
      "Epoch [4/5], step [32000/476702], loss: 12.4202, total loss: 80441.8182\n",
      "Epoch [4/5], step [33000/476702], loss: 3.3236, total loss: 83413.8838\n",
      "Epoch [4/5], step [34000/476702], loss: 5.7136, total loss: 85838.5848\n",
      "Epoch [4/5], step [35000/476702], loss: 0.1394, total loss: 88321.2504\n",
      "Epoch [4/5], step [36000/476702], loss: 0.9678, total loss: 91161.7629\n",
      "Epoch [4/5], step [37000/476702], loss: 2.1949, total loss: 93878.3391\n",
      "Epoch [4/5], step [38000/476702], loss: 0.1139, total loss: 96504.9902\n",
      "Epoch [4/5], step [39000/476702], loss: 0.0476, total loss: 98869.5407\n",
      "Epoch [4/5], step [40000/476702], loss: 3.7906, total loss: 101374.2749\n",
      "Epoch [4/5], step [41000/476702], loss: 0.3219, total loss: 103664.7894\n",
      "Epoch [4/5], step [42000/476702], loss: 2.0622, total loss: 106167.6432\n",
      "Epoch [4/5], step [43000/476702], loss: 3.7916, total loss: 108899.6120\n",
      "Epoch [4/5], step [44000/476702], loss: 0.0766, total loss: 111445.6497\n",
      "Epoch [4/5], step [45000/476702], loss: 0.0540, total loss: 114362.0283\n",
      "Epoch [4/5], step [46000/476702], loss: 3.8093, total loss: 116688.5771\n",
      "Epoch [4/5], step [47000/476702], loss: 0.9254, total loss: 119050.3124\n",
      "Epoch [4/5], step [48000/476702], loss: 3.0530, total loss: 121543.2105\n",
      "Epoch [4/5], step [49000/476702], loss: 0.0551, total loss: 124430.7866\n",
      "Epoch [4/5], step [50000/476702], loss: 0.0945, total loss: 127412.8776\n",
      "Epoch [4/5], step [51000/476702], loss: 0.6142, total loss: 130254.5850\n",
      "Epoch [4/5], step [52000/476702], loss: 0.1355, total loss: 132746.4887\n",
      "Epoch [4/5], step [53000/476702], loss: 0.5533, total loss: 135498.5595\n",
      "Epoch [4/5], step [54000/476702], loss: 2.5189, total loss: 137857.9734\n",
      "Epoch [4/5], step [55000/476702], loss: 2.1114, total loss: 140232.5533\n",
      "Epoch [4/5], step [56000/476702], loss: 3.4684, total loss: 142993.7757\n",
      "Epoch [4/5], step [57000/476702], loss: 0.4786, total loss: 145249.7964\n",
      "Epoch [4/5], step [58000/476702], loss: 0.0446, total loss: 148201.4635\n",
      "Epoch [4/5], step [59000/476702], loss: 0.1880, total loss: 150848.7100\n",
      "Epoch [4/5], step [60000/476702], loss: 3.4564, total loss: 153370.0043\n",
      "Epoch [4/5], step [61000/476702], loss: 1.1678, total loss: 155823.1126\n",
      "Epoch [4/5], step [62000/476702], loss: 0.1316, total loss: 158221.3183\n",
      "Epoch [4/5], step [63000/476702], loss: 0.2212, total loss: 160882.8176\n",
      "Epoch [4/5], step [64000/476702], loss: 0.0915, total loss: 163486.7770\n",
      "Epoch [4/5], step [65000/476702], loss: 0.2157, total loss: 166037.6936\n",
      "Epoch [4/5], step [66000/476702], loss: 0.1316, total loss: 168430.6968\n",
      "Epoch [4/5], step [67000/476702], loss: 3.6914, total loss: 170897.2191\n",
      "Epoch [4/5], step [68000/476702], loss: 1.1993, total loss: 173307.1024\n",
      "Epoch [4/5], step [69000/476702], loss: 0.3410, total loss: 175977.6877\n",
      "Epoch [4/5], step [70000/476702], loss: 6.5194, total loss: 178737.3802\n",
      "Epoch [4/5], step [71000/476702], loss: 12.8821, total loss: 181416.4658\n",
      "Epoch [4/5], step [72000/476702], loss: 3.1393, total loss: 184020.6592\n",
      "Epoch [4/5], step [73000/476702], loss: 0.5067, total loss: 186603.1361\n",
      "Epoch [4/5], step [74000/476702], loss: 0.5737, total loss: 189145.5769\n",
      "Epoch [4/5], step [75000/476702], loss: 0.1129, total loss: 191562.6380\n",
      "Epoch [4/5], step [76000/476702], loss: 0.5287, total loss: 193931.8132\n",
      "Epoch [4/5], step [77000/476702], loss: 0.0572, total loss: 196704.0651\n",
      "Epoch [4/5], step [78000/476702], loss: 2.5993, total loss: 199183.5155\n",
      "Epoch [4/5], step [79000/476702], loss: 10.0668, total loss: 201801.7890\n",
      "Epoch [4/5], step [80000/476702], loss: 10.2170, total loss: 204481.8868\n",
      "Epoch [4/5], step [81000/476702], loss: 2.0116, total loss: 207019.3847\n",
      "Epoch [4/5], step [82000/476702], loss: 0.2444, total loss: 209487.3546\n",
      "Epoch [4/5], step [83000/476702], loss: 1.8142, total loss: 212027.0510\n",
      "Epoch [4/5], step [84000/476702], loss: 1.6403, total loss: 214378.5471\n",
      "Epoch [4/5], step [85000/476702], loss: 3.2147, total loss: 216820.9285\n",
      "Epoch [4/5], step [86000/476702], loss: 0.0101, total loss: 219292.4110\n",
      "Epoch [4/5], step [87000/476702], loss: 5.5648, total loss: 221891.6395\n",
      "Epoch [4/5], step [88000/476702], loss: 2.6758, total loss: 223998.9979\n",
      "Epoch [4/5], step [89000/476702], loss: 0.3587, total loss: 225937.9495\n",
      "Epoch [4/5], step [90000/476702], loss: 5.1165, total loss: 228355.0781\n",
      "Epoch [4/5], step [91000/476702], loss: 5.9406, total loss: 230619.1597\n",
      "Epoch [4/5], step [92000/476702], loss: 0.0895, total loss: 233070.2435\n",
      "Epoch [4/5], step [93000/476702], loss: 0.0257, total loss: 235320.9410\n",
      "Epoch [4/5], step [94000/476702], loss: 4.8528, total loss: 237785.5664\n",
      "Epoch [4/5], step [95000/476702], loss: 4.2086, total loss: 240385.5689\n",
      "Epoch [4/5], step [96000/476702], loss: 3.5322, total loss: 242699.9042\n",
      "Epoch [4/5], step [97000/476702], loss: 7.3610, total loss: 245368.8164\n",
      "Epoch [4/5], step [98000/476702], loss: 2.7319, total loss: 247869.9380\n",
      "Epoch [4/5], step [99000/476702], loss: 0.8673, total loss: 250112.2673\n",
      "Epoch [4/5], step [100000/476702], loss: 4.5265, total loss: 252584.8777\n",
      "Epoch [4/5], step [101000/476702], loss: 0.2836, total loss: 255052.3123\n",
      "Epoch [4/5], step [102000/476702], loss: 3.2696, total loss: 257672.9229\n",
      "Epoch [4/5], step [103000/476702], loss: 2.2579, total loss: 260251.2142\n",
      "Epoch [4/5], step [104000/476702], loss: 3.0272, total loss: 262623.4848\n",
      "Epoch [4/5], step [105000/476702], loss: 0.9976, total loss: 265319.6640\n",
      "Epoch [4/5], step [106000/476702], loss: 0.6674, total loss: 268201.1230\n",
      "Epoch [4/5], step [107000/476702], loss: 10.7582, total loss: 270641.3362\n",
      "Epoch [4/5], step [108000/476702], loss: 0.0146, total loss: 273035.3690\n",
      "Epoch [4/5], step [109000/476702], loss: 1.4618, total loss: 275265.0744\n",
      "Epoch [4/5], step [110000/476702], loss: 0.8614, total loss: 277760.7737\n",
      "Epoch [4/5], step [111000/476702], loss: 0.0918, total loss: 279979.6997\n",
      "Epoch [4/5], step [112000/476702], loss: 3.3309, total loss: 282311.8034\n",
      "Epoch [4/5], step [113000/476702], loss: 0.4052, total loss: 284697.7402\n",
      "Epoch [4/5], step [114000/476702], loss: 1.3126, total loss: 287108.7172\n",
      "Epoch [4/5], step [115000/476702], loss: 0.6505, total loss: 289534.9599\n",
      "Epoch [4/5], step [116000/476702], loss: 0.7609, total loss: 292259.6252\n",
      "Epoch [4/5], step [117000/476702], loss: 0.7318, total loss: 294656.5757\n",
      "Epoch [4/5], step [118000/476702], loss: 0.0659, total loss: 296849.9712\n",
      "Epoch [4/5], step [119000/476702], loss: 0.5070, total loss: 299104.9151\n",
      "Epoch [4/5], step [120000/476702], loss: 0.0002, total loss: 301918.7855\n",
      "Epoch [4/5], step [121000/476702], loss: 10.2702, total loss: 304175.0934\n",
      "Epoch [4/5], step [122000/476702], loss: 0.2629, total loss: 306589.9673\n",
      "Epoch [4/5], step [123000/476702], loss: 8.4770, total loss: 309102.4480\n",
      "Epoch [4/5], step [124000/476702], loss: 5.5776, total loss: 311611.5719\n",
      "Epoch [4/5], step [125000/476702], loss: 3.7899, total loss: 313864.7697\n",
      "Epoch [4/5], step [126000/476702], loss: 3.7032, total loss: 316110.3631\n",
      "Epoch [4/5], step [127000/476702], loss: 0.0089, total loss: 318669.2811\n",
      "Epoch [4/5], step [128000/476702], loss: 4.3975, total loss: 321481.2020\n",
      "Epoch [4/5], step [129000/476702], loss: 10.2732, total loss: 323997.6906\n",
      "Epoch [4/5], step [130000/476702], loss: 1.8526, total loss: 326325.8689\n",
      "Epoch [4/5], step [131000/476702], loss: 0.7736, total loss: 328932.8948\n",
      "Epoch [4/5], step [132000/476702], loss: 0.4997, total loss: 331171.7431\n",
      "Epoch [4/5], step [133000/476702], loss: 2.2131, total loss: 333504.3202\n",
      "Epoch [4/5], step [134000/476702], loss: 0.0434, total loss: 335876.1358\n",
      "Epoch [4/5], step [135000/476702], loss: 0.8555, total loss: 338301.7250\n",
      "Epoch [4/5], step [136000/476702], loss: 0.4215, total loss: 340509.7294\n",
      "Epoch [4/5], step [137000/476702], loss: 7.6082, total loss: 342943.6094\n",
      "Epoch [4/5], step [138000/476702], loss: 4.7406, total loss: 345315.5623\n",
      "Epoch [4/5], step [139000/476702], loss: 1.7849, total loss: 347528.6639\n",
      "Epoch [4/5], step [140000/476702], loss: 4.6780, total loss: 349756.4284\n",
      "Epoch [4/5], step [141000/476702], loss: 3.2938, total loss: 351886.4297\n",
      "Epoch [4/5], step [142000/476702], loss: 4.1844, total loss: 354123.3431\n",
      "Epoch [4/5], step [143000/476702], loss: 0.2325, total loss: 356627.5486\n",
      "Epoch [4/5], step [144000/476702], loss: 1.6453, total loss: 359109.5683\n",
      "Epoch [4/5], step [145000/476702], loss: 0.3475, total loss: 361680.8437\n",
      "Epoch [4/5], step [146000/476702], loss: 0.1154, total loss: 364211.8691\n",
      "Epoch [4/5], step [147000/476702], loss: 2.9484, total loss: 366606.3937\n",
      "Epoch [4/5], step [148000/476702], loss: 4.1387, total loss: 369166.7397\n",
      "Epoch [4/5], step [149000/476702], loss: 0.2126, total loss: 371508.6199\n",
      "Epoch [4/5], step [150000/476702], loss: 1.5494, total loss: 373681.7176\n",
      "Epoch [4/5], step [151000/476702], loss: 8.2105, total loss: 376421.2386\n",
      "Epoch [4/5], step [152000/476702], loss: 1.2032, total loss: 379078.1025\n",
      "Epoch [4/5], step [153000/476702], loss: 0.0030, total loss: 381860.6320\n",
      "Epoch [4/5], step [154000/476702], loss: 7.2691, total loss: 384542.4964\n",
      "Epoch [4/5], step [155000/476702], loss: 3.3968, total loss: 387136.6035\n",
      "Epoch [4/5], step [156000/476702], loss: 0.8130, total loss: 389457.3979\n",
      "Epoch [4/5], step [157000/476702], loss: 2.1352, total loss: 392003.2269\n",
      "Epoch [4/5], step [158000/476702], loss: 0.8239, total loss: 394777.8998\n",
      "Epoch [4/5], step [159000/476702], loss: 0.9807, total loss: 397484.8278\n",
      "Epoch [4/5], step [160000/476702], loss: 1.9720, total loss: 399887.0573\n",
      "Epoch [4/5], step [161000/476702], loss: 0.0036, total loss: 402473.5980\n",
      "Epoch [4/5], step [162000/476702], loss: 0.0130, total loss: 404820.5626\n",
      "Epoch [4/5], step [163000/476702], loss: 11.2175, total loss: 407516.5795\n",
      "Epoch [4/5], step [164000/476702], loss: 0.1315, total loss: 410175.8445\n",
      "Epoch [4/5], step [165000/476702], loss: 4.0400, total loss: 412445.5318\n",
      "Epoch [4/5], step [166000/476702], loss: 0.3854, total loss: 414545.0196\n",
      "Epoch [4/5], step [167000/476702], loss: 0.0454, total loss: 417033.0377\n",
      "Epoch [4/5], step [168000/476702], loss: 1.5285, total loss: 419656.6504\n",
      "Epoch [4/5], step [169000/476702], loss: 0.9303, total loss: 422249.7184\n",
      "Epoch [4/5], step [170000/476702], loss: 2.2538, total loss: 424674.8504\n",
      "Epoch [4/5], step [171000/476702], loss: 0.1416, total loss: 426994.0000\n",
      "Epoch [4/5], step [172000/476702], loss: 0.0843, total loss: 429596.8534\n",
      "Epoch [4/5], step [173000/476702], loss: 3.2720, total loss: 432062.5187\n",
      "Epoch [4/5], step [174000/476702], loss: 0.6350, total loss: 434618.9529\n",
      "Epoch [4/5], step [175000/476702], loss: 0.0140, total loss: 436766.1477\n",
      "Epoch [4/5], step [176000/476702], loss: 2.1286, total loss: 439108.6646\n",
      "Epoch [4/5], step [177000/476702], loss: 2.4818, total loss: 441629.3703\n",
      "Epoch [4/5], step [178000/476702], loss: 0.1084, total loss: 444131.2147\n",
      "Epoch [4/5], step [179000/476702], loss: 2.0986, total loss: 446828.1729\n",
      "Epoch [4/5], step [180000/476702], loss: 4.6986, total loss: 449245.0505\n",
      "Epoch [4/5], step [181000/476702], loss: 1.9829, total loss: 451950.8365\n",
      "Epoch [4/5], step [182000/476702], loss: 0.1784, total loss: 454467.6101\n",
      "Epoch [4/5], step [183000/476702], loss: 11.2347, total loss: 456806.8475\n",
      "Epoch [4/5], step [184000/476702], loss: 5.0446, total loss: 459119.9099\n",
      "Epoch [4/5], step [185000/476702], loss: 0.0636, total loss: 461389.8171\n",
      "Epoch [4/5], step [186000/476702], loss: 4.3549, total loss: 463681.3165\n",
      "Epoch [4/5], step [187000/476702], loss: 2.5609, total loss: 465996.3531\n",
      "Epoch [4/5], step [188000/476702], loss: 2.1745, total loss: 468604.4619\n",
      "Epoch [4/5], step [189000/476702], loss: 5.9304, total loss: 471275.2638\n",
      "Epoch [4/5], step [190000/476702], loss: 3.5547, total loss: 473364.3167\n",
      "Epoch [4/5], step [191000/476702], loss: 4.7644, total loss: 476133.4135\n",
      "Epoch [4/5], step [192000/476702], loss: 1.8059, total loss: 478676.4284\n",
      "Epoch [4/5], step [193000/476702], loss: 0.3062, total loss: 481198.3603\n",
      "Epoch [4/5], step [194000/476702], loss: 11.8259, total loss: 483437.0271\n",
      "Epoch [4/5], step [195000/476702], loss: 3.6502, total loss: 485900.6515\n",
      "Epoch [4/5], step [196000/476702], loss: 3.4527, total loss: 488369.6609\n",
      "Epoch [4/5], step [197000/476702], loss: 0.6174, total loss: 490604.3650\n",
      "Epoch [4/5], step [198000/476702], loss: 0.1771, total loss: 493401.1529\n",
      "Epoch [4/5], step [199000/476702], loss: 2.3821, total loss: 496058.0323\n",
      "Epoch [4/5], step [200000/476702], loss: 5.6294, total loss: 498666.5722\n",
      "Epoch [4/5], step [201000/476702], loss: 0.7109, total loss: 501393.3664\n",
      "Epoch [4/5], step [202000/476702], loss: 0.3771, total loss: 503930.5013\n",
      "Epoch [4/5], step [203000/476702], loss: 4.2501, total loss: 506709.7375\n",
      "Epoch [4/5], step [204000/476702], loss: 0.3410, total loss: 509167.7532\n",
      "Epoch [4/5], step [205000/476702], loss: 5.8227, total loss: 511754.6162\n",
      "Epoch [4/5], step [206000/476702], loss: 0.7600, total loss: 514023.4957\n",
      "Epoch [4/5], step [207000/476702], loss: 3.6759, total loss: 516561.3923\n",
      "Epoch [4/5], step [208000/476702], loss: 0.6153, total loss: 519157.3193\n",
      "Epoch [4/5], step [209000/476702], loss: 2.1355, total loss: 521635.6581\n",
      "Epoch [4/5], step [210000/476702], loss: 0.7486, total loss: 524226.1475\n",
      "Epoch [4/5], step [211000/476702], loss: 0.8179, total loss: 526885.5228\n",
      "Epoch [4/5], step [212000/476702], loss: 6.5552, total loss: 529387.0971\n",
      "Epoch [4/5], step [213000/476702], loss: 0.7871, total loss: 531648.7664\n",
      "Epoch [4/5], step [214000/476702], loss: 1.7788, total loss: 534269.9779\n",
      "Epoch [4/5], step [215000/476702], loss: 0.3591, total loss: 536855.5960\n",
      "Epoch [4/5], step [216000/476702], loss: 0.6578, total loss: 539862.6617\n",
      "Epoch [4/5], step [217000/476702], loss: 0.0018, total loss: 542964.2460\n",
      "Epoch [4/5], step [218000/476702], loss: 1.0538, total loss: 545890.7736\n",
      "Epoch [4/5], step [219000/476702], loss: 0.4492, total loss: 548871.7163\n",
      "Epoch [4/5], step [220000/476702], loss: 0.1117, total loss: 551625.3763\n",
      "Epoch [4/5], step [221000/476702], loss: 1.2134, total loss: 553910.6187\n",
      "Epoch [4/5], step [222000/476702], loss: 0.0050, total loss: 556424.5383\n",
      "Epoch [4/5], step [223000/476702], loss: 0.2256, total loss: 558816.6211\n",
      "Epoch [4/5], step [224000/476702], loss: 0.6821, total loss: 561187.5534\n",
      "Epoch [4/5], step [225000/476702], loss: 0.0661, total loss: 563642.3886\n",
      "Epoch [4/5], step [226000/476702], loss: 0.0486, total loss: 566088.6024\n",
      "Epoch [4/5], step [227000/476702], loss: 0.0077, total loss: 568618.1750\n",
      "Epoch [4/5], step [228000/476702], loss: 0.8075, total loss: 571530.2678\n",
      "Epoch [4/5], step [229000/476702], loss: 5.4973, total loss: 574241.8735\n",
      "Epoch [4/5], step [230000/476702], loss: 0.0370, total loss: 576986.7008\n",
      "Epoch [4/5], step [231000/476702], loss: 3.9540, total loss: 579414.5136\n",
      "Epoch [4/5], step [232000/476702], loss: 0.2724, total loss: 581710.5359\n",
      "Epoch [4/5], step [233000/476702], loss: 0.0496, total loss: 584195.8368\n",
      "Epoch [4/5], step [234000/476702], loss: 1.4657, total loss: 586632.7736\n",
      "Epoch [4/5], step [235000/476702], loss: 0.3351, total loss: 589384.7998\n",
      "Epoch [4/5], step [236000/476702], loss: 0.0031, total loss: 592024.3536\n",
      "Epoch [4/5], step [237000/476702], loss: 2.8518, total loss: 595014.1983\n",
      "Epoch [4/5], step [238000/476702], loss: 3.8864, total loss: 597903.6991\n",
      "Epoch [4/5], step [239000/476702], loss: 0.8750, total loss: 600565.3486\n",
      "Epoch [4/5], step [240000/476702], loss: 1.3521, total loss: 603303.8936\n",
      "Epoch [4/5], step [241000/476702], loss: 1.8714, total loss: 605655.3673\n",
      "Epoch [4/5], step [242000/476702], loss: 0.5473, total loss: 608528.7320\n",
      "Epoch [4/5], step [243000/476702], loss: 0.0037, total loss: 611025.7165\n",
      "Epoch [4/5], step [244000/476702], loss: 0.0046, total loss: 613327.6675\n",
      "Epoch [4/5], step [245000/476702], loss: 1.1037, total loss: 616046.5041\n",
      "Epoch [4/5], step [246000/476702], loss: 8.9222, total loss: 618597.9948\n",
      "Epoch [4/5], step [247000/476702], loss: 5.8930, total loss: 621198.3627\n",
      "Epoch [4/5], step [248000/476702], loss: 1.6181, total loss: 623547.8820\n",
      "Epoch [4/5], step [249000/476702], loss: 3.1616, total loss: 626180.6992\n",
      "Epoch [4/5], step [250000/476702], loss: 4.7476, total loss: 628548.4832\n",
      "Epoch [4/5], step [251000/476702], loss: 5.5396, total loss: 630902.7468\n",
      "Epoch [4/5], step [252000/476702], loss: 3.2025, total loss: 633147.6023\n",
      "Epoch [4/5], step [253000/476702], loss: 4.8861, total loss: 635848.9528\n",
      "Epoch [4/5], step [254000/476702], loss: 3.6942, total loss: 638271.9196\n",
      "Epoch [4/5], step [255000/476702], loss: 1.0745, total loss: 640746.8180\n",
      "Epoch [4/5], step [256000/476702], loss: 0.0169, total loss: 643258.2995\n",
      "Epoch [4/5], step [257000/476702], loss: 0.2872, total loss: 645829.5110\n",
      "Epoch [4/5], step [258000/476702], loss: 6.3786, total loss: 648409.6761\n",
      "Epoch [4/5], step [259000/476702], loss: 2.1102, total loss: 651223.0693\n",
      "Epoch [4/5], step [260000/476702], loss: 3.7601, total loss: 653808.0897\n",
      "Epoch [4/5], step [261000/476702], loss: 2.0923, total loss: 656256.1395\n",
      "Epoch [4/5], step [262000/476702], loss: 1.8429, total loss: 658698.1900\n",
      "Epoch [4/5], step [263000/476702], loss: 0.0297, total loss: 661548.0729\n",
      "Epoch [4/5], step [264000/476702], loss: 0.0004, total loss: 663968.6337\n",
      "Epoch [4/5], step [265000/476702], loss: 0.0942, total loss: 666797.4191\n",
      "Epoch [4/5], step [266000/476702], loss: 0.0240, total loss: 669725.8249\n",
      "Epoch [4/5], step [267000/476702], loss: 0.6291, total loss: 672516.4258\n",
      "Epoch [4/5], step [268000/476702], loss: 2.5923, total loss: 675074.9827\n",
      "Epoch [4/5], step [269000/476702], loss: 3.4691, total loss: 677778.9617\n",
      "Epoch [4/5], step [270000/476702], loss: 0.0231, total loss: 680308.0469\n",
      "Epoch [4/5], step [271000/476702], loss: 5.4878, total loss: 682821.5874\n",
      "Epoch [4/5], step [272000/476702], loss: 2.7949, total loss: 685306.0496\n",
      "Epoch [4/5], step [273000/476702], loss: 0.0157, total loss: 688073.4363\n",
      "Epoch [4/5], step [274000/476702], loss: 0.0014, total loss: 690455.8436\n",
      "Epoch [4/5], step [275000/476702], loss: 11.2329, total loss: 693055.6225\n",
      "Epoch [4/5], step [276000/476702], loss: 0.5857, total loss: 695481.6714\n",
      "Epoch [4/5], step [277000/476702], loss: 9.6089, total loss: 698351.8086\n",
      "Epoch [4/5], step [278000/476702], loss: 1.2596, total loss: 700969.3757\n",
      "Epoch [4/5], step [279000/476702], loss: 2.8319, total loss: 703427.1046\n",
      "Epoch [4/5], step [280000/476702], loss: 11.4296, total loss: 706207.4652\n",
      "Epoch [4/5], step [281000/476702], loss: 0.1014, total loss: 708968.4686\n",
      "Epoch [4/5], step [282000/476702], loss: 3.1457, total loss: 711456.7313\n",
      "Epoch [4/5], step [283000/476702], loss: 4.2801, total loss: 714020.9369\n",
      "Epoch [4/5], step [284000/476702], loss: 0.2751, total loss: 716628.4654\n",
      "Epoch [4/5], step [285000/476702], loss: 0.4324, total loss: 719108.0627\n",
      "Epoch [4/5], step [286000/476702], loss: 3.1752, total loss: 721812.1780\n",
      "Epoch [4/5], step [287000/476702], loss: 0.3156, total loss: 724098.7366\n",
      "Epoch [4/5], step [288000/476702], loss: 0.9378, total loss: 727025.6086\n",
      "Epoch [4/5], step [289000/476702], loss: 0.0029, total loss: 729311.6982\n",
      "Epoch [4/5], step [290000/476702], loss: 1.8225, total loss: 732120.4390\n",
      "Epoch [4/5], step [291000/476702], loss: 1.6257, total loss: 734667.3850\n",
      "Epoch [4/5], step [292000/476702], loss: 0.0870, total loss: 737512.6516\n",
      "Epoch [4/5], step [293000/476702], loss: 0.0657, total loss: 739909.9451\n",
      "Epoch [4/5], step [294000/476702], loss: 5.4378, total loss: 742394.5885\n",
      "Epoch [4/5], step [295000/476702], loss: 0.0262, total loss: 745071.6781\n",
      "Epoch [4/5], step [296000/476702], loss: 0.0561, total loss: 747413.2416\n",
      "Epoch [4/5], step [297000/476702], loss: 0.7591, total loss: 749914.0307\n",
      "Epoch [4/5], step [298000/476702], loss: 1.0581, total loss: 752378.7825\n",
      "Epoch [4/5], step [299000/476702], loss: 1.9390, total loss: 754801.5128\n",
      "Epoch [4/5], step [300000/476702], loss: 0.3171, total loss: 757336.6030\n",
      "Epoch [4/5], step [301000/476702], loss: 0.9163, total loss: 759942.0084\n",
      "Epoch [4/5], step [302000/476702], loss: 2.3558, total loss: 762625.5298\n",
      "Epoch [4/5], step [303000/476702], loss: 0.1994, total loss: 765053.5539\n",
      "Epoch [4/5], step [304000/476702], loss: 0.4766, total loss: 767505.7048\n",
      "Epoch [4/5], step [305000/476702], loss: 1.1537, total loss: 769931.3864\n",
      "Epoch [4/5], step [306000/476702], loss: 0.7506, total loss: 772325.2490\n",
      "Epoch [4/5], step [307000/476702], loss: 0.0390, total loss: 775013.9342\n",
      "Epoch [4/5], step [308000/476702], loss: 0.1977, total loss: 777742.1260\n",
      "Epoch [4/5], step [309000/476702], loss: 2.4160, total loss: 780019.7714\n",
      "Epoch [4/5], step [310000/476702], loss: 5.3577, total loss: 782672.8545\n",
      "Epoch [4/5], step [311000/476702], loss: 4.8585, total loss: 784914.2193\n",
      "Epoch [4/5], step [312000/476702], loss: 3.5286, total loss: 787805.5936\n",
      "Epoch [4/5], step [313000/476702], loss: 3.5978, total loss: 790391.3454\n",
      "Epoch [4/5], step [314000/476702], loss: 2.6875, total loss: 792701.4150\n",
      "Epoch [4/5], step [315000/476702], loss: 1.8036, total loss: 795223.0629\n",
      "Epoch [4/5], step [316000/476702], loss: 1.1964, total loss: 797920.8984\n",
      "Epoch [4/5], step [317000/476702], loss: 0.0178, total loss: 800282.4641\n",
      "Epoch [4/5], step [318000/476702], loss: 0.0543, total loss: 802701.3876\n",
      "Epoch [4/5], step [319000/476702], loss: 2.5664, total loss: 804969.2620\n",
      "Epoch [4/5], step [320000/476702], loss: 7.5757, total loss: 807643.1475\n",
      "Epoch [4/5], step [321000/476702], loss: 2.0101, total loss: 810029.2685\n",
      "Epoch [4/5], step [322000/476702], loss: 0.0239, total loss: 812495.6049\n",
      "Epoch [4/5], step [323000/476702], loss: 2.7875, total loss: 815076.0590\n",
      "Epoch [4/5], step [324000/476702], loss: 1.6246, total loss: 817615.7523\n",
      "Epoch [4/5], step [325000/476702], loss: 2.4523, total loss: 820154.7974\n",
      "Epoch [4/5], step [326000/476702], loss: 0.4063, total loss: 822744.5409\n",
      "Epoch [4/5], step [327000/476702], loss: 4.2958, total loss: 825385.7171\n",
      "Epoch [4/5], step [328000/476702], loss: 0.0113, total loss: 827797.7763\n",
      "Epoch [4/5], step [329000/476702], loss: 1.9187, total loss: 830424.3831\n",
      "Epoch [4/5], step [330000/476702], loss: 3.7216, total loss: 832985.7047\n",
      "Epoch [4/5], step [331000/476702], loss: 1.7083, total loss: 835601.5215\n",
      "Epoch [4/5], step [332000/476702], loss: 0.7155, total loss: 838084.7302\n",
      "Epoch [4/5], step [333000/476702], loss: 2.0270, total loss: 840497.2461\n",
      "Epoch [4/5], step [334000/476702], loss: 5.4175, total loss: 843347.2922\n",
      "Epoch [4/5], step [335000/476702], loss: 5.4585, total loss: 845822.2462\n",
      "Epoch [4/5], step [336000/476702], loss: 0.3886, total loss: 848490.1150\n",
      "Epoch [4/5], step [337000/476702], loss: 1.2203, total loss: 850751.5070\n",
      "Epoch [4/5], step [338000/476702], loss: 0.0938, total loss: 852910.2972\n",
      "Epoch [4/5], step [339000/476702], loss: 0.0185, total loss: 855819.0994\n",
      "Epoch [4/5], step [340000/476702], loss: 1.7922, total loss: 858103.3078\n",
      "Epoch [4/5], step [341000/476702], loss: 4.5356, total loss: 860906.1463\n",
      "Epoch [4/5], step [342000/476702], loss: 0.5630, total loss: 863827.5741\n",
      "Epoch [4/5], step [343000/476702], loss: 0.0002, total loss: 866305.1490\n",
      "Epoch [4/5], step [344000/476702], loss: 8.3230, total loss: 868606.5583\n",
      "Epoch [4/5], step [345000/476702], loss: 4.6803, total loss: 871308.5433\n",
      "Epoch [4/5], step [346000/476702], loss: 1.7832, total loss: 873753.7339\n",
      "Epoch [4/5], step [347000/476702], loss: 0.1788, total loss: 876366.8627\n",
      "Epoch [4/5], step [348000/476702], loss: 0.6538, total loss: 878821.3731\n",
      "Epoch [4/5], step [349000/476702], loss: 10.3249, total loss: 881426.0644\n",
      "Epoch [4/5], step [350000/476702], loss: 1.3955, total loss: 884199.9838\n",
      "Epoch [4/5], step [351000/476702], loss: 0.0102, total loss: 886858.4958\n",
      "Epoch [4/5], step [352000/476702], loss: 5.5149, total loss: 889521.3561\n",
      "Epoch [4/5], step [353000/476702], loss: 7.5550, total loss: 892188.2545\n",
      "Epoch [4/5], step [354000/476702], loss: 1.6099, total loss: 894984.6663\n",
      "Epoch [4/5], step [355000/476702], loss: 2.1380, total loss: 897423.9422\n",
      "Epoch [4/5], step [356000/476702], loss: 0.3470, total loss: 899913.8199\n",
      "Epoch [4/5], step [357000/476702], loss: 0.0336, total loss: 902454.6274\n",
      "Epoch [4/5], step [358000/476702], loss: 0.9399, total loss: 904818.3216\n",
      "Epoch [4/5], step [359000/476702], loss: 1.2797, total loss: 907116.4504\n",
      "Epoch [4/5], step [360000/476702], loss: 0.0410, total loss: 909948.2987\n",
      "Epoch [4/5], step [361000/476702], loss: 2.3236, total loss: 912649.3186\n",
      "Epoch [4/5], step [362000/476702], loss: 3.1536, total loss: 915257.2055\n",
      "Epoch [4/5], step [363000/476702], loss: 0.4029, total loss: 917445.6677\n",
      "Epoch [4/5], step [364000/476702], loss: 0.1937, total loss: 919820.6225\n",
      "Epoch [4/5], step [365000/476702], loss: 0.3700, total loss: 922313.0905\n",
      "Epoch [4/5], step [366000/476702], loss: 5.0584, total loss: 924792.0113\n",
      "Epoch [4/5], step [367000/476702], loss: 0.3212, total loss: 927505.1322\n",
      "Epoch [4/5], step [368000/476702], loss: 1.9102, total loss: 929949.2988\n",
      "Epoch [4/5], step [369000/476702], loss: 3.8405, total loss: 932527.1643\n",
      "Epoch [4/5], step [370000/476702], loss: 3.3649, total loss: 935286.3985\n",
      "Epoch [4/5], step [371000/476702], loss: 5.4516, total loss: 937902.3621\n",
      "Epoch [4/5], step [372000/476702], loss: 2.5004, total loss: 940542.7283\n",
      "Epoch [4/5], step [373000/476702], loss: 0.0135, total loss: 942744.6460\n",
      "Epoch [4/5], step [374000/476702], loss: 8.0542, total loss: 945329.2753\n",
      "Epoch [4/5], step [375000/476702], loss: 1.1578, total loss: 947899.2726\n",
      "Epoch [4/5], step [376000/476702], loss: 0.0152, total loss: 950641.0591\n",
      "Epoch [4/5], step [377000/476702], loss: 5.9981, total loss: 953427.4151\n",
      "Epoch [4/5], step [378000/476702], loss: 9.7410, total loss: 956283.2694\n",
      "Epoch [4/5], step [379000/476702], loss: 2.3090, total loss: 959093.1632\n",
      "Epoch [4/5], step [380000/476702], loss: 0.0545, total loss: 961752.6149\n",
      "Epoch [4/5], step [381000/476702], loss: 9.6091, total loss: 964498.4370\n",
      "Epoch [4/5], step [382000/476702], loss: 1.5380, total loss: 967102.6733\n",
      "Epoch [4/5], step [383000/476702], loss: 0.1801, total loss: 969350.9144\n",
      "Epoch [4/5], step [384000/476702], loss: 1.9558, total loss: 972048.0874\n",
      "Epoch [4/5], step [385000/476702], loss: 0.0364, total loss: 974686.0405\n",
      "Epoch [4/5], step [386000/476702], loss: 5.6881, total loss: 977281.0060\n",
      "Epoch [4/5], step [387000/476702], loss: 3.8462, total loss: 979666.4049\n",
      "Epoch [4/5], step [388000/476702], loss: 4.1636, total loss: 981873.2899\n",
      "Epoch [4/5], step [389000/476702], loss: 0.0800, total loss: 984442.9061\n",
      "Epoch [4/5], step [390000/476702], loss: 0.0662, total loss: 987206.7470\n",
      "Epoch [4/5], step [391000/476702], loss: 4.4614, total loss: 989785.9848\n",
      "Epoch [4/5], step [392000/476702], loss: 0.0052, total loss: 992366.1418\n",
      "Epoch [4/5], step [393000/476702], loss: 0.0022, total loss: 994905.6753\n",
      "Epoch [4/5], step [394000/476702], loss: 0.0164, total loss: 997441.5478\n",
      "Epoch [4/5], step [395000/476702], loss: 4.1549, total loss: 999941.7354\n",
      "Epoch [4/5], step [396000/476702], loss: 0.2720, total loss: 1002476.7165\n",
      "Epoch [4/5], step [397000/476702], loss: 3.1012, total loss: 1004987.5014\n",
      "Epoch [4/5], step [398000/476702], loss: 1.3516, total loss: 1007528.3726\n",
      "Epoch [4/5], step [399000/476702], loss: 3.5211, total loss: 1010091.9376\n",
      "Epoch [4/5], step [400000/476702], loss: 0.1485, total loss: 1012430.2839\n",
      "Epoch [4/5], step [401000/476702], loss: 5.7213, total loss: 1015275.1886\n",
      "Epoch [4/5], step [402000/476702], loss: 1.9622, total loss: 1017774.8249\n",
      "Epoch [4/5], step [403000/476702], loss: 0.4196, total loss: 1020600.8878\n",
      "Epoch [4/5], step [404000/476702], loss: 0.1141, total loss: 1023456.6482\n",
      "Epoch [4/5], step [405000/476702], loss: 4.7217, total loss: 1025948.4608\n",
      "Epoch [4/5], step [406000/476702], loss: 3.4579, total loss: 1028677.4983\n",
      "Epoch [4/5], step [407000/476702], loss: 0.0125, total loss: 1031310.1017\n",
      "Epoch [4/5], step [408000/476702], loss: 0.0421, total loss: 1034048.4998\n",
      "Epoch [4/5], step [409000/476702], loss: 2.3389, total loss: 1036664.7860\n",
      "Epoch [4/5], step [410000/476702], loss: 5.0379, total loss: 1039200.4893\n",
      "Epoch [4/5], step [411000/476702], loss: 1.5917, total loss: 1041594.7810\n",
      "Epoch [4/5], step [412000/476702], loss: 0.9664, total loss: 1044208.2512\n",
      "Epoch [4/5], step [413000/476702], loss: 2.6223, total loss: 1046576.7207\n",
      "Epoch [4/5], step [414000/476702], loss: 8.9473, total loss: 1049026.5802\n",
      "Epoch [4/5], step [415000/476702], loss: 0.5148, total loss: 1051521.0587\n",
      "Epoch [4/5], step [416000/476702], loss: 0.0940, total loss: 1054207.5624\n",
      "Epoch [4/5], step [417000/476702], loss: 0.5756, total loss: 1056683.4997\n",
      "Epoch [4/5], step [418000/476702], loss: 0.7931, total loss: 1059405.2545\n",
      "Epoch [4/5], step [419000/476702], loss: 7.3574, total loss: 1061624.8124\n",
      "Epoch [4/5], step [420000/476702], loss: 2.4219, total loss: 1064260.9826\n",
      "Epoch [4/5], step [421000/476702], loss: 0.0662, total loss: 1066814.0555\n",
      "Epoch [4/5], step [422000/476702], loss: 1.6889, total loss: 1069048.3630\n",
      "Epoch [4/5], step [423000/476702], loss: 0.0035, total loss: 1071407.1311\n",
      "Epoch [4/5], step [424000/476702], loss: 0.0766, total loss: 1073978.6133\n",
      "Epoch [4/5], step [425000/476702], loss: 8.1841, total loss: 1076484.6309\n",
      "Epoch [4/5], step [426000/476702], loss: 0.1312, total loss: 1078976.0523\n",
      "Epoch [4/5], step [427000/476702], loss: 0.4227, total loss: 1081345.2302\n",
      "Epoch [4/5], step [428000/476702], loss: 2.0405, total loss: 1083564.5131\n",
      "Epoch [4/5], step [429000/476702], loss: 0.2217, total loss: 1086085.1009\n",
      "Epoch [4/5], step [430000/476702], loss: 1.9271, total loss: 1088548.0444\n",
      "Epoch [4/5], step [431000/476702], loss: 0.6875, total loss: 1090875.7076\n",
      "Epoch [4/5], step [432000/476702], loss: 3.2734, total loss: 1092988.3072\n",
      "Epoch [4/5], step [433000/476702], loss: 0.1437, total loss: 1095227.0147\n",
      "Epoch [4/5], step [434000/476702], loss: 0.0013, total loss: 1097350.3676\n",
      "Epoch [4/5], step [435000/476702], loss: 4.1675, total loss: 1099744.5634\n",
      "Epoch [4/5], step [436000/476702], loss: 0.0488, total loss: 1102407.0033\n",
      "Epoch [4/5], step [437000/476702], loss: 1.7713, total loss: 1104730.7910\n",
      "Epoch [4/5], step [438000/476702], loss: 2.2890, total loss: 1107159.5930\n",
      "Epoch [4/5], step [439000/476702], loss: 9.0378, total loss: 1109600.6590\n",
      "Epoch [4/5], step [440000/476702], loss: 4.1384, total loss: 1111855.2583\n",
      "Epoch [4/5], step [441000/476702], loss: 14.6485, total loss: 1114218.3488\n",
      "Epoch [4/5], step [442000/476702], loss: 0.0030, total loss: 1116763.2859\n",
      "Epoch [4/5], step [443000/476702], loss: 1.6415, total loss: 1119391.5457\n",
      "Epoch [4/5], step [444000/476702], loss: 0.0605, total loss: 1121657.6440\n",
      "Epoch [4/5], step [445000/476702], loss: 3.8343, total loss: 1124283.8828\n",
      "Epoch [4/5], step [446000/476702], loss: 7.9933, total loss: 1126545.8926\n",
      "Epoch [4/5], step [447000/476702], loss: 4.7960, total loss: 1129235.3058\n",
      "Epoch [4/5], step [448000/476702], loss: 0.2608, total loss: 1131418.5538\n",
      "Epoch [4/5], step [449000/476702], loss: 1.5539, total loss: 1134197.2481\n",
      "Epoch [4/5], step [450000/476702], loss: 2.0008, total loss: 1136565.7592\n",
      "Epoch [4/5], step [451000/476702], loss: 1.6379, total loss: 1139063.6436\n",
      "Epoch [4/5], step [452000/476702], loss: 0.1035, total loss: 1141822.6261\n",
      "Epoch [4/5], step [453000/476702], loss: 2.4859, total loss: 1144169.5105\n",
      "Epoch [4/5], step [454000/476702], loss: 10.8292, total loss: 1146480.8684\n",
      "Epoch [4/5], step [455000/476702], loss: 4.2306, total loss: 1149138.1091\n",
      "Epoch [4/5], step [456000/476702], loss: 0.0213, total loss: 1151653.0172\n",
      "Epoch [4/5], step [457000/476702], loss: 0.3710, total loss: 1154033.7326\n",
      "Epoch [4/5], step [458000/476702], loss: 1.6518, total loss: 1156194.3912\n",
      "Epoch [4/5], step [459000/476702], loss: 0.5314, total loss: 1158931.0187\n",
      "Epoch [4/5], step [460000/476702], loss: 8.8408, total loss: 1161337.6989\n",
      "Epoch [4/5], step [461000/476702], loss: 0.1186, total loss: 1163708.1166\n",
      "Epoch [4/5], step [462000/476702], loss: 6.2415, total loss: 1166071.0073\n",
      "Epoch [4/5], step [463000/476702], loss: 1.4609, total loss: 1168314.9086\n",
      "Epoch [4/5], step [464000/476702], loss: 0.5528, total loss: 1170658.1077\n",
      "Epoch [4/5], step [465000/476702], loss: 3.6820, total loss: 1173050.0358\n",
      "Epoch [4/5], step [466000/476702], loss: 0.0388, total loss: 1175409.5709\n",
      "Epoch [4/5], step [467000/476702], loss: 2.8810, total loss: 1178028.7704\n",
      "Epoch [4/5], step [468000/476702], loss: 4.7525, total loss: 1180367.9498\n",
      "Epoch [4/5], step [469000/476702], loss: 0.3218, total loss: 1182733.4017\n",
      "Epoch [4/5], step [470000/476702], loss: 1.5725, total loss: 1185026.4019\n",
      "Epoch [4/5], step [471000/476702], loss: 0.0829, total loss: 1187691.5017\n",
      "Epoch [4/5], step [472000/476702], loss: 3.1829, total loss: 1190129.0528\n",
      "Epoch [4/5], step [473000/476702], loss: 0.2884, total loss: 1192731.3634\n",
      "Epoch [4/5], step [474000/476702], loss: 5.4283, total loss: 1195169.8733\n",
      "Epoch [4/5], step [475000/476702], loss: 7.2180, total loss: 1197988.9313\n",
      "Epoch [4/5], step [476000/476702], loss: 3.8797, total loss: 1200549.9848\n",
      "Epoch [5/5], step [1000/476702], loss: 1.8114, total loss: 2112.6968\n",
      "Epoch [5/5], step [2000/476702], loss: 0.3664, total loss: 4723.9859\n",
      "Epoch [5/5], step [3000/476702], loss: 0.9693, total loss: 7089.0675\n",
      "Epoch [5/5], step [4000/476702], loss: 1.9700, total loss: 9285.0462\n",
      "Epoch [5/5], step [5000/476702], loss: 1.7450, total loss: 11838.1382\n",
      "Epoch [5/5], step [6000/476702], loss: 1.8450, total loss: 14302.9348\n",
      "Epoch [5/5], step [7000/476702], loss: 0.5437, total loss: 16603.3861\n",
      "Epoch [5/5], step [8000/476702], loss: 1.9420, total loss: 19087.2125\n",
      "Epoch [5/5], step [9000/476702], loss: 1.7168, total loss: 21719.7327\n",
      "Epoch [5/5], step [10000/476702], loss: 0.5885, total loss: 24016.0740\n",
      "Epoch [5/5], step [11000/476702], loss: 1.8845, total loss: 26243.5327\n",
      "Epoch [5/5], step [12000/476702], loss: 0.1200, total loss: 28501.3104\n",
      "Epoch [5/5], step [13000/476702], loss: 0.0124, total loss: 30957.5519\n",
      "Epoch [5/5], step [14000/476702], loss: 0.9020, total loss: 33362.1597\n",
      "Epoch [5/5], step [15000/476702], loss: 0.0649, total loss: 35879.3808\n",
      "Epoch [5/5], step [16000/476702], loss: 0.6463, total loss: 38159.7385\n",
      "Epoch [5/5], step [17000/476702], loss: 2.5873, total loss: 40662.8528\n",
      "Epoch [5/5], step [18000/476702], loss: 1.6914, total loss: 43045.8705\n",
      "Epoch [5/5], step [19000/476702], loss: 6.3190, total loss: 45739.0382\n",
      "Epoch [5/5], step [20000/476702], loss: 4.1096, total loss: 48161.8409\n",
      "Epoch [5/5], step [21000/476702], loss: 0.1574, total loss: 50410.5465\n",
      "Epoch [5/5], step [22000/476702], loss: 3.1211, total loss: 52891.4903\n",
      "Epoch [5/5], step [23000/476702], loss: 1.6402, total loss: 55420.3076\n",
      "Epoch [5/5], step [24000/476702], loss: 3.2060, total loss: 57869.5206\n",
      "Epoch [5/5], step [25000/476702], loss: 8.4179, total loss: 60393.6141\n",
      "Epoch [5/5], step [26000/476702], loss: 2.6312, total loss: 63103.1616\n",
      "Epoch [5/5], step [27000/476702], loss: 6.9064, total loss: 65336.2217\n",
      "Epoch [5/5], step [28000/476702], loss: 10.1701, total loss: 67672.0336\n",
      "Epoch [5/5], step [29000/476702], loss: 5.3389, total loss: 69978.9348\n",
      "Epoch [5/5], step [30000/476702], loss: 0.0011, total loss: 72482.0674\n",
      "Epoch [5/5], step [31000/476702], loss: 0.4179, total loss: 74912.4112\n",
      "Epoch [5/5], step [32000/476702], loss: 12.3722, total loss: 77289.8005\n",
      "Epoch [5/5], step [33000/476702], loss: 3.3605, total loss: 80145.1275\n",
      "Epoch [5/5], step [34000/476702], loss: 5.8848, total loss: 82493.2001\n",
      "Epoch [5/5], step [35000/476702], loss: 0.1330, total loss: 84889.9101\n",
      "Epoch [5/5], step [36000/476702], loss: 0.8572, total loss: 87637.6604\n",
      "Epoch [5/5], step [37000/476702], loss: 2.2868, total loss: 90270.1288\n",
      "Epoch [5/5], step [38000/476702], loss: 0.1004, total loss: 92800.2290\n",
      "Epoch [5/5], step [39000/476702], loss: 0.0396, total loss: 95050.0721\n",
      "Epoch [5/5], step [40000/476702], loss: 4.1057, total loss: 97475.2884\n",
      "Epoch [5/5], step [41000/476702], loss: 0.3497, total loss: 99703.0446\n",
      "Epoch [5/5], step [42000/476702], loss: 1.6928, total loss: 102114.9127\n",
      "Epoch [5/5], step [43000/476702], loss: 3.8121, total loss: 104736.7104\n",
      "Epoch [5/5], step [44000/476702], loss: 0.0786, total loss: 107186.0473\n",
      "Epoch [5/5], step [45000/476702], loss: 0.0318, total loss: 110004.0447\n",
      "Epoch [5/5], step [46000/476702], loss: 3.5350, total loss: 112232.2034\n",
      "Epoch [5/5], step [47000/476702], loss: 0.6579, total loss: 114499.4478\n",
      "Epoch [5/5], step [48000/476702], loss: 3.1386, total loss: 116896.5883\n",
      "Epoch [5/5], step [49000/476702], loss: 0.0435, total loss: 119660.6789\n",
      "Epoch [5/5], step [50000/476702], loss: 0.0766, total loss: 122532.1353\n",
      "Epoch [5/5], step [51000/476702], loss: 0.5709, total loss: 125244.3463\n",
      "Epoch [5/5], step [52000/476702], loss: 0.1613, total loss: 127655.6375\n",
      "Epoch [5/5], step [53000/476702], loss: 0.6513, total loss: 130298.3540\n",
      "Epoch [5/5], step [54000/476702], loss: 2.7374, total loss: 132577.0324\n",
      "Epoch [5/5], step [55000/476702], loss: 1.9038, total loss: 134855.1242\n",
      "Epoch [5/5], step [56000/476702], loss: 3.7240, total loss: 137510.2621\n",
      "Epoch [5/5], step [57000/476702], loss: 0.4080, total loss: 139690.9418\n",
      "Epoch [5/5], step [58000/476702], loss: 0.0450, total loss: 142531.3579\n",
      "Epoch [5/5], step [59000/476702], loss: 0.1775, total loss: 145090.4744\n",
      "Epoch [5/5], step [60000/476702], loss: 3.3389, total loss: 147522.7820\n",
      "Epoch [5/5], step [61000/476702], loss: 1.1648, total loss: 149896.8929\n",
      "Epoch [5/5], step [62000/476702], loss: 0.0975, total loss: 152211.7667\n",
      "Epoch [5/5], step [63000/476702], loss: 0.2314, total loss: 154790.5382\n",
      "Epoch [5/5], step [64000/476702], loss: 0.0730, total loss: 157306.0156\n",
      "Epoch [5/5], step [65000/476702], loss: 0.2056, total loss: 159757.1654\n",
      "Epoch [5/5], step [66000/476702], loss: 0.1312, total loss: 162069.7745\n",
      "Epoch [5/5], step [67000/476702], loss: 3.3809, total loss: 164432.7914\n",
      "Epoch [5/5], step [68000/476702], loss: 1.5185, total loss: 166755.6955\n",
      "Epoch [5/5], step [69000/476702], loss: 0.2996, total loss: 169308.7328\n",
      "Epoch [5/5], step [70000/476702], loss: 6.4800, total loss: 171973.3379\n",
      "Epoch [5/5], step [71000/476702], loss: 12.2134, total loss: 174547.2321\n",
      "Epoch [5/5], step [72000/476702], loss: 2.9617, total loss: 177049.2217\n",
      "Epoch [5/5], step [73000/476702], loss: 0.4967, total loss: 179522.3780\n",
      "Epoch [5/5], step [74000/476702], loss: 0.5431, total loss: 181961.9636\n",
      "Epoch [5/5], step [75000/476702], loss: 0.0975, total loss: 184304.7960\n",
      "Epoch [5/5], step [76000/476702], loss: 0.5223, total loss: 186582.0876\n",
      "Epoch [5/5], step [77000/476702], loss: 0.0614, total loss: 189264.0467\n",
      "Epoch [5/5], step [78000/476702], loss: 2.1888, total loss: 191656.5978\n",
      "Epoch [5/5], step [79000/476702], loss: 9.9749, total loss: 194185.1727\n",
      "Epoch [5/5], step [80000/476702], loss: 9.7272, total loss: 196762.4823\n",
      "Epoch [5/5], step [81000/476702], loss: 2.2373, total loss: 199201.2782\n",
      "Epoch [5/5], step [82000/476702], loss: 0.2484, total loss: 201579.2324\n",
      "Epoch [5/5], step [83000/476702], loss: 1.4177, total loss: 204019.2962\n",
      "Epoch [5/5], step [84000/476702], loss: 1.5472, total loss: 206270.5753\n",
      "Epoch [5/5], step [85000/476702], loss: 3.3259, total loss: 208626.9036\n",
      "Epoch [5/5], step [86000/476702], loss: 0.0073, total loss: 211017.4337\n",
      "Epoch [5/5], step [87000/476702], loss: 5.3197, total loss: 213510.4903\n",
      "Epoch [5/5], step [88000/476702], loss: 2.5615, total loss: 215536.0458\n",
      "Epoch [5/5], step [89000/476702], loss: 0.3294, total loss: 217395.9247\n",
      "Epoch [5/5], step [90000/476702], loss: 5.1685, total loss: 219738.0610\n",
      "Epoch [5/5], step [91000/476702], loss: 5.7952, total loss: 221913.7903\n",
      "Epoch [5/5], step [92000/476702], loss: 0.0840, total loss: 224286.4975\n",
      "Epoch [5/5], step [93000/476702], loss: 0.0283, total loss: 226461.5836\n",
      "Epoch [5/5], step [94000/476702], loss: 4.5407, total loss: 228816.9037\n",
      "Epoch [5/5], step [95000/476702], loss: 3.8223, total loss: 231317.8231\n",
      "Epoch [5/5], step [96000/476702], loss: 3.2052, total loss: 233546.1260\n",
      "Epoch [5/5], step [97000/476702], loss: 7.5454, total loss: 236099.9649\n",
      "Epoch [5/5], step [98000/476702], loss: 2.5548, total loss: 238502.7322\n",
      "Epoch [5/5], step [99000/476702], loss: 0.6982, total loss: 240650.6354\n",
      "Epoch [5/5], step [100000/476702], loss: 4.7790, total loss: 243015.2000\n",
      "Epoch [5/5], step [101000/476702], loss: 0.2207, total loss: 245406.9152\n",
      "Epoch [5/5], step [102000/476702], loss: 3.3113, total loss: 247942.8192\n",
      "Epoch [5/5], step [103000/476702], loss: 2.2530, total loss: 250431.2142\n",
      "Epoch [5/5], step [104000/476702], loss: 2.9796, total loss: 252726.9038\n",
      "Epoch [5/5], step [105000/476702], loss: 1.1772, total loss: 255312.7495\n",
      "Epoch [5/5], step [106000/476702], loss: 0.7759, total loss: 258056.5230\n",
      "Epoch [5/5], step [107000/476702], loss: 10.6865, total loss: 260361.4033\n",
      "Epoch [5/5], step [108000/476702], loss: 0.0154, total loss: 262649.4313\n",
      "Epoch [5/5], step [109000/476702], loss: 1.5334, total loss: 264799.3783\n",
      "Epoch [5/5], step [110000/476702], loss: 0.9575, total loss: 267206.8994\n",
      "Epoch [5/5], step [111000/476702], loss: 0.0925, total loss: 269332.9541\n",
      "Epoch [5/5], step [112000/476702], loss: 3.1246, total loss: 271583.9248\n",
      "Epoch [5/5], step [113000/476702], loss: 0.3326, total loss: 273867.3888\n",
      "Epoch [5/5], step [114000/476702], loss: 1.2334, total loss: 276213.9465\n",
      "Epoch [5/5], step [115000/476702], loss: 0.4806, total loss: 278541.2320\n",
      "Epoch [5/5], step [116000/476702], loss: 0.6744, total loss: 281190.1750\n",
      "Epoch [5/5], step [117000/476702], loss: 0.7407, total loss: 283502.6291\n",
      "Epoch [5/5], step [118000/476702], loss: 0.0606, total loss: 285624.1500\n",
      "Epoch [5/5], step [119000/476702], loss: 0.2833, total loss: 287786.8763\n",
      "Epoch [5/5], step [120000/476702], loss: 0.0002, total loss: 290474.7912\n",
      "Epoch [5/5], step [121000/476702], loss: 9.7729, total loss: 292652.4652\n",
      "Epoch [5/5], step [122000/476702], loss: 0.2318, total loss: 294979.4101\n",
      "Epoch [5/5], step [123000/476702], loss: 7.2211, total loss: 297391.4775\n",
      "Epoch [5/5], step [124000/476702], loss: 5.5873, total loss: 299782.1890\n",
      "Epoch [5/5], step [125000/476702], loss: 3.8029, total loss: 301946.1223\n",
      "Epoch [5/5], step [126000/476702], loss: 3.6067, total loss: 304113.8502\n",
      "Epoch [5/5], step [127000/476702], loss: 0.0106, total loss: 306577.7282\n",
      "Epoch [5/5], step [128000/476702], loss: 4.3970, total loss: 309286.9072\n",
      "Epoch [5/5], step [129000/476702], loss: 9.5580, total loss: 311690.0999\n",
      "Epoch [5/5], step [130000/476702], loss: 1.7388, total loss: 313919.4411\n",
      "Epoch [5/5], step [131000/476702], loss: 0.8019, total loss: 316427.7180\n",
      "Epoch [5/5], step [132000/476702], loss: 0.4794, total loss: 318594.6141\n",
      "Epoch [5/5], step [133000/476702], loss: 2.0114, total loss: 320853.6794\n",
      "Epoch [5/5], step [134000/476702], loss: 0.0165, total loss: 323127.0787\n",
      "Epoch [5/5], step [135000/476702], loss: 0.7647, total loss: 325455.1897\n",
      "Epoch [5/5], step [136000/476702], loss: 0.6608, total loss: 327604.4418\n",
      "Epoch [5/5], step [137000/476702], loss: 7.1390, total loss: 329938.1639\n",
      "Epoch [5/5], step [138000/476702], loss: 4.0260, total loss: 332221.5097\n",
      "Epoch [5/5], step [139000/476702], loss: 1.8839, total loss: 334344.9607\n",
      "Epoch [5/5], step [140000/476702], loss: 4.6738, total loss: 336478.2996\n",
      "Epoch [5/5], step [141000/476702], loss: 3.3363, total loss: 338533.0240\n",
      "Epoch [5/5], step [142000/476702], loss: 4.3393, total loss: 340686.3332\n",
      "Epoch [5/5], step [143000/476702], loss: 0.1747, total loss: 343095.4467\n",
      "Epoch [5/5], step [144000/476702], loss: 1.6904, total loss: 345483.4245\n",
      "Epoch [5/5], step [145000/476702], loss: 0.3464, total loss: 347971.9790\n",
      "Epoch [5/5], step [146000/476702], loss: 0.1063, total loss: 350420.6369\n",
      "Epoch [5/5], step [147000/476702], loss: 2.6742, total loss: 352723.4716\n",
      "Epoch [5/5], step [148000/476702], loss: 4.3028, total loss: 355192.2508\n",
      "Epoch [5/5], step [149000/476702], loss: 0.2220, total loss: 357444.7525\n",
      "Epoch [5/5], step [150000/476702], loss: 1.5052, total loss: 359546.8112\n",
      "Epoch [5/5], step [151000/476702], loss: 8.2961, total loss: 362197.9955\n",
      "Epoch [5/5], step [152000/476702], loss: 1.0566, total loss: 364773.5276\n",
      "Epoch [5/5], step [153000/476702], loss: 0.0018, total loss: 367456.7157\n",
      "Epoch [5/5], step [154000/476702], loss: 7.1839, total loss: 370044.8377\n",
      "Epoch [5/5], step [155000/476702], loss: 3.5223, total loss: 372544.9916\n",
      "Epoch [5/5], step [156000/476702], loss: 0.7672, total loss: 374804.1820\n",
      "Epoch [5/5], step [157000/476702], loss: 2.2474, total loss: 377263.7879\n",
      "Epoch [5/5], step [158000/476702], loss: 0.8289, total loss: 379933.1430\n",
      "Epoch [5/5], step [159000/476702], loss: 1.0425, total loss: 382547.9736\n",
      "Epoch [5/5], step [160000/476702], loss: 2.0386, total loss: 384837.8839\n",
      "Epoch [5/5], step [161000/476702], loss: 0.0027, total loss: 387330.2772\n",
      "Epoch [5/5], step [162000/476702], loss: 0.0104, total loss: 389578.6621\n",
      "Epoch [5/5], step [163000/476702], loss: 10.7002, total loss: 392172.7461\n",
      "Epoch [5/5], step [164000/476702], loss: 0.1495, total loss: 394722.2994\n",
      "Epoch [5/5], step [165000/476702], loss: 4.2262, total loss: 396902.7466\n",
      "Epoch [5/5], step [166000/476702], loss: 0.2621, total loss: 398918.0774\n",
      "Epoch [5/5], step [167000/476702], loss: 0.0837, total loss: 401302.0650\n",
      "Epoch [5/5], step [168000/476702], loss: 1.3279, total loss: 403837.2952\n",
      "Epoch [5/5], step [169000/476702], loss: 0.8179, total loss: 406327.7567\n",
      "Epoch [5/5], step [170000/476702], loss: 2.2964, total loss: 408653.9821\n",
      "Epoch [5/5], step [171000/476702], loss: 0.1462, total loss: 410911.8559\n",
      "Epoch [5/5], step [172000/476702], loss: 0.0787, total loss: 413415.5113\n",
      "Epoch [5/5], step [173000/476702], loss: 3.1865, total loss: 415779.3498\n",
      "Epoch [5/5], step [174000/476702], loss: 0.5917, total loss: 418225.4905\n",
      "Epoch [5/5], step [175000/476702], loss: 0.0089, total loss: 420292.1933\n",
      "Epoch [5/5], step [176000/476702], loss: 1.7531, total loss: 422538.9254\n",
      "Epoch [5/5], step [177000/476702], loss: 2.1878, total loss: 424993.6732\n",
      "Epoch [5/5], step [178000/476702], loss: 0.0599, total loss: 427403.8205\n",
      "Epoch [5/5], step [179000/476702], loss: 1.8454, total loss: 429999.5064\n",
      "Epoch [5/5], step [180000/476702], loss: 5.1773, total loss: 432321.9726\n",
      "Epoch [5/5], step [181000/476702], loss: 1.8699, total loss: 434931.0746\n",
      "Epoch [5/5], step [182000/476702], loss: 0.2234, total loss: 437349.1584\n",
      "Epoch [5/5], step [183000/476702], loss: 10.7976, total loss: 439597.2635\n",
      "Epoch [5/5], step [184000/476702], loss: 5.2349, total loss: 441831.9439\n",
      "Epoch [5/5], step [185000/476702], loss: 0.0574, total loss: 444017.8668\n",
      "Epoch [5/5], step [186000/476702], loss: 4.7293, total loss: 446233.5348\n",
      "Epoch [5/5], step [187000/476702], loss: 2.7907, total loss: 448462.7621\n",
      "Epoch [5/5], step [188000/476702], loss: 2.0237, total loss: 450945.3472\n",
      "Epoch [5/5], step [189000/476702], loss: 5.9956, total loss: 453516.8295\n",
      "Epoch [5/5], step [190000/476702], loss: 3.0211, total loss: 455533.0256\n",
      "Epoch [5/5], step [191000/476702], loss: 4.0549, total loss: 458188.6531\n",
      "Epoch [5/5], step [192000/476702], loss: 1.1808, total loss: 460619.5484\n",
      "Epoch [5/5], step [193000/476702], loss: 0.2784, total loss: 463053.2848\n",
      "Epoch [5/5], step [194000/476702], loss: 11.7717, total loss: 465202.2743\n",
      "Epoch [5/5], step [195000/476702], loss: 3.4792, total loss: 467574.5798\n",
      "Epoch [5/5], step [196000/476702], loss: 3.0067, total loss: 469955.4612\n",
      "Epoch [5/5], step [197000/476702], loss: 0.6299, total loss: 472108.9744\n",
      "Epoch [5/5], step [198000/476702], loss: 0.1980, total loss: 474799.9099\n",
      "Epoch [5/5], step [199000/476702], loss: 2.1100, total loss: 477355.9999\n",
      "Epoch [5/5], step [200000/476702], loss: 5.6043, total loss: 479840.1020\n",
      "Epoch [5/5], step [201000/476702], loss: 0.6604, total loss: 482463.1476\n",
      "Epoch [5/5], step [202000/476702], loss: 0.4143, total loss: 484904.5314\n",
      "Epoch [5/5], step [203000/476702], loss: 3.8118, total loss: 487578.5323\n",
      "Epoch [5/5], step [204000/476702], loss: 0.2876, total loss: 489957.8854\n",
      "Epoch [5/5], step [205000/476702], loss: 5.9444, total loss: 492461.2831\n",
      "Epoch [5/5], step [206000/476702], loss: 0.7333, total loss: 494643.6475\n",
      "Epoch [5/5], step [207000/476702], loss: 3.4207, total loss: 497086.9155\n",
      "Epoch [5/5], step [208000/476702], loss: 0.4883, total loss: 499581.6089\n",
      "Epoch [5/5], step [209000/476702], loss: 1.8996, total loss: 501962.1660\n",
      "Epoch [5/5], step [210000/476702], loss: 0.7298, total loss: 504445.6805\n",
      "Epoch [5/5], step [211000/476702], loss: 0.8661, total loss: 507015.4287\n",
      "Epoch [5/5], step [212000/476702], loss: 6.3331, total loss: 509419.4456\n",
      "Epoch [5/5], step [213000/476702], loss: 0.7524, total loss: 511597.1636\n",
      "Epoch [5/5], step [214000/476702], loss: 1.5051, total loss: 514133.6330\n",
      "Epoch [5/5], step [215000/476702], loss: 0.2681, total loss: 516616.0666\n",
      "Epoch [5/5], step [216000/476702], loss: 0.3381, total loss: 519523.0336\n",
      "Epoch [5/5], step [217000/476702], loss: 0.0022, total loss: 522503.1838\n",
      "Epoch [5/5], step [218000/476702], loss: 0.9828, total loss: 525306.8137\n",
      "Epoch [5/5], step [219000/476702], loss: 0.3964, total loss: 528155.6472\n",
      "Epoch [5/5], step [220000/476702], loss: 0.0992, total loss: 530800.9356\n",
      "Epoch [5/5], step [221000/476702], loss: 1.3503, total loss: 532988.1023\n",
      "Epoch [5/5], step [222000/476702], loss: 0.0048, total loss: 535412.2728\n",
      "Epoch [5/5], step [223000/476702], loss: 0.1290, total loss: 537716.0758\n",
      "Epoch [5/5], step [224000/476702], loss: 0.6357, total loss: 540004.7715\n",
      "Epoch [5/5], step [225000/476702], loss: 0.0715, total loss: 542370.5826\n",
      "Epoch [5/5], step [226000/476702], loss: 0.0313, total loss: 544715.6333\n",
      "Epoch [5/5], step [227000/476702], loss: 0.0063, total loss: 547137.8797\n",
      "Epoch [5/5], step [228000/476702], loss: 0.4502, total loss: 549943.3633\n",
      "Epoch [5/5], step [229000/476702], loss: 5.5287, total loss: 552550.8736\n",
      "Epoch [5/5], step [230000/476702], loss: 0.0283, total loss: 555216.3875\n",
      "Epoch [5/5], step [231000/476702], loss: 3.8954, total loss: 557553.0760\n",
      "Epoch [5/5], step [232000/476702], loss: 0.2591, total loss: 559774.0746\n",
      "Epoch [5/5], step [233000/476702], loss: 0.0417, total loss: 562166.4653\n",
      "Epoch [5/5], step [234000/476702], loss: 1.3691, total loss: 564513.1463\n",
      "Epoch [5/5], step [235000/476702], loss: 0.3281, total loss: 567162.0558\n",
      "Epoch [5/5], step [236000/476702], loss: 0.0025, total loss: 569716.3572\n",
      "Epoch [5/5], step [237000/476702], loss: 2.9433, total loss: 572584.0626\n",
      "Epoch [5/5], step [238000/476702], loss: 3.3129, total loss: 575368.2360\n",
      "Epoch [5/5], step [239000/476702], loss: 0.9420, total loss: 577928.3083\n",
      "Epoch [5/5], step [240000/476702], loss: 1.3534, total loss: 580571.7551\n",
      "Epoch [5/5], step [241000/476702], loss: 1.5618, total loss: 582832.9275\n",
      "Epoch [5/5], step [242000/476702], loss: 0.5806, total loss: 585596.0041\n",
      "Epoch [5/5], step [243000/476702], loss: 0.0039, total loss: 588005.1689\n",
      "Epoch [5/5], step [244000/476702], loss: 0.0030, total loss: 590228.0756\n",
      "Epoch [5/5], step [245000/476702], loss: 1.0097, total loss: 592836.7643\n",
      "Epoch [5/5], step [246000/476702], loss: 8.0095, total loss: 595290.6792\n",
      "Epoch [5/5], step [247000/476702], loss: 5.7005, total loss: 597795.3360\n",
      "Epoch [5/5], step [248000/476702], loss: 1.4035, total loss: 600063.1632\n",
      "Epoch [5/5], step [249000/476702], loss: 2.2982, total loss: 602593.7777\n",
      "Epoch [5/5], step [250000/476702], loss: 4.6498, total loss: 604883.5947\n",
      "Epoch [5/5], step [251000/476702], loss: 4.9965, total loss: 607150.9531\n",
      "Epoch [5/5], step [252000/476702], loss: 3.2615, total loss: 609303.3454\n",
      "Epoch [5/5], step [253000/476702], loss: 4.8857, total loss: 611922.2036\n",
      "Epoch [5/5], step [254000/476702], loss: 3.0592, total loss: 614257.6101\n",
      "Epoch [5/5], step [255000/476702], loss: 0.8663, total loss: 616628.8436\n",
      "Epoch [5/5], step [256000/476702], loss: 0.0159, total loss: 619051.2432\n",
      "Epoch [5/5], step [257000/476702], loss: 0.2084, total loss: 621540.3991\n",
      "Epoch [5/5], step [258000/476702], loss: 5.6840, total loss: 624025.9083\n",
      "Epoch [5/5], step [259000/476702], loss: 1.6648, total loss: 626733.0949\n",
      "Epoch [5/5], step [260000/476702], loss: 4.0519, total loss: 629236.6262\n",
      "Epoch [5/5], step [261000/476702], loss: 2.1840, total loss: 631597.3730\n",
      "Epoch [5/5], step [262000/476702], loss: 1.6808, total loss: 633953.4125\n",
      "Epoch [5/5], step [263000/476702], loss: 0.0282, total loss: 636700.2640\n",
      "Epoch [5/5], step [264000/476702], loss: 0.0003, total loss: 639022.7303\n",
      "Epoch [5/5], step [265000/476702], loss: 0.1182, total loss: 641749.7428\n",
      "Epoch [5/5], step [266000/476702], loss: 0.0187, total loss: 644544.5968\n",
      "Epoch [5/5], step [267000/476702], loss: 0.5534, total loss: 647223.5915\n",
      "Epoch [5/5], step [268000/476702], loss: 2.5066, total loss: 649698.2580\n",
      "Epoch [5/5], step [269000/476702], loss: 2.3364, total loss: 652304.5536\n",
      "Epoch [5/5], step [270000/476702], loss: 0.0251, total loss: 654712.1363\n",
      "Epoch [5/5], step [271000/476702], loss: 5.1949, total loss: 657139.0106\n",
      "Epoch [5/5], step [272000/476702], loss: 2.7795, total loss: 659536.4949\n",
      "Epoch [5/5], step [273000/476702], loss: 0.0113, total loss: 662192.2229\n",
      "Epoch [5/5], step [274000/476702], loss: 0.0011, total loss: 664499.6652\n",
      "Epoch [5/5], step [275000/476702], loss: 10.7860, total loss: 667004.4494\n",
      "Epoch [5/5], step [276000/476702], loss: 0.5640, total loss: 669351.7773\n",
      "Epoch [5/5], step [277000/476702], loss: 8.9748, total loss: 672123.8380\n",
      "Epoch [5/5], step [278000/476702], loss: 1.2397, total loss: 674642.7477\n",
      "Epoch [5/5], step [279000/476702], loss: 2.5030, total loss: 677013.3926\n",
      "Epoch [5/5], step [280000/476702], loss: 10.8442, total loss: 679705.7656\n",
      "Epoch [5/5], step [281000/476702], loss: 0.0654, total loss: 682374.0566\n",
      "Epoch [5/5], step [282000/476702], loss: 2.6726, total loss: 684765.5223\n",
      "Epoch [5/5], step [283000/476702], loss: 3.7187, total loss: 687226.3113\n",
      "Epoch [5/5], step [284000/476702], loss: 0.2978, total loss: 689737.0894\n",
      "Epoch [5/5], step [285000/476702], loss: 0.2878, total loss: 692113.0313\n",
      "Epoch [5/5], step [286000/476702], loss: 3.0575, total loss: 694710.4922\n",
      "Epoch [5/5], step [287000/476702], loss: 0.3096, total loss: 696917.9369\n",
      "Epoch [5/5], step [288000/476702], loss: 0.8837, total loss: 699761.8783\n",
      "Epoch [5/5], step [289000/476702], loss: 0.0021, total loss: 701953.6157\n",
      "Epoch [5/5], step [290000/476702], loss: 1.9940, total loss: 704641.1249\n",
      "Epoch [5/5], step [291000/476702], loss: 1.5900, total loss: 707090.1084\n",
      "Epoch [5/5], step [292000/476702], loss: 0.1027, total loss: 709818.1777\n",
      "Epoch [5/5], step [293000/476702], loss: 0.0676, total loss: 712127.6118\n",
      "Epoch [5/5], step [294000/476702], loss: 4.8553, total loss: 714510.4089\n",
      "Epoch [5/5], step [295000/476702], loss: 0.0143, total loss: 717068.1777\n",
      "Epoch [5/5], step [296000/476702], loss: 0.0531, total loss: 719327.3299\n",
      "Epoch [5/5], step [297000/476702], loss: 0.7440, total loss: 721741.2305\n",
      "Epoch [5/5], step [298000/476702], loss: 1.0336, total loss: 724133.8610\n",
      "Epoch [5/5], step [299000/476702], loss: 1.9040, total loss: 726463.0522\n",
      "Epoch [5/5], step [300000/476702], loss: 0.4303, total loss: 728912.0748\n",
      "Epoch [5/5], step [301000/476702], loss: 0.9089, total loss: 731417.8873\n",
      "Epoch [5/5], step [302000/476702], loss: 1.9175, total loss: 734018.3289\n",
      "Epoch [5/5], step [303000/476702], loss: 0.1988, total loss: 736347.8214\n",
      "Epoch [5/5], step [304000/476702], loss: 0.5449, total loss: 738715.5936\n",
      "Epoch [5/5], step [305000/476702], loss: 1.2074, total loss: 741046.1202\n",
      "Epoch [5/5], step [306000/476702], loss: 0.8800, total loss: 743346.5752\n",
      "Epoch [5/5], step [307000/476702], loss: 0.0539, total loss: 745937.2066\n",
      "Epoch [5/5], step [308000/476702], loss: 0.2499, total loss: 748571.3377\n",
      "Epoch [5/5], step [309000/476702], loss: 2.4598, total loss: 750761.9774\n",
      "Epoch [5/5], step [310000/476702], loss: 4.0783, total loss: 753322.5356\n",
      "Epoch [5/5], step [311000/476702], loss: 4.5578, total loss: 755487.9785\n",
      "Epoch [5/5], step [312000/476702], loss: 3.0216, total loss: 758274.3817\n",
      "Epoch [5/5], step [313000/476702], loss: 1.6301, total loss: 760760.0790\n",
      "Epoch [5/5], step [314000/476702], loss: 2.4696, total loss: 762988.0975\n",
      "Epoch [5/5], step [315000/476702], loss: 1.6633, total loss: 765406.6828\n",
      "Epoch [5/5], step [316000/476702], loss: 0.9123, total loss: 768012.5480\n",
      "Epoch [5/5], step [317000/476702], loss: 0.0170, total loss: 770299.1290\n",
      "Epoch [5/5], step [318000/476702], loss: 0.0349, total loss: 772631.9903\n",
      "Epoch [5/5], step [319000/476702], loss: 2.4800, total loss: 774828.1102\n",
      "Epoch [5/5], step [320000/476702], loss: 7.0994, total loss: 777405.0199\n",
      "Epoch [5/5], step [321000/476702], loss: 2.0290, total loss: 779715.9700\n",
      "Epoch [5/5], step [322000/476702], loss: 0.0257, total loss: 782097.6339\n",
      "Epoch [5/5], step [323000/476702], loss: 2.5573, total loss: 784581.5729\n",
      "Epoch [5/5], step [324000/476702], loss: 1.6303, total loss: 787014.8475\n",
      "Epoch [5/5], step [325000/476702], loss: 2.2904, total loss: 789471.6523\n",
      "Epoch [5/5], step [326000/476702], loss: 0.3859, total loss: 791973.3495\n",
      "Epoch [5/5], step [327000/476702], loss: 4.1194, total loss: 794511.5084\n",
      "Epoch [5/5], step [328000/476702], loss: 0.0091, total loss: 796844.9344\n",
      "Epoch [5/5], step [329000/476702], loss: 1.0963, total loss: 799383.6109\n",
      "Epoch [5/5], step [330000/476702], loss: 3.5554, total loss: 801851.7226\n",
      "Epoch [5/5], step [331000/476702], loss: 1.7407, total loss: 804361.0299\n",
      "Epoch [5/5], step [332000/476702], loss: 0.6221, total loss: 806758.7113\n",
      "Epoch [5/5], step [333000/476702], loss: 1.9170, total loss: 809064.3351\n",
      "Epoch [5/5], step [334000/476702], loss: 5.0616, total loss: 811826.5554\n",
      "Epoch [5/5], step [335000/476702], loss: 5.1260, total loss: 814220.0624\n",
      "Epoch [5/5], step [336000/476702], loss: 0.3268, total loss: 816788.7801\n",
      "Epoch [5/5], step [337000/476702], loss: 1.0958, total loss: 818981.8729\n",
      "Epoch [5/5], step [338000/476702], loss: 0.1191, total loss: 821051.5110\n",
      "Epoch [5/5], step [339000/476702], loss: 0.0189, total loss: 823855.4515\n",
      "Epoch [5/5], step [340000/476702], loss: 1.9412, total loss: 826076.3468\n",
      "Epoch [5/5], step [341000/476702], loss: 4.5835, total loss: 828787.6475\n",
      "Epoch [5/5], step [342000/476702], loss: 0.5701, total loss: 831586.2006\n",
      "Epoch [5/5], step [343000/476702], loss: 0.0002, total loss: 833988.0626\n",
      "Epoch [5/5], step [344000/476702], loss: 8.2424, total loss: 836217.5574\n",
      "Epoch [5/5], step [345000/476702], loss: 4.4259, total loss: 838828.1159\n",
      "Epoch [5/5], step [346000/476702], loss: 1.5141, total loss: 841190.7475\n",
      "Epoch [5/5], step [347000/476702], loss: 0.1943, total loss: 843707.5517\n",
      "Epoch [5/5], step [348000/476702], loss: 0.7186, total loss: 846080.0946\n",
      "Epoch [5/5], step [349000/476702], loss: 10.3818, total loss: 848587.8612\n",
      "Epoch [5/5], step [350000/476702], loss: 1.3346, total loss: 851272.4130\n",
      "Epoch [5/5], step [351000/476702], loss: 0.0110, total loss: 853835.0551\n",
      "Epoch [5/5], step [352000/476702], loss: 4.0749, total loss: 856402.4267\n",
      "Epoch [5/5], step [353000/476702], loss: 7.3954, total loss: 858986.1995\n",
      "Epoch [5/5], step [354000/476702], loss: 1.4930, total loss: 861675.8226\n",
      "Epoch [5/5], step [355000/476702], loss: 1.9595, total loss: 864040.5556\n",
      "Epoch [5/5], step [356000/476702], loss: 0.3887, total loss: 866438.5697\n",
      "Epoch [5/5], step [357000/476702], loss: 0.0288, total loss: 868891.6971\n",
      "Epoch [5/5], step [358000/476702], loss: 0.9092, total loss: 871172.6269\n",
      "Epoch [5/5], step [359000/476702], loss: 1.3823, total loss: 873394.5910\n",
      "Epoch [5/5], step [360000/476702], loss: 0.0332, total loss: 876144.7277\n",
      "Epoch [5/5], step [361000/476702], loss: 1.9614, total loss: 878741.6513\n",
      "Epoch [5/5], step [362000/476702], loss: 3.0191, total loss: 881258.9324\n",
      "Epoch [5/5], step [363000/476702], loss: 0.3624, total loss: 883371.0102\n",
      "Epoch [5/5], step [364000/476702], loss: 0.1625, total loss: 885660.4662\n",
      "Epoch [5/5], step [365000/476702], loss: 0.4953, total loss: 888068.9598\n",
      "Epoch [5/5], step [366000/476702], loss: 4.3987, total loss: 890455.0702\n",
      "Epoch [5/5], step [367000/476702], loss: 0.2646, total loss: 893069.8149\n",
      "Epoch [5/5], step [368000/476702], loss: 1.7390, total loss: 895430.8802\n",
      "Epoch [5/5], step [369000/476702], loss: 3.3873, total loss: 897937.1998\n",
      "Epoch [5/5], step [370000/476702], loss: 3.1922, total loss: 900607.7299\n",
      "Epoch [5/5], step [371000/476702], loss: 5.4137, total loss: 903134.4768\n",
      "Epoch [5/5], step [372000/476702], loss: 2.5197, total loss: 905667.5836\n",
      "Epoch [5/5], step [373000/476702], loss: 0.0138, total loss: 907792.5092\n",
      "Epoch [5/5], step [374000/476702], loss: 7.3521, total loss: 910302.2261\n",
      "Epoch [5/5], step [375000/476702], loss: 1.1783, total loss: 912787.8810\n",
      "Epoch [5/5], step [376000/476702], loss: 0.0111, total loss: 915422.5209\n",
      "Epoch [5/5], step [377000/476702], loss: 5.6564, total loss: 918101.3067\n",
      "Epoch [5/5], step [378000/476702], loss: 10.1573, total loss: 920851.8891\n",
      "Epoch [5/5], step [379000/476702], loss: 2.3061, total loss: 923574.2907\n",
      "Epoch [5/5], step [380000/476702], loss: 0.0542, total loss: 926144.0825\n",
      "Epoch [5/5], step [381000/476702], loss: 9.2623, total loss: 928778.5952\n",
      "Epoch [5/5], step [382000/476702], loss: 1.6038, total loss: 931302.9109\n",
      "Epoch [5/5], step [383000/476702], loss: 0.1620, total loss: 933480.5258\n",
      "Epoch [5/5], step [384000/476702], loss: 2.1098, total loss: 936097.8253\n",
      "Epoch [5/5], step [385000/476702], loss: 0.0247, total loss: 938656.4935\n",
      "Epoch [5/5], step [386000/476702], loss: 5.0338, total loss: 941149.5023\n",
      "Epoch [5/5], step [387000/476702], loss: 3.4869, total loss: 943456.5516\n",
      "Epoch [5/5], step [388000/476702], loss: 3.6035, total loss: 945589.4000\n",
      "Epoch [5/5], step [389000/476702], loss: 0.0725, total loss: 948082.1799\n",
      "Epoch [5/5], step [390000/476702], loss: 0.0575, total loss: 950756.5638\n",
      "Epoch [5/5], step [391000/476702], loss: 4.2012, total loss: 953266.2224\n",
      "Epoch [5/5], step [392000/476702], loss: 0.0078, total loss: 955757.5733\n",
      "Epoch [5/5], step [393000/476702], loss: 0.0022, total loss: 958211.7316\n",
      "Epoch [5/5], step [394000/476702], loss: 0.0139, total loss: 960660.9856\n",
      "Epoch [5/5], step [395000/476702], loss: 4.0003, total loss: 963081.8130\n",
      "Epoch [5/5], step [396000/476702], loss: 0.2277, total loss: 965535.9955\n",
      "Epoch [5/5], step [397000/476702], loss: 2.7261, total loss: 967967.0980\n",
      "Epoch [5/5], step [398000/476702], loss: 1.2634, total loss: 970416.0071\n",
      "Epoch [5/5], step [399000/476702], loss: 3.7681, total loss: 972880.4606\n",
      "Epoch [5/5], step [400000/476702], loss: 0.1137, total loss: 975138.6744\n",
      "Epoch [5/5], step [401000/476702], loss: 5.6787, total loss: 977874.2771\n",
      "Epoch [5/5], step [402000/476702], loss: 1.9048, total loss: 980282.7717\n",
      "Epoch [5/5], step [403000/476702], loss: 0.4622, total loss: 982995.5060\n",
      "Epoch [5/5], step [404000/476702], loss: 0.1011, total loss: 985758.5730\n",
      "Epoch [5/5], step [405000/476702], loss: 4.7189, total loss: 988172.1729\n",
      "Epoch [5/5], step [406000/476702], loss: 3.7423, total loss: 990805.4340\n",
      "Epoch [5/5], step [407000/476702], loss: 0.0095, total loss: 993355.0733\n",
      "Epoch [5/5], step [408000/476702], loss: 0.0322, total loss: 995990.3727\n",
      "Epoch [5/5], step [409000/476702], loss: 2.3038, total loss: 998531.6297\n",
      "Epoch [5/5], step [410000/476702], loss: 4.2150, total loss: 1000986.9330\n",
      "Epoch [5/5], step [411000/476702], loss: 1.4691, total loss: 1003295.7948\n",
      "Epoch [5/5], step [412000/476702], loss: 0.6794, total loss: 1005814.5181\n",
      "Epoch [5/5], step [413000/476702], loss: 2.4538, total loss: 1008090.7210\n",
      "Epoch [5/5], step [414000/476702], loss: 8.6965, total loss: 1010447.3570\n",
      "Epoch [5/5], step [415000/476702], loss: 0.3830, total loss: 1012848.6777\n",
      "Epoch [5/5], step [416000/476702], loss: 0.1439, total loss: 1015442.1564\n",
      "Epoch [5/5], step [417000/476702], loss: 0.5806, total loss: 1017846.6624\n",
      "Epoch [5/5], step [418000/476702], loss: 0.7545, total loss: 1020468.3543\n",
      "Epoch [5/5], step [419000/476702], loss: 7.1995, total loss: 1022625.7840\n",
      "Epoch [5/5], step [420000/476702], loss: 2.2929, total loss: 1025171.5169\n",
      "Epoch [5/5], step [421000/476702], loss: 0.0822, total loss: 1027642.6209\n",
      "Epoch [5/5], step [422000/476702], loss: 1.6400, total loss: 1029802.0829\n",
      "Epoch [5/5], step [423000/476702], loss: 0.0032, total loss: 1032089.1427\n",
      "Epoch [5/5], step [424000/476702], loss: 0.0947, total loss: 1034568.3310\n",
      "Epoch [5/5], step [425000/476702], loss: 8.2288, total loss: 1036994.3692\n",
      "Epoch [5/5], step [426000/476702], loss: 0.1330, total loss: 1039419.8342\n",
      "Epoch [5/5], step [427000/476702], loss: 0.4993, total loss: 1041709.7321\n",
      "Epoch [5/5], step [428000/476702], loss: 2.1114, total loss: 1043864.2438\n",
      "Epoch [5/5], step [429000/476702], loss: 0.1923, total loss: 1046299.8945\n",
      "Epoch [5/5], step [430000/476702], loss: 1.5746, total loss: 1048683.3423\n",
      "Epoch [5/5], step [431000/476702], loss: 0.5395, total loss: 1050928.2915\n",
      "Epoch [5/5], step [432000/476702], loss: 2.2491, total loss: 1052972.4726\n",
      "Epoch [5/5], step [433000/476702], loss: 0.1563, total loss: 1055141.9639\n",
      "Epoch [5/5], step [434000/476702], loss: 0.0011, total loss: 1057190.3831\n",
      "Epoch [5/5], step [435000/476702], loss: 4.0322, total loss: 1059497.7605\n",
      "Epoch [5/5], step [436000/476702], loss: 0.0369, total loss: 1062057.9759\n",
      "Epoch [5/5], step [437000/476702], loss: 2.0207, total loss: 1064306.5190\n",
      "Epoch [5/5], step [438000/476702], loss: 1.9758, total loss: 1066648.5020\n",
      "Epoch [5/5], step [439000/476702], loss: 9.0823, total loss: 1069026.5297\n",
      "Epoch [5/5], step [440000/476702], loss: 4.0080, total loss: 1071201.7714\n",
      "Epoch [5/5], step [441000/476702], loss: 14.2760, total loss: 1073501.7148\n",
      "Epoch [5/5], step [442000/476702], loss: 0.0024, total loss: 1075958.1322\n",
      "Epoch [5/5], step [443000/476702], loss: 1.5533, total loss: 1078507.4472\n",
      "Epoch [5/5], step [444000/476702], loss: 0.0540, total loss: 1080697.0488\n",
      "Epoch [5/5], step [445000/476702], loss: 3.9589, total loss: 1083239.0912\n",
      "Epoch [5/5], step [446000/476702], loss: 7.6269, total loss: 1085410.0910\n",
      "Epoch [5/5], step [447000/476702], loss: 4.6048, total loss: 1088014.0506\n",
      "Epoch [5/5], step [448000/476702], loss: 0.2761, total loss: 1090116.3730\n",
      "Epoch [5/5], step [449000/476702], loss: 1.3788, total loss: 1092784.0194\n",
      "Epoch [5/5], step [450000/476702], loss: 2.0603, total loss: 1095072.1644\n",
      "Epoch [5/5], step [451000/476702], loss: 1.6233, total loss: 1097477.9397\n",
      "Epoch [5/5], step [452000/476702], loss: 0.1112, total loss: 1100156.4659\n",
      "Epoch [5/5], step [453000/476702], loss: 2.6059, total loss: 1102422.6300\n",
      "Epoch [5/5], step [454000/476702], loss: 10.1668, total loss: 1104663.9776\n",
      "Epoch [5/5], step [455000/476702], loss: 4.5003, total loss: 1107231.0267\n",
      "Epoch [5/5], step [456000/476702], loss: 0.0263, total loss: 1109672.0241\n",
      "Epoch [5/5], step [457000/476702], loss: 0.3019, total loss: 1111967.8892\n",
      "Epoch [5/5], step [458000/476702], loss: 1.5833, total loss: 1114058.0107\n",
      "Epoch [5/5], step [459000/476702], loss: 0.6396, total loss: 1116701.0725\n",
      "Epoch [5/5], step [460000/476702], loss: 8.6416, total loss: 1119010.5585\n",
      "Epoch [5/5], step [461000/476702], loss: 0.0950, total loss: 1121313.2771\n",
      "Epoch [5/5], step [462000/476702], loss: 6.3378, total loss: 1123589.6089\n",
      "Epoch [5/5], step [463000/476702], loss: 1.6020, total loss: 1125763.7323\n",
      "Epoch [5/5], step [464000/476702], loss: 0.6213, total loss: 1128036.0021\n",
      "Epoch [5/5], step [465000/476702], loss: 3.5693, total loss: 1130351.9942\n",
      "Epoch [5/5], step [466000/476702], loss: 0.0289, total loss: 1132633.8369\n",
      "Epoch [5/5], step [467000/476702], loss: 3.0168, total loss: 1135171.0222\n",
      "Epoch [5/5], step [468000/476702], loss: 4.6186, total loss: 1137411.8373\n",
      "Epoch [5/5], step [469000/476702], loss: 0.3343, total loss: 1139695.2761\n",
      "Epoch [5/5], step [470000/476702], loss: 1.6023, total loss: 1141907.9067\n",
      "Epoch [5/5], step [471000/476702], loss: 0.0671, total loss: 1144477.6788\n",
      "Epoch [5/5], step [472000/476702], loss: 2.8621, total loss: 1146842.1748\n",
      "Epoch [5/5], step [473000/476702], loss: 0.2811, total loss: 1149369.3911\n",
      "Epoch [5/5], step [474000/476702], loss: 5.1026, total loss: 1151734.0630\n",
      "Epoch [5/5], step [475000/476702], loss: 7.2161, total loss: 1154456.4691\n",
      "Epoch [5/5], step [476000/476702], loss: 3.6718, total loss: 1156931.2866\n",
      "[1645495.7948140234, 1350388.6591364976, 1260381.6627421654, 1202173.0726405587, 1158508.909683858]\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "num_epoch = 5\n",
    "step_count = len(data)\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(CONTEXT_SIZE, EMBEDDING_DIM, vocab_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for i, sample in enumerate(data):\n",
    "\n",
    "        context, target = sample\n",
    "        # Prepare the inputs to be passed to the model\n",
    "        context_idxs = make_context_vector(context, word_to_ix)\n",
    "\n",
    "        # Reset grad\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Run forward and get log probabilities over the word that matches the context\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long).to(device))\n",
    "\n",
    "        # Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if((i+1) % 1000 == 0):\n",
    "            print(\n",
    "                        f\"Epoch [{epoch + 1}/{num_epoch}]\"\n",
    "                        f\", step [{i + 1}/{step_count}]\"\n",
    "                        f\", loss: {loss.item():.4f}\"\n",
    "                        f\", total loss: {total_loss:.4f}\"\n",
    "                    )\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model \n",
    "torch.save(model.state_dict(), './models/word2vec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model for test\n",
    "model = CBOW(CONTEXT_SIZE, EMBEDDING_DIM, vocab_size).to(device)\n",
    "model.load_state_dict(torch.load('./models/word2vec.h5'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
