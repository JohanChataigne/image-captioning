{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n",
      "GPU found :)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as m\n",
    "from torch.utils.data import DataLoader\n",
    "from transforms import *\n",
    "from torchvision.transforms import Compose\n",
    "from torchsummary import summary\n",
    "from repeat_image_dataset import RepeatImageDataset\n",
    "from text_preprocessing import *\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(\"GPU found :)\" if torch.cuda.is_available() else \"No GPU :(\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "EMBEDDING_SIZE = 512\n",
    "train_annotations_file = './flickr8k/annotations/annotations_image_id_train.csv'\n",
    "test_annotations_file = './flickr8k/annotations/annotations_image_id_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datas section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init text preprocessing class\n",
    "tp = TextPreprocessor(train_annotations_file, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat dataset size: 30000\n"
     ]
    }
   ],
   "source": [
    "transforms = Compose([Rescale(256), \n",
    "                      RandomCrop(IMAGE_SIZE), \n",
    "                      ToTensor(), Normalize(),\n",
    "                      OneHotEncode(tp)])\n",
    "\n",
    "train_repeat_dataset = RepeatImageDataset('./flickr8k/images/train/', train_annotations_file, transform=transforms)\n",
    "\n",
    "print(f'Repeat dataset size: {len(train_repeat_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# Build data loaders\n",
    "train_repeat_loader = DataLoader(train_repeat_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve pretrained model for features extraction\n",
    "base_cnn = m.resnet18(pretrained=True)\n",
    "#base_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 1.96 GiB total capacity; 520.56 MiB already allocated; 1.88 MiB free; 550.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5b6d2445a838>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Keep only the feature extraction layers of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m350\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m350\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 1.96 GiB total capacity; 520.56 MiB already allocated; 1.88 MiB free; 550.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Keep only the feature extraction layers of the model\n",
    "cnn = nn.Sequential(*(list(base_cnn.children())[:-1])).to(device, dtype=torch.float)\n",
    "summary(cnn, (3, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8255\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tp.vocab_size\n",
    "print(vocab_size)\n",
    "\n",
    "# RNN with LSTM of  layer\n",
    "class LSTMCaptioning(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMCaptioning, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.hidden2out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, previous_state):\n",
    "         \n",
    "        # Get hidden states for each t (out) , and latest one (h = (ht, ct))\n",
    "        lstm_out, (hn, cn) = self.lstm(x, previous_state)\n",
    "        \n",
    "        # Convert output of rnn to output targeted size\n",
    "        out = self.hidden2out(lstm_out.view(1, -1))\n",
    "        \n",
    "        # Compute probability distribution over all words for this t\n",
    "        pt = F.log_softmax(out, dim=1)\n",
    "                           \n",
    "        return (hn, cn), pt\n",
    "\n",
    "# Not trained embedding layer to encode words to hidden space\n",
    "embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = EMBEDDING_SIZE\n",
    "hidden_size = 300\n",
    "\n",
    "model = LSTMCaptioning(input_size, hidden_size, vocab_size).to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], step [600/6000], loss: 0.9465\n",
      "Epoch [1/10], step [1200/6000], loss: 1.8174\n",
      "Epoch [1/10], step [1800/6000], loss: 0.3157\n",
      "Epoch [1/10], step [2400/6000], loss: 0.3442\n",
      "Epoch [1/10], step [3000/6000], loss: 0.4863\n",
      "Epoch [1/10], step [3600/6000], loss: 0.4348\n",
      "Epoch [1/10], step [4200/6000], loss: 2.2075\n",
      "Epoch [1/10], step [4800/6000], loss: 16.5890\n",
      "Epoch [1/10], step [5400/6000], loss: 2.9704\n",
      "Epoch [1/10], step [6000/6000], loss: 2.3771\n",
      "Epoch [2/10], step [600/6000], loss: 0.5536\n",
      "Epoch [2/10], step [1200/6000], loss: 1.7598\n",
      "Epoch [2/10], step [1800/6000], loss: 0.3626\n",
      "Epoch [2/10], step [2400/6000], loss: 17.1973\n",
      "Epoch [2/10], step [3000/6000], loss: 0.4023\n",
      "Epoch [2/10], step [3600/6000], loss: 0.5249\n",
      "Epoch [2/10], step [4200/6000], loss: 2.1344\n",
      "Epoch [2/10], step [4800/6000], loss: 16.1560\n",
      "Epoch [2/10], step [5400/6000], loss: 0.5574\n",
      "Epoch [2/10], step [6000/6000], loss: 0.4632\n",
      "Epoch [3/10], step [600/6000], loss: 0.5931\n",
      "Epoch [3/10], step [1200/6000], loss: 1.6696\n",
      "Epoch [3/10], step [1800/6000], loss: 0.4569\n",
      "Epoch [3/10], step [2400/6000], loss: 17.9105\n",
      "Epoch [3/10], step [3000/6000], loss: 0.4368\n",
      "Epoch [3/10], step [3600/6000], loss: 2.4281\n",
      "Epoch [3/10], step [4200/6000], loss: 2.1371\n",
      "Epoch [3/10], step [4800/6000], loss: 3.1183\n",
      "Epoch [3/10], step [5400/6000], loss: 0.4649\n",
      "Epoch [3/10], step [6000/6000], loss: 5.7710\n",
      "Epoch [4/10], step [600/6000], loss: 0.6282\n",
      "Epoch [4/10], step [1200/6000], loss: 2.4595\n",
      "Epoch [4/10], step [1800/6000], loss: 0.3514\n",
      "Epoch [4/10], step [2400/6000], loss: 18.7327\n",
      "Epoch [4/10], step [3000/6000], loss: 0.3282\n",
      "Epoch [4/10], step [3600/6000], loss: 0.4565\n",
      "Epoch [4/10], step [4200/6000], loss: 2.3021\n",
      "Epoch [4/10], step [4800/6000], loss: 3.3730\n",
      "Epoch [4/10], step [5400/6000], loss: 0.4666\n",
      "Epoch [4/10], step [6000/6000], loss: 0.4780\n",
      "Epoch [5/10], step [600/6000], loss: 0.6544\n",
      "Epoch [5/10], step [1200/6000], loss: 1.5796\n",
      "Epoch [5/10], step [1800/6000], loss: 0.3316\n",
      "Epoch [5/10], step [2400/6000], loss: 0.3721\n",
      "Epoch [5/10], step [3000/6000], loss: 0.3776\n",
      "Epoch [5/10], step [3600/6000], loss: 0.5903\n",
      "Epoch [5/10], step [4200/6000], loss: 2.2626\n",
      "Epoch [5/10], step [4800/6000], loss: 18.7613\n",
      "Epoch [5/10], step [5400/6000], loss: 0.3651\n",
      "Epoch [5/10], step [6000/6000], loss: 2.3657\n",
      "Epoch [6/10], step [600/6000], loss: 0.8486\n",
      "Epoch [6/10], step [1200/6000], loss: 1.7267\n",
      "Epoch [6/10], step [1800/6000], loss: 0.4437\n",
      "Epoch [6/10], step [2400/6000], loss: 0.2500\n",
      "Epoch [6/10], step [3000/6000], loss: 0.4075\n",
      "Epoch [6/10], step [3600/6000], loss: 0.6953\n",
      "Epoch [6/10], step [4200/6000], loss: 1.8956\n",
      "Epoch [6/10], step [4800/6000], loss: 2.8290\n",
      "Epoch [6/10], step [5400/6000], loss: 0.5798\n",
      "Epoch [6/10], step [6000/6000], loss: 0.5767\n",
      "Epoch [7/10], step [600/6000], loss: 0.6708\n",
      "Epoch [7/10], step [1200/6000], loss: 1.7409\n",
      "Epoch [7/10], step [1800/6000], loss: 18.6209\n",
      "Epoch [7/10], step [2400/6000], loss: 0.2587\n",
      "Epoch [7/10], step [3000/6000], loss: 0.3107\n",
      "Epoch [7/10], step [3600/6000], loss: 2.9058\n",
      "Epoch [7/10], step [4200/6000], loss: 1.9044\n",
      "Epoch [7/10], step [4800/6000], loss: 3.7658\n",
      "Epoch [7/10], step [5400/6000], loss: 0.6080\n",
      "Epoch [7/10], step [6000/6000], loss: 0.6157\n",
      "Epoch [8/10], step [600/6000], loss: 0.7842\n",
      "Epoch [8/10], step [1200/6000], loss: 1.6958\n",
      "Epoch [8/10], step [1800/6000], loss: 0.3592\n",
      "Epoch [8/10], step [2400/6000], loss: 16.6761\n",
      "Epoch [8/10], step [3000/6000], loss: 2.9398\n",
      "Epoch [8/10], step [3600/6000], loss: 1.9892\n",
      "Epoch [8/10], step [4200/6000], loss: 1.9317\n",
      "Epoch [8/10], step [4800/6000], loss: 12.9344\n",
      "Epoch [8/10], step [5400/6000], loss: 3.4400\n",
      "Epoch [8/10], step [6000/6000], loss: 0.5555\n",
      "Epoch [9/10], step [600/6000], loss: 0.6855\n",
      "Epoch [9/10], step [1200/6000], loss: 2.4077\n",
      "Epoch [9/10], step [1800/6000], loss: 0.3187\n",
      "Epoch [9/10], step [2400/6000], loss: 0.2263\n",
      "Epoch [9/10], step [3000/6000], loss: 0.4607\n",
      "Epoch [9/10], step [3600/6000], loss: 1.7967\n",
      "Epoch [9/10], step [4200/6000], loss: 1.8596\n",
      "Epoch [9/10], step [4800/6000], loss: 3.2897\n",
      "Epoch [9/10], step [5400/6000], loss: 0.6738\n",
      "Epoch [9/10], step [6000/6000], loss: 0.6199\n",
      "Epoch [10/10], step [600/6000], loss: 0.6307\n",
      "Epoch [10/10], step [1200/6000], loss: 1.8742\n",
      "Epoch [10/10], step [1800/6000], loss: 0.3509\n",
      "Epoch [10/10], step [2400/6000], loss: 0.3646\n",
      "Epoch [10/10], step [3000/6000], loss: 3.6824\n",
      "Epoch [10/10], step [3600/6000], loss: 2.1247\n",
      "Epoch [10/10], step [4200/6000], loss: 1.8220\n",
      "Epoch [10/10], step [4800/6000], loss: 9.2791\n",
      "Epoch [10/10], step [5400/6000], loss: 0.4595\n",
      "Epoch [10/10], step [6000/6000], loss: 0.6367\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epoch = 10\n",
    "step_count = len(train_random_loader)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# Random init the lstm state\n",
    "h0 = torch.rand((1, batch_size, hidden_size)).to(device, dtype=torch.float)\n",
    "c0 = torch.rand((1, batch_size, hidden_size)).to(device, dtype=torch.float)\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for i, sample in enumerate(train_repeat_loader):\n",
    "        \n",
    "        image = sample['image'].to(device, dtype=torch.float)\n",
    "        caption = sample['caption'].to(device, dtype=torch.long)\n",
    "        \n",
    "        # Reset grad\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Get the input image embedding \n",
    "        image_embedding = cnn(image).view(-1, batch_size, EMBEDDING_SIZE)\n",
    "        \n",
    "        \n",
    "        # Forward pass for t=-1: image\n",
    "        (hn, cn), probs = model(image_embedding, (h0, c0))\n",
    "        \n",
    "        del image_embedding\n",
    "        del image\n",
    "        \n",
    "        target = tp.target_from_vect(caption[:, 0]).to(device)\n",
    "        \n",
    "        # Compute loss for 1st word prediction\n",
    "        loss = loss_function(probs, target)\n",
    "        \n",
    "        # Forward pass for t>=0: n - 1 first words of the sentence\n",
    "        for j, word in enumerate(caption[:, :-1]):\n",
    "\n",
    "            \n",
    "            # Get index of the word in embedding matrix\n",
    "            idxs = torch.argmax(word)\n",
    "            \n",
    "            # Encode word to hidden space\n",
    "            word_embedding = embedding(idxs).view(1, batch_size, EMBEDDING_SIZE)\n",
    "            \n",
    "            # Feed the rnn\n",
    "            (hn, cn), probs = model(word_embedding, (hn, cn))\n",
    "            \n",
    "            target = tp.target_from_vect(caption[:, j+1]).to(device)\n",
    "            \n",
    "            # Add current word's loss\n",
    "            loss += loss_function(probs, target)\n",
    "\n",
    "        \n",
    "        # Compute loss and backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Debug\n",
    "        if((i+1) % (step_count/5) == 0):\n",
    "            print(\n",
    "                        f\"Epoch [{epoch + 1}/{num_epoch}]\"\n",
    "                        f\", step [{i + 1}/{step_count}]\"\n",
    "                        f\", loss: {loss.item():.4f}\"\n",
    "                    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model \n",
    "torch.save(model.state_dict(), './models/model_v1_repeat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMCaptioning(\n",
       "  (lstm): LSTM(512, 300)\n",
       "  (hidden2out): Linear(in_features=300, out_features=8255, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model for evaluation\n",
    "trained_model = LSTMCaptioning(input_size, hidden_size, vocab_size)\n",
    "trained_model.load_state_dict(torch.load('./models/model_v1_repeat'))\n",
    "trained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test loaders for datasets\n",
    "\n",
    "# Only preprocess images\n",
    "test_transforms = Compose([Rescale(256), \n",
    "                      RandomCrop(IMAGE_SIZE), \n",
    "                      ToTensor(), \n",
    "                      Normalize()])\n",
    "\n",
    "test_repeat_dataset = RepeatImageDataset('./flickr8k/images/test/', test_annotations_file, transform=test_transforms)\n",
    "\n",
    "test_repeat_loader = DataLoader(test_repeat_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    for sample in test_repeat_loader:\n",
    "        \n",
    "        caption = list()\n",
    "\n",
    "        # Random init the lstm state\n",
    "        h0 = torch.rand((1, batch_size, hidden_size)).to(device, dtype=torch.float)\n",
    "        c0 = torch.rand((1, batch_size, hidden_size)).to(device, dtype=torch.float)\n",
    "\n",
    "        # Encode input image\n",
    "        image = sample['image'].to(device, dtype=torch.float)\n",
    "        image_embedding = cnn(image).view(-1, batch_size, EMBEDDING_SIZE).to(device)\n",
    "\n",
    "        # Get first word prediction probabilities\n",
    "        (hn, cn), probs = model(image_embedding, (h0, c0))\n",
    "\n",
    "        # Extract predicted word\n",
    "        pred_idx = torch.argmax(probs)\n",
    "        pred_word_vect = tp.encoding_matrix[pred_idx]\n",
    "        predicted_word = tp.vect_to_word(pred_word_vect)\n",
    "\n",
    "        caption.append(predicted_word)\n",
    "        \n",
    "        print(predicted_word)\n",
    "        \n",
    "        i = 0\n",
    "        # Build caption until model outputs stop word\n",
    "        while predicted_word != '<stop>' and i < 20:\n",
    "\n",
    "            word_embedding = embedding(pred_idx).view(1, batch_size, EMBEDDING_SIZE).to(device)\n",
    "\n",
    "            (hn, cn), probs = model(word_embedding, (hn, cn))\n",
    "\n",
    "            pred_idx = torch.argmax(probs)\n",
    "            pred_word_vect = tp.encoding_matrix[pred_idx]\n",
    "            predicted_word = tp.vect_to_word(pred_word_vect)\n",
    "\n",
    "            caption.append(predicted_word)\n",
    "\n",
    "            print(predicted_word)\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        caption = \" \".join(caption)\n",
    "\n",
    "        print(caption)\n",
    "        \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
