{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n",
      "GPU found :)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as m\n",
    "from torch.utils.data import DataLoader\n",
    "from transforms import *\n",
    "from torchvision.transforms import Compose\n",
    "from torchsummary import summary\n",
    "from repeat_image_dataset import RepeatImageDataset\n",
    "from text_preprocessing import *\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(\"GPU found :)\" if torch.cuda.is_available() else \"No GPU :(\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "EMBEDDING_SIZE = 512\n",
    "CONTEXT_SIZE = 4\n",
    "train_annotations_file = './flickr8k/annotations/annotations_image_id_train.csv'\n",
    "test_annotations_file = './flickr8k/annotations/annotations_image_id_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datas section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init text preprocessing class\n",
    "tp = TextPreprocessor(train_annotations_file, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat dataset size: 30000\n"
     ]
    }
   ],
   "source": [
    "transforms = Compose([Rescale(256), \n",
    "                      RandomCrop(IMAGE_SIZE), \n",
    "                      ToTensor(), Normalize(),\n",
    "                      OneHotEncode(tp)])\n",
    "\n",
    "train_repeat_dataset = RepeatImageDataset('./flickr8k/images/train/', train_annotations_file, transform=transforms)\n",
    "\n",
    "print(f'Repeat dataset size: {len(train_repeat_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# Build data loaders\n",
    "train_repeat_loader = DataLoader(train_repeat_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve pretrained model for features extraction\n",
    "base_cnn = m.resnet18(pretrained=True)\n",
    "#base_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the feature extraction layers of the model\n",
    "cnn = nn.Sequential(*(list(base_cnn.children())[:-1])).to(device, dtype=torch.float)\n",
    "#summary(cnn, (3, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(8255, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tp.vocab_size\n",
    "print(vocab_size)\n",
    "\n",
    "# RNN with LSTM of  layer\n",
    "class LSTMCaptioning(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMCaptioning, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.hidden2out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, previous_state):\n",
    "         \n",
    "        # Get hidden states for each t (out) , and latest one (h = (ht, ct))\n",
    "        lstm_out, (hn, cn) = self.lstm(x, previous_state)\n",
    "        \n",
    "        # Convert output of rnn to output targeted size\n",
    "        out = self.hidden2out(lstm_out.view(1, -1))\n",
    "        \n",
    "        # Compute probability distribution over all words for this t\n",
    "        pt = F.log_softmax(out, dim=1)\n",
    "                           \n",
    "        return (hn, cn), pt\n",
    "\n",
    "# Need to copy class here to load trained Ngram model    \n",
    "class NGram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGram, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embeds = self.embeddings(inputs).view(len(inputs), -1)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return log_probs\n",
    "\n",
    "# Load model for evaluation\n",
    "ngram_model = NGram(vocab_size, EMBEDDING_SIZE, CONTEXT_SIZE)\n",
    "ngram_model.load_state_dict(torch.load('./models/ngram_512_v1'))\n",
    "\n",
    "embedding = list(ngram_model.children())[0].to(device)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = EMBEDDING_SIZE\n",
    "hidden_size = 256\n",
    "\n",
    "model = LSTMCaptioning(input_size, hidden_size, vocab_size).to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epoch = 10\n",
    "step_count = len(train_repeat_loader)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# Random init the lstm state\n",
    "h0 = torch.rand((1, batch_size, hidden_size)).to(device, dtype=torch.float)\n",
    "c0 = torch.rand((1, batch_size, hidden_size)).to(device, dtype=torch.float)\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for i, sample in enumerate(train_repeat_loader):\n",
    "        \n",
    "        \n",
    "        image = sample['image'].to(device, dtype=torch.float)\n",
    "        caption = sample['caption'].to(device, dtype=torch.long)\n",
    "        \n",
    "        # Reset grad\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Get the input image embedding \n",
    "        image_embedding = cnn(image).view(-1, batch_size, EMBEDDING_SIZE)\n",
    "        \n",
    "        \n",
    "        # Forward pass for t=-1: image\n",
    "        (hn, cn), probs = model(image_embedding, (h0, c0))\n",
    "        \n",
    "        del image_embedding\n",
    "        del image\n",
    "        \n",
    "        target = tp.target_from_vect(caption[:, 0]).to(device)\n",
    "        \n",
    "        # Compute loss for 1st word prediction\n",
    "        loss = loss_function(probs, target)\n",
    "        \n",
    "        # Forward pass for t>=0: n - 1 first words of the sentence\n",
    "        for j, word in enumerate(caption[:, :-1][0]):\n",
    "\n",
    "            \n",
    "            # Get index of the word in embedding matrix\n",
    "            idxs = torch.argmax(word)\n",
    "            \n",
    "            # Encode word to hidden space\n",
    "            word_embedding = embedding(idxs).view(1, batch_size, EMBEDDING_SIZE)\n",
    "            \n",
    "            # Feed the rnn\n",
    "            (hn, cn), probs = model(word_embedding, (hn, cn))\n",
    "            \n",
    "            target = tp.target_from_vect(caption[:, j+1]).to(device)\n",
    "            \n",
    "            # Add current word's loss\n",
    "            loss += loss_function(probs, target)\n",
    "\n",
    "        \n",
    "        # Compute loss and backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Debug\n",
    "        if((i+1) % int(step_count/5) == 0):\n",
    "            print(\n",
    "                        f\"Epoch [{epoch + 1}/{num_epoch}]\"\n",
    "                        f\", step [{i + 1}/{step_count}]\"\n",
    "                        f\", loss: {loss.item():.4f}\"\n",
    "                    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            }, './models/model_v1_repeat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for evaluation\n",
    "trained_model = LSTMCaptioning(input_size, hidden_size, vocab_size)\n",
    "trained_model.load_state_dict(torch.load('./models/model_v1_repeat'))\n",
    "trained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test loaders for datasets\n",
    "\n",
    "# Only preprocess images\n",
    "test_transforms = Compose([Rescale(256), \n",
    "                      RandomCrop(IMAGE_SIZE), \n",
    "                      ToTensor(), \n",
    "                      Normalize()])\n",
    "\n",
    "test_repeat_dataset = RepeatImageDataset('./flickr8k/images/test/', test_annotations_file, transform=test_transforms)\n",
    "\n",
    "test_repeat_loader = DataLoader(test_repeat_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    for sample in test_repeat_loader:\n",
    "        \n",
    "        caption = list()\n",
    "\n",
    "        # Random init the lstm state\n",
    "        h0 = torch.rand((1, batch_size, hidden_size)).to(device, dtype=torch.float)\n",
    "        c0 = torch.rand((1, batch_size, hidden_size)).to(device, dtype=torch.float)\n",
    "\n",
    "        # Encode input image\n",
    "        image = sample['image'].to(device, dtype=torch.float)\n",
    "        image_embedding = cnn(image).view(-1, batch_size, EMBEDDING_SIZE).to(device)\n",
    "\n",
    "        # Get first word prediction probabilities\n",
    "        (hn, cn), probs = model(image_embedding, (h0, c0))\n",
    "\n",
    "        # Extract predicted word\n",
    "        pred_idx = torch.argmax(probs)\n",
    "        pred_word_vect = tp.encoding_matrix[pred_idx]\n",
    "        predicted_word = tp.vect_to_word(pred_word_vect)\n",
    "\n",
    "        caption.append(predicted_word)\n",
    "        \n",
    "        print(predicted_word)\n",
    "        \n",
    "        i = 0\n",
    "        # Build caption until model outputs stop word\n",
    "        while predicted_word != '<stop>' and i < 20:\n",
    "\n",
    "            word_embedding = embedding(pred_idx).view(1, batch_size, EMBEDDING_SIZE).to(device)\n",
    "\n",
    "            (hn, cn), probs = model(word_embedding, (hn, cn))\n",
    "\n",
    "            pred_idx = torch.argmax(probs)\n",
    "            pred_word_vect = tp.encoding_matrix[pred_idx]\n",
    "            predicted_word = tp.vect_to_word(pred_word_vect)\n",
    "\n",
    "            caption.append(predicted_word)\n",
    "\n",
    "            print(predicted_word)\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        caption = \" \".join(caption)\n",
    "\n",
    "        print(caption)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0330, -0.8076,  0.4611],\n",
      "        [-1.4472,  0.5975, -0.1127],\n",
      "        [-0.7613, -0.2993,  0.7059],\n",
      "        [-0.5506,  0.8039, -1.2187],\n",
      "        [ 0.9716,  0.3158, -0.1192],\n",
      "        [ 0.1314,  2.0221, -0.5677],\n",
      "        [-1.1597,  1.0611, -0.8042],\n",
      "        [-0.6171,  1.3323, -1.8513],\n",
      "        [-0.5031, -1.8007, -1.6748],\n",
      "        [-2.0235, -0.1504,  0.9782]])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[ 0.9716,  2.0221,  0.9782],\n",
      "        [ 0.1314,  1.3323,  0.7059],\n",
      "        [-0.0330,  1.0611,  0.4611]]),\n",
      "indices=tensor([[4, 5, 9],\n",
      "        [5, 7, 2],\n",
      "        [0, 6, 0]]))\n"
     ]
    }
   ],
   "source": [
    "test = torch.randn(10,3)\n",
    "print(test)\n",
    "print(torch.topk(test, k=3, dim=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
