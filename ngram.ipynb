{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding model using N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n",
      "GPU found :)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(\"GPU found :)\" if torch.cuda.is_available() else \"No GPU :(\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from text_preprocessing import TextPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 512\n",
    "CONTEXT_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TextPreprocessor('./flickr8k/annotations/annotations_image_id_train.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['<start>', 'A', 'child', 'in'], 'a'), (['A', 'child', 'in', 'a'], 'pink'), (['child', 'in', 'a', 'pink'], 'dress')]\n"
     ]
    }
   ],
   "source": [
    "# Add start and stop words for each sentence and build words list\n",
    "raw_sentences = (\" \".join(list(map(lambda s: '<start> ' + s + ' <stop>', tp.raw_sentences)))).split()\n",
    "\n",
    "# Build ngrams\n",
    "ngrams = list()\n",
    "\n",
    "for i in range(len(raw_sentences) - CONTEXT_SIZE):\n",
    "    \n",
    "    ngram = ([raw_sentences[i+k] for k in range(CONTEXT_SIZE)], raw_sentences[i+CONTEXT_SIZE])\n",
    "    ngrams.append(ngram)\n",
    "\n",
    "print(ngrams[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramsDataset(Dataset):\n",
    "    \"\"\"Image captioning dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, ngrams):      \n",
    "        self.ngrams = ngrams\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ngrams) \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()            \n",
    "\n",
    "        context = self.ngrams[index][0]\n",
    "        context = list(map(lambda w: tp.word_to_idx(w), context))\n",
    "        \n",
    "        target = tp.word_to_idx(self.ngrams[index][1])\n",
    "        \n",
    "        sample = {'context': torch.tensor(context), 'target': torch.tensor(target)}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NgramsDataset(ngrams)\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGram(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGram, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embeds = self.embeddings(inputs).view(len(inputs), -1)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], step [40/404], loss: 4.2476\n",
      "Epoch [1/10], step [80/404], loss: 3.6838\n",
      "Epoch [1/10], step [120/404], loss: 3.5214\n",
      "Epoch [1/10], step [160/404], loss: 3.5556\n",
      "Epoch [1/10], step [200/404], loss: 3.4874\n",
      "Epoch [1/10], step [240/404], loss: 3.1855\n",
      "Epoch [1/10], step [280/404], loss: 3.4051\n",
      "Epoch [1/10], step [320/404], loss: 3.2361\n",
      "Epoch [1/10], step [360/404], loss: 3.2790\n",
      "Epoch [1/10], step [400/404], loss: 3.2117\n",
      "Epoch [2/10], step [40/404], loss: 2.9381\n",
      "Epoch [2/10], step [80/404], loss: 2.9675\n",
      "Epoch [2/10], step [120/404], loss: 3.0198\n",
      "Epoch [2/10], step [160/404], loss: 2.9371\n",
      "Epoch [2/10], step [200/404], loss: 2.9130\n",
      "Epoch [2/10], step [240/404], loss: 2.9320\n",
      "Epoch [2/10], step [280/404], loss: 2.7778\n",
      "Epoch [2/10], step [320/404], loss: 2.9541\n",
      "Epoch [2/10], step [360/404], loss: 2.8595\n",
      "Epoch [2/10], step [400/404], loss: 2.8264\n",
      "Epoch [3/10], step [40/404], loss: 2.7966\n",
      "Epoch [3/10], step [80/404], loss: 2.6855\n",
      "Epoch [3/10], step [120/404], loss: 2.7185\n",
      "Epoch [3/10], step [160/404], loss: 2.7414\n",
      "Epoch [3/10], step [200/404], loss: 2.7950\n",
      "Epoch [3/10], step [240/404], loss: 2.7121\n",
      "Epoch [3/10], step [280/404], loss: 2.6114\n",
      "Epoch [3/10], step [320/404], loss: 2.7104\n",
      "Epoch [3/10], step [360/404], loss: 2.8642\n",
      "Epoch [3/10], step [400/404], loss: 2.5885\n",
      "Epoch [4/10], step [40/404], loss: 2.5831\n",
      "Epoch [4/10], step [80/404], loss: 2.5764\n",
      "Epoch [4/10], step [120/404], loss: 2.5351\n",
      "Epoch [4/10], step [160/404], loss: 2.4640\n",
      "Epoch [4/10], step [200/404], loss: 2.6016\n",
      "Epoch [4/10], step [240/404], loss: 2.5738\n",
      "Epoch [4/10], step [280/404], loss: 2.5687\n",
      "Epoch [4/10], step [320/404], loss: 2.5886\n",
      "Epoch [4/10], step [360/404], loss: 2.6079\n",
      "Epoch [4/10], step [400/404], loss: 2.5607\n",
      "Epoch [5/10], step [40/404], loss: 2.4205\n",
      "Epoch [5/10], step [80/404], loss: 2.4288\n",
      "Epoch [5/10], step [120/404], loss: 2.4731\n",
      "Epoch [5/10], step [160/404], loss: 2.4397\n",
      "Epoch [5/10], step [200/404], loss: 2.3493\n",
      "Epoch [5/10], step [240/404], loss: 2.4918\n",
      "Epoch [5/10], step [280/404], loss: 2.5274\n",
      "Epoch [5/10], step [320/404], loss: 2.4930\n",
      "Epoch [5/10], step [360/404], loss: 2.5192\n",
      "Epoch [5/10], step [400/404], loss: 2.4409\n",
      "Epoch [6/10], step [40/404], loss: 2.2174\n",
      "Epoch [6/10], step [80/404], loss: 2.3400\n",
      "Epoch [6/10], step [120/404], loss: 2.2790\n",
      "Epoch [6/10], step [160/404], loss: 2.2172\n",
      "Epoch [6/10], step [200/404], loss: 2.2016\n",
      "Epoch [6/10], step [240/404], loss: 2.2190\n",
      "Epoch [6/10], step [280/404], loss: 2.3657\n",
      "Epoch [6/10], step [320/404], loss: 2.3791\n",
      "Epoch [6/10], step [360/404], loss: 2.2897\n",
      "Epoch [6/10], step [400/404], loss: 2.3541\n",
      "Epoch [7/10], step [40/404], loss: 2.2254\n",
      "Epoch [7/10], step [80/404], loss: 2.1451\n",
      "Epoch [7/10], step [120/404], loss: 2.0693\n",
      "Epoch [7/10], step [160/404], loss: 2.2187\n",
      "Epoch [7/10], step [200/404], loss: 2.1551\n",
      "Epoch [7/10], step [240/404], loss: 2.1030\n",
      "Epoch [7/10], step [280/404], loss: 2.2147\n",
      "Epoch [7/10], step [320/404], loss: 2.3343\n",
      "Epoch [7/10], step [360/404], loss: 2.0713\n",
      "Epoch [7/10], step [400/404], loss: 2.2047\n",
      "Epoch [8/10], step [40/404], loss: 2.0767\n",
      "Epoch [8/10], step [80/404], loss: 2.1686\n",
      "Epoch [8/10], step [120/404], loss: 2.1087\n",
      "Epoch [8/10], step [160/404], loss: 2.1128\n",
      "Epoch [8/10], step [200/404], loss: 2.0703\n",
      "Epoch [8/10], step [240/404], loss: 2.1078\n",
      "Epoch [8/10], step [280/404], loss: 2.0893\n",
      "Epoch [8/10], step [320/404], loss: 2.1097\n",
      "Epoch [8/10], step [360/404], loss: 2.1048\n",
      "Epoch [8/10], step [400/404], loss: 2.1095\n",
      "Epoch [9/10], step [40/404], loss: 2.0361\n",
      "Epoch [9/10], step [80/404], loss: 1.9800\n",
      "Epoch [9/10], step [120/404], loss: 1.9631\n",
      "Epoch [9/10], step [160/404], loss: 2.0157\n",
      "Epoch [9/10], step [200/404], loss: 2.0381\n",
      "Epoch [9/10], step [240/404], loss: 1.9165\n",
      "Epoch [9/10], step [280/404], loss: 2.0592\n",
      "Epoch [9/10], step [320/404], loss: 2.0277\n",
      "Epoch [9/10], step [360/404], loss: 2.0929\n",
      "Epoch [9/10], step [400/404], loss: 2.1157\n",
      "Epoch [10/10], step [40/404], loss: 1.7921\n",
      "Epoch [10/10], step [80/404], loss: 2.0005\n",
      "Epoch [10/10], step [120/404], loss: 1.9102\n",
      "Epoch [10/10], step [160/404], loss: 1.9135\n",
      "Epoch [10/10], step [200/404], loss: 2.0916\n",
      "Epoch [10/10], step [240/404], loss: 2.0278\n",
      "Epoch [10/10], step [280/404], loss: 2.0422\n",
      "Epoch [10/10], step [320/404], loss: 1.9382\n",
      "Epoch [10/10], step [360/404], loss: 2.0653\n",
      "Epoch [10/10], step [400/404], loss: 2.0631\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "step_count = len(train_loader)\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGram(tp.vocab_size, EMBEDDING_SIZE, CONTEXT_SIZE).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for i, sample in enumerate(train_loader):\n",
    "\n",
    "        context = sample['context'].to(device)\n",
    "        target = sample['target'].to(device)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, target)\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Debug\n",
    "        if((i+1) % int(step_count/10) == 0):\n",
    "            print(\n",
    "                        f\"Epoch [{epoch + 1}/{num_epoch}]\"\n",
    "                        f\", step [{i + 1}/{step_count}]\"\n",
    "                        f\", loss: {loss.item():.4f}\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model \n",
    "torch.save(model.state_dict(), './models/ngram')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
