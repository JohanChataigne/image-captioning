{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n",
      "GPU found :)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as m\n",
    "from torch.utils.data import DataLoader\n",
    "from transforms import *\n",
    "from torchvision.transforms import Compose\n",
    "from torchsummary import summary\n",
    "from repeat_image_dataset import RepeatImageDataset\n",
    "from random_caption_dataset import RandomCaptionDataset\n",
    "from text_preprocessing import *\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(\"GPU found :)\" if torch.cuda.is_available() else \"No GPU :(\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "EMBEDDING_SIZE = 512\n",
    "train_annotations_file = './flickr8k/annotations/annotations_image_id_train.csv'\n",
    "test_annotations_file = './flickr8k/annotations/annotations_image_id_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datas section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init text preprocessing class\n",
    "\n",
    "tp = TextPreprocessor(train_annotations_file, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat dataset size: 30000\n",
      "Repeat dataset size: 6000\n"
     ]
    }
   ],
   "source": [
    "transforms = Compose([Rescale(256), \n",
    "                      RandomCrop(IMAGE_SIZE), \n",
    "                      ToTensor(), Normalize(),\n",
    "                      OneHotEncode(tp)])\n",
    "\n",
    "train_repeat_dataset = RepeatImageDataset('./flickr8k/images/train/', train_annotations_file, transform=transforms)\n",
    "train_random_dataset = RandomCaptionDataset('./flickr8k/images/train/', train_annotations_file, transform=transforms)\n",
    "\n",
    "print(f'Repeat dataset size: {len(train_repeat_dataset)}')\n",
    "print(f'Repeat dataset size: {len(train_random_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# Build data loaders\n",
    "train_repeat_loader = DataLoader(train_repeat_dataset, batch_size=batch_size)\n",
    "train_random_loader = DataLoader(train_random_dataset, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve pretrained model for features extraction\n",
    "base_cnn = m.resnet18(pretrained=True)\n",
    "#base_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the feature extraction layers of the model\n",
    "cnn = nn.Sequential(*(list(base_cnn.children())[:-1])).to(device, dtype=torch.float)\n",
    "#summary(cnn, (3, IMAGE_SIZE, IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8255\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tp.vocab_size\n",
    "print(vocab_size)\n",
    "\n",
    "# RNN with LSTM of  layer\n",
    "class LSTMCaptioning(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMCaptioning, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.hidden2out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, previous_state):\n",
    "         \n",
    "        # Get hidden states for each t (out) , and latest one (h = (ht, ct))\n",
    "        lstm_out, (hn, cn) = self.lstm(x, previous_state)\n",
    "        \n",
    "        # Convert output of rnn to output targeted size\n",
    "        out = self.hidden2out(lstm_out.view(1, -1))\n",
    "        \n",
    "        # Compute probability distribution over all words for this t\n",
    "        pt = F.log_softmax(out, dim=1)\n",
    "                           \n",
    "        return (hn, cn), pt\n",
    "\n",
    "# Not trained embedding layer to encode words to hidden space\n",
    "embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = EMBEDDING_SIZE\n",
    "hidden_size = 300\n",
    "\n",
    "model = LSTMCaptioning(input_size, hidden_size, vocab_size).to(device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], step [600/6000], loss: 0.9465\n",
      "Epoch [1/10], step [1200/6000], loss: 1.8174\n",
      "Epoch [1/10], step [1800/6000], loss: 0.3157\n",
      "Epoch [1/10], step [2400/6000], loss: 0.3442\n",
      "Epoch [1/10], step [3000/6000], loss: 0.4863\n",
      "Epoch [1/10], step [3600/6000], loss: 0.4348\n",
      "Epoch [1/10], step [4200/6000], loss: 2.2075\n",
      "Epoch [1/10], step [4800/6000], loss: 16.5890\n",
      "Epoch [1/10], step [5400/6000], loss: 2.9704\n",
      "Epoch [1/10], step [6000/6000], loss: 2.3771\n",
      "Epoch [2/10], step [600/6000], loss: 0.5536\n",
      "Epoch [2/10], step [1200/6000], loss: 1.7598\n",
      "Epoch [2/10], step [1800/6000], loss: 0.3626\n",
      "Epoch [2/10], step [2400/6000], loss: 17.1973\n",
      "Epoch [2/10], step [3000/6000], loss: 0.4023\n",
      "Epoch [2/10], step [3600/6000], loss: 0.5249\n",
      "Epoch [2/10], step [4200/6000], loss: 2.1344\n",
      "Epoch [2/10], step [4800/6000], loss: 16.1560\n",
      "Epoch [2/10], step [5400/6000], loss: 0.5574\n",
      "Epoch [2/10], step [6000/6000], loss: 0.4632\n",
      "Epoch [3/10], step [600/6000], loss: 0.5931\n",
      "Epoch [3/10], step [1200/6000], loss: 1.6696\n",
      "Epoch [3/10], step [1800/6000], loss: 0.4569\n",
      "Epoch [3/10], step [2400/6000], loss: 17.9105\n",
      "Epoch [3/10], step [3000/6000], loss: 0.4368\n",
      "Epoch [3/10], step [3600/6000], loss: 2.4281\n",
      "Epoch [3/10], step [4200/6000], loss: 2.1371\n",
      "Epoch [3/10], step [4800/6000], loss: 3.1183\n",
      "Epoch [3/10], step [5400/6000], loss: 0.4649\n",
      "Epoch [3/10], step [6000/6000], loss: 5.7710\n",
      "Epoch [4/10], step [600/6000], loss: 0.6282\n",
      "Epoch [4/10], step [1200/6000], loss: 2.4595\n",
      "Epoch [4/10], step [1800/6000], loss: 0.3514\n",
      "Epoch [4/10], step [2400/6000], loss: 18.7327\n",
      "Epoch [4/10], step [3000/6000], loss: 0.3282\n",
      "Epoch [4/10], step [3600/6000], loss: 0.4565\n",
      "Epoch [4/10], step [4200/6000], loss: 2.3021\n",
      "Epoch [4/10], step [4800/6000], loss: 3.3730\n",
      "Epoch [4/10], step [5400/6000], loss: 0.4666\n",
      "Epoch [4/10], step [6000/6000], loss: 0.4780\n",
      "Epoch [5/10], step [600/6000], loss: 0.6544\n",
      "Epoch [5/10], step [1200/6000], loss: 1.5796\n",
      "Epoch [5/10], step [1800/6000], loss: 0.3316\n",
      "Epoch [5/10], step [2400/6000], loss: 0.3721\n",
      "Epoch [5/10], step [3000/6000], loss: 0.3776\n",
      "Epoch [5/10], step [3600/6000], loss: 0.5903\n",
      "Epoch [5/10], step [4200/6000], loss: 2.2626\n",
      "Epoch [5/10], step [4800/6000], loss: 18.7613\n",
      "Epoch [5/10], step [5400/6000], loss: 0.3651\n",
      "Epoch [5/10], step [6000/6000], loss: 2.3657\n",
      "Epoch [6/10], step [600/6000], loss: 0.8486\n",
      "Epoch [6/10], step [1200/6000], loss: 1.7267\n",
      "Epoch [6/10], step [1800/6000], loss: 0.4437\n",
      "Epoch [6/10], step [2400/6000], loss: 0.2500\n",
      "Epoch [6/10], step [3000/6000], loss: 0.4075\n",
      "Epoch [6/10], step [3600/6000], loss: 0.6953\n",
      "Epoch [6/10], step [4200/6000], loss: 1.8956\n",
      "Epoch [6/10], step [4800/6000], loss: 2.8290\n",
      "Epoch [6/10], step [5400/6000], loss: 0.5798\n",
      "Epoch [6/10], step [6000/6000], loss: 0.5767\n",
      "Epoch [7/10], step [600/6000], loss: 0.6708\n",
      "Epoch [7/10], step [1200/6000], loss: 1.7409\n",
      "Epoch [7/10], step [1800/6000], loss: 18.6209\n",
      "Epoch [7/10], step [2400/6000], loss: 0.2587\n",
      "Epoch [7/10], step [3000/6000], loss: 0.3107\n",
      "Epoch [7/10], step [3600/6000], loss: 2.9058\n",
      "Epoch [7/10], step [4200/6000], loss: 1.9044\n",
      "Epoch [7/10], step [4800/6000], loss: 3.7658\n",
      "Epoch [7/10], step [5400/6000], loss: 0.6080\n",
      "Epoch [7/10], step [6000/6000], loss: 0.6157\n",
      "Epoch [8/10], step [600/6000], loss: 0.7842\n",
      "Epoch [8/10], step [1200/6000], loss: 1.6958\n",
      "Epoch [8/10], step [1800/6000], loss: 0.3592\n",
      "Epoch [8/10], step [2400/6000], loss: 16.6761\n",
      "Epoch [8/10], step [3000/6000], loss: 2.9398\n",
      "Epoch [8/10], step [3600/6000], loss: 1.9892\n",
      "Epoch [8/10], step [4200/6000], loss: 1.9317\n",
      "Epoch [8/10], step [4800/6000], loss: 12.9344\n",
      "Epoch [8/10], step [5400/6000], loss: 3.4400\n",
      "Epoch [8/10], step [6000/6000], loss: 0.5555\n",
      "Epoch [9/10], step [600/6000], loss: 0.6855\n",
      "Epoch [9/10], step [1200/6000], loss: 2.4077\n",
      "Epoch [9/10], step [1800/6000], loss: 0.3187\n",
      "Epoch [9/10], step [2400/6000], loss: 0.2263\n",
      "Epoch [9/10], step [3000/6000], loss: 0.4607\n",
      "Epoch [9/10], step [3600/6000], loss: 1.7967\n",
      "Epoch [9/10], step [4200/6000], loss: 1.8596\n",
      "Epoch [9/10], step [4800/6000], loss: 3.2897\n",
      "Epoch [9/10], step [5400/6000], loss: 0.6738\n",
      "Epoch [9/10], step [6000/6000], loss: 0.6199\n",
      "Epoch [10/10], step [600/6000], loss: 0.6307\n",
      "Epoch [10/10], step [1200/6000], loss: 1.8742\n",
      "Epoch [10/10], step [1800/6000], loss: 0.3509\n",
      "Epoch [10/10], step [2400/6000], loss: 0.3646\n",
      "Epoch [10/10], step [3000/6000], loss: 3.6824\n",
      "Epoch [10/10], step [3600/6000], loss: 2.1247\n",
      "Epoch [10/10], step [4200/6000], loss: 1.8220\n",
      "Epoch [10/10], step [4800/6000], loss: 9.2791\n",
      "Epoch [10/10], step [5400/6000], loss: 0.4595\n",
      "Epoch [10/10], step [6000/6000], loss: 0.6367\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epoch = 10\n",
    "step_count = len(train_random_loader)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# Random init the lstm state\n",
    "h0 = torch.rand((1, batch_size, hidden_size)).to(device, dtype=torch.float)\n",
    "c0 = torch.rand((1, batch_size, hidden_size)).to(device, dtype=torch.float)\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for i, sample in enumerate(train_random_loader):\n",
    "        \n",
    "        image = sample['image'].to(device, dtype=torch.float)\n",
    "        caption = sample['caption'].to(device, dtype=torch.long)\n",
    "        \n",
    "        # Reset grad\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Get the input image embedding \n",
    "        image_embedding = cnn(image).view(-1, batch_size, EMBEDDING_SIZE)\n",
    "        \n",
    "        \n",
    "        # Forward pass for t=-1: image\n",
    "        (hn, cn), probs = model(image_embedding, (h0, c0))\n",
    "        \n",
    "        del image_embedding\n",
    "        del image\n",
    "        \n",
    "        target = tp.target_from_vect(caption[:, 0]).to(device)\n",
    "        \n",
    "        # Compute loss for 1st word prediction\n",
    "        loss = loss_function(probs, target)\n",
    "        \n",
    "        # Forward pass for t>=0: n - 1 first words of the sentence\n",
    "        for j, word in enumerate(caption[:, :-1]):\n",
    "\n",
    "            \n",
    "            # Get index of the word in embedding matrix\n",
    "            idxs = torch.argmax(word)\n",
    "            \n",
    "            # Encode word to hidden space\n",
    "            word_embedding = embedding(idxs).view(1, batch_size, EMBEDDING_SIZE)\n",
    "            \n",
    "            # Feed the rnn\n",
    "            (hn, cn), probs = model(word_embedding, (hn, cn))\n",
    "            \n",
    "            target = tp.target_from_vect(caption[:, j+1]).to(device)\n",
    "            \n",
    "            # Add current word's loss\n",
    "            loss += loss_function(probs, target)\n",
    "\n",
    "        \n",
    "        # Compute loss and backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        # Debug\n",
    "        if((i+1) % (step_count/10) == 0):\n",
    "            print(\n",
    "                        f\"Epoch [{epoch + 1}/{num_epoch}]\"\n",
    "                        f\", step [{i + 1}/{step_count}]\"\n",
    "                        f\", loss: {loss.item():.4f}\"\n",
    "                    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model \n",
    "torch.save(model.state_dict(), './models/model_v1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
