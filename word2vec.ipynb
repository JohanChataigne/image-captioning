{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.7.1\n",
      "GPU found :)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(\"GPU found :)\" if torch.cuda.is_available() else \"No GPU :(\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40460 entries, 0 to 40459\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   image_id  40460 non-null  object\n",
      " 1   caption   40460 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 632.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_vocab = pd.read_csv('./flickr8k/annotations/annotations_image_id.csv', sep=';')\n",
    "df_vocab.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9629\n"
     ]
    }
   ],
   "source": [
    "raw_sentences = list(df_vocab.iloc[:, 1])\n",
    "\n",
    "raw_text = raw_sentences[0]\n",
    "\n",
    "# Build raw_text\n",
    "for i in range(1, len(raw_sentences)):\n",
    "    raw_text += ' ' + raw_sentences[i]\n",
    "\n",
    "#print(raw_text)\n",
    "raw_text = raw_text.split()\n",
    "\n",
    "# Get vocabulary\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['A', 'child', 'a', 'pink'], 'in'), (['child', 'in', 'pink', 'dress'], 'a'), (['in', 'a', 'dress', 'is'], 'pink')]\n",
      "[(0, (['A', 'child', 'a', 'pink'], 'in')), (1, (['child', 'in', 'pink', 'dress'], 'a')), (2, (['in', 'a', 'dress', 'is'], 'pink'))]\n"
     ]
    }
   ],
   "source": [
    "# Size of the context of one word, i.e. words on the left and words on the right we keep as context\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Map each word to an index\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Build the data to train the model\n",
    "data = []\n",
    "\n",
    "for i in range(CONTEXT_SIZE, len(raw_text) - CONTEXT_SIZE):\n",
    "    idx = list(range(i - CONTEXT_SIZE, i)) + list(range(i + 1, i + CONTEXT_SIZE + 1))\n",
    "    context = [raw_text[k] for k in idx]\n",
    "    target = raw_text[i]\n",
    "    \n",
    "    data.append((context, target))\n",
    "\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, context_size, embedding_dim, vocab_size):\n",
    "        \n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * 2 * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], step [1000/476702], loss: 8.9400, total loss: 8816.3221\n",
      "Epoch [1/5], step [2000/476702], loss: 0.5576, total loss: 16779.3860\n",
      "Epoch [1/5], step [3000/476702], loss: 4.9743, total loss: 23815.3299\n",
      "Epoch [1/5], step [4000/476702], loss: 2.8110, total loss: 30178.1256\n",
      "Epoch [1/5], step [5000/476702], loss: 9.2805, total loss: 36445.2720\n",
      "Epoch [1/5], step [6000/476702], loss: 7.0085, total loss: 42226.7163\n",
      "Epoch [1/5], step [7000/476702], loss: 0.4405, total loss: 47824.1943\n",
      "Epoch [1/5], step [8000/476702], loss: 6.1613, total loss: 53576.7170\n",
      "Epoch [1/5], step [9000/476702], loss: 4.5952, total loss: 59313.2254\n",
      "Epoch [1/5], step [10000/476702], loss: 8.4296, total loss: 64603.4270\n",
      "Epoch [1/5], step [11000/476702], loss: 2.1138, total loss: 69429.6583\n",
      "Epoch [1/5], step [12000/476702], loss: 0.0878, total loss: 74385.3465\n",
      "Epoch [1/5], step [13000/476702], loss: 1.9168, total loss: 79567.2594\n",
      "Epoch [1/5], step [14000/476702], loss: 1.3770, total loss: 84586.6596\n",
      "Epoch [1/5], step [15000/476702], loss: 0.1220, total loss: 89253.4267\n",
      "Epoch [1/5], step [16000/476702], loss: 8.5839, total loss: 93960.3048\n",
      "Epoch [1/5], step [17000/476702], loss: 4.5868, total loss: 98939.6976\n",
      "Epoch [1/5], step [18000/476702], loss: 1.2932, total loss: 103706.7240\n",
      "Epoch [1/5], step [19000/476702], loss: 7.0057, total loss: 108594.0926\n",
      "Epoch [1/5], step [20000/476702], loss: 10.6065, total loss: 113170.6185\n",
      "Epoch [1/5], step [21000/476702], loss: 1.1730, total loss: 117555.9027\n",
      "Epoch [1/5], step [22000/476702], loss: 4.8045, total loss: 121997.6562\n",
      "Epoch [1/5], step [23000/476702], loss: 3.3913, total loss: 126656.8622\n",
      "Epoch [1/5], step [24000/476702], loss: 10.1924, total loss: 131054.8051\n",
      "Epoch [1/5], step [25000/476702], loss: 10.7379, total loss: 135596.6984\n",
      "Epoch [1/5], step [26000/476702], loss: 7.4113, total loss: 140218.6890\n",
      "Epoch [1/5], step [27000/476702], loss: 6.3574, total loss: 144405.7912\n",
      "Epoch [1/5], step [28000/476702], loss: 9.0671, total loss: 148721.0014\n",
      "Epoch [1/5], step [29000/476702], loss: 6.1019, total loss: 152902.4293\n",
      "Epoch [1/5], step [30000/476702], loss: 1.3706, total loss: 157410.8244\n",
      "Epoch [1/5], step [31000/476702], loss: 1.6223, total loss: 161702.3074\n",
      "Epoch [1/5], step [32000/476702], loss: 11.6961, total loss: 165853.7905\n",
      "Epoch [1/5], step [33000/476702], loss: 2.6635, total loss: 170419.9590\n",
      "Epoch [1/5], step [34000/476702], loss: 11.8686, total loss: 174492.2512\n",
      "Epoch [1/5], step [35000/476702], loss: 2.6606, total loss: 178558.5470\n",
      "Epoch [1/5], step [36000/476702], loss: 2.1415, total loss: 182850.0354\n",
      "Epoch [1/5], step [37000/476702], loss: 2.5542, total loss: 187009.2321\n",
      "Epoch [1/5], step [38000/476702], loss: 1.1106, total loss: 191096.4338\n",
      "Epoch [1/5], step [39000/476702], loss: 0.7311, total loss: 195228.0221\n",
      "Epoch [1/5], step [40000/476702], loss: 3.5342, total loss: 199102.8051\n",
      "Epoch [1/5], step [41000/476702], loss: 0.8277, total loss: 202722.4092\n",
      "Epoch [1/5], step [42000/476702], loss: 3.8053, total loss: 206568.5066\n",
      "Epoch [1/5], step [43000/476702], loss: 5.2772, total loss: 210599.3972\n",
      "Epoch [1/5], step [44000/476702], loss: 0.5589, total loss: 214822.3572\n",
      "Epoch [1/5], step [45000/476702], loss: 0.2703, total loss: 219271.2777\n",
      "Epoch [1/5], step [46000/476702], loss: 4.9967, total loss: 223181.6808\n",
      "Epoch [1/5], step [47000/476702], loss: 2.6601, total loss: 226811.8617\n",
      "Epoch [1/5], step [48000/476702], loss: 3.4240, total loss: 230579.4167\n",
      "Epoch [1/5], step [49000/476702], loss: 0.8170, total loss: 235071.7645\n",
      "Epoch [1/5], step [50000/476702], loss: 0.4941, total loss: 239542.5143\n",
      "Epoch [1/5], step [51000/476702], loss: 1.0194, total loss: 243856.5656\n",
      "Epoch [1/5], step [52000/476702], loss: 0.1363, total loss: 247642.2368\n",
      "Epoch [1/5], step [53000/476702], loss: 1.7120, total loss: 251605.3637\n",
      "Epoch [1/5], step [54000/476702], loss: 1.9934, total loss: 255310.4523\n",
      "Epoch [1/5], step [55000/476702], loss: 2.8169, total loss: 259106.3596\n",
      "Epoch [1/5], step [56000/476702], loss: 2.7673, total loss: 263311.6763\n",
      "Epoch [1/5], step [57000/476702], loss: 0.1604, total loss: 266834.6667\n",
      "Epoch [1/5], step [58000/476702], loss: 0.0315, total loss: 271043.1692\n",
      "Epoch [1/5], step [59000/476702], loss: 0.0808, total loss: 274886.1549\n",
      "Epoch [1/5], step [60000/476702], loss: 5.2368, total loss: 278665.1892\n",
      "Epoch [1/5], step [61000/476702], loss: 0.7970, total loss: 282290.0319\n",
      "Epoch [1/5], step [62000/476702], loss: 0.6425, total loss: 285807.1541\n",
      "Epoch [1/5], step [63000/476702], loss: 0.4176, total loss: 289553.0762\n",
      "Epoch [1/5], step [64000/476702], loss: 0.1883, total loss: 293361.7015\n",
      "Epoch [1/5], step [65000/476702], loss: 0.5941, total loss: 297153.1350\n",
      "Epoch [1/5], step [66000/476702], loss: 0.2164, total loss: 300537.6863\n",
      "Epoch [1/5], step [67000/476702], loss: 3.4237, total loss: 304241.6901\n",
      "Epoch [1/5], step [68000/476702], loss: 0.8824, total loss: 307803.7775\n",
      "Epoch [1/5], step [69000/476702], loss: 2.1274, total loss: 311863.2235\n",
      "Epoch [1/5], step [70000/476702], loss: 9.9939, total loss: 315739.9347\n",
      "Epoch [1/5], step [71000/476702], loss: 15.2832, total loss: 319658.5453\n",
      "Epoch [1/5], step [72000/476702], loss: 3.2754, total loss: 323420.5149\n",
      "Epoch [1/5], step [73000/476702], loss: 1.8295, total loss: 327302.2191\n",
      "Epoch [1/5], step [74000/476702], loss: 0.0784, total loss: 331040.0834\n",
      "Epoch [1/5], step [75000/476702], loss: 0.1139, total loss: 334403.4010\n",
      "Epoch [1/5], step [76000/476702], loss: 0.9832, total loss: 338011.6751\n",
      "Epoch [1/5], step [77000/476702], loss: 0.1048, total loss: 341995.2140\n",
      "Epoch [1/5], step [78000/476702], loss: 5.1389, total loss: 345574.9546\n",
      "Epoch [1/5], step [79000/476702], loss: 11.6949, total loss: 349387.3314\n",
      "Epoch [1/5], step [80000/476702], loss: 9.7000, total loss: 353105.4335\n",
      "Epoch [1/5], step [81000/476702], loss: 1.6325, total loss: 356826.7749\n",
      "Epoch [1/5], step [82000/476702], loss: 0.2004, total loss: 360355.1512\n",
      "Epoch [1/5], step [83000/476702], loss: 3.2109, total loss: 364042.3308\n",
      "Epoch [1/5], step [84000/476702], loss: 3.2593, total loss: 367675.7780\n",
      "Epoch [1/5], step [85000/476702], loss: 3.1569, total loss: 371167.9354\n",
      "Epoch [1/5], step [86000/476702], loss: 0.0191, total loss: 374769.0843\n",
      "Epoch [1/5], step [87000/476702], loss: 8.1266, total loss: 378609.7590\n",
      "Epoch [1/5], step [88000/476702], loss: 2.4562, total loss: 381823.4588\n",
      "Epoch [1/5], step [89000/476702], loss: 1.6075, total loss: 384706.5655\n",
      "Epoch [1/5], step [90000/476702], loss: 6.9670, total loss: 388193.2334\n",
      "Epoch [1/5], step [91000/476702], loss: 8.9456, total loss: 391490.5940\n",
      "Epoch [1/5], step [92000/476702], loss: 5.8636, total loss: 394840.4600\n",
      "Epoch [1/5], step [93000/476702], loss: 0.0464, total loss: 398155.5428\n",
      "Epoch [1/5], step [94000/476702], loss: 6.9481, total loss: 401581.0871\n",
      "Epoch [1/5], step [95000/476702], loss: 7.1572, total loss: 405226.7294\n",
      "Epoch [1/5], step [96000/476702], loss: 4.1381, total loss: 408584.6997\n",
      "Epoch [1/5], step [97000/476702], loss: 7.4796, total loss: 412333.6962\n",
      "Epoch [1/5], step [98000/476702], loss: 3.6541, total loss: 415846.7584\n",
      "Epoch [1/5], step [99000/476702], loss: 1.6349, total loss: 419105.7904\n",
      "Epoch [1/5], step [100000/476702], loss: 3.5137, total loss: 422686.3703\n",
      "Epoch [1/5], step [101000/476702], loss: 0.7294, total loss: 425983.4567\n",
      "Epoch [1/5], step [102000/476702], loss: 3.5570, total loss: 429570.9189\n",
      "Epoch [1/5], step [103000/476702], loss: 3.3835, total loss: 433079.7465\n",
      "Epoch [1/5], step [104000/476702], loss: 4.3174, total loss: 436423.3886\n",
      "Epoch [1/5], step [105000/476702], loss: 1.8920, total loss: 440287.9824\n",
      "Epoch [1/5], step [106000/476702], loss: 1.9332, total loss: 444496.4742\n",
      "Epoch [1/5], step [107000/476702], loss: 9.8505, total loss: 448163.7688\n",
      "Epoch [1/5], step [108000/476702], loss: 0.0878, total loss: 451578.0697\n",
      "Epoch [1/5], step [109000/476702], loss: 2.5522, total loss: 454713.4612\n",
      "Epoch [1/5], step [110000/476702], loss: 1.8077, total loss: 458047.2792\n",
      "Epoch [1/5], step [111000/476702], loss: 0.0982, total loss: 461313.5024\n",
      "Epoch [1/5], step [112000/476702], loss: 5.1482, total loss: 464684.5792\n",
      "Epoch [1/5], step [113000/476702], loss: 0.7137, total loss: 468033.1100\n",
      "Epoch [1/5], step [114000/476702], loss: 2.0528, total loss: 471308.4490\n",
      "Epoch [1/5], step [115000/476702], loss: 2.3348, total loss: 474898.9089\n",
      "Epoch [1/5], step [116000/476702], loss: 1.9632, total loss: 478534.7320\n",
      "Epoch [1/5], step [117000/476702], loss: 1.5972, total loss: 481812.7449\n",
      "Epoch [1/5], step [118000/476702], loss: 0.3066, total loss: 485028.4877\n",
      "Epoch [1/5], step [119000/476702], loss: 2.7369, total loss: 488325.2945\n",
      "Epoch [1/5], step [120000/476702], loss: 0.0008, total loss: 492284.8162\n",
      "Epoch [1/5], step [121000/476702], loss: 10.7001, total loss: 495394.9684\n",
      "Epoch [1/5], step [122000/476702], loss: 0.4108, total loss: 498720.6605\n",
      "Epoch [1/5], step [123000/476702], loss: 10.7628, total loss: 502280.1812\n",
      "Epoch [1/5], step [124000/476702], loss: 7.3195, total loss: 505785.7733\n",
      "Epoch [1/5], step [125000/476702], loss: 3.0492, total loss: 508909.4876\n",
      "Epoch [1/5], step [126000/476702], loss: 4.4792, total loss: 511940.3678\n",
      "Epoch [1/5], step [127000/476702], loss: 0.0176, total loss: 515395.5628\n",
      "Epoch [1/5], step [128000/476702], loss: 3.8434, total loss: 519244.3390\n",
      "Epoch [1/5], step [129000/476702], loss: 12.2658, total loss: 522752.5405\n",
      "Epoch [1/5], step [130000/476702], loss: 3.5923, total loss: 526209.2121\n",
      "Epoch [1/5], step [131000/476702], loss: 0.8626, total loss: 529705.1879\n",
      "Epoch [1/5], step [132000/476702], loss: 0.6039, total loss: 532908.2678\n",
      "Epoch [1/5], step [133000/476702], loss: 4.6297, total loss: 535943.4092\n",
      "Epoch [1/5], step [134000/476702], loss: 1.0462, total loss: 539346.2075\n",
      "Epoch [1/5], step [135000/476702], loss: 1.3580, total loss: 542659.4391\n",
      "Epoch [1/5], step [136000/476702], loss: 0.2823, total loss: 545595.4298\n",
      "Epoch [1/5], step [137000/476702], loss: 11.8635, total loss: 548945.3195\n",
      "Epoch [1/5], step [138000/476702], loss: 9.8903, total loss: 552235.8491\n",
      "Epoch [1/5], step [139000/476702], loss: 2.1796, total loss: 555352.3245\n",
      "Epoch [1/5], step [140000/476702], loss: 4.8216, total loss: 558613.7245\n",
      "Epoch [1/5], step [141000/476702], loss: 4.3181, total loss: 561619.6038\n",
      "Epoch [1/5], step [142000/476702], loss: 5.8577, total loss: 564775.2189\n",
      "Epoch [1/5], step [143000/476702], loss: 0.3208, total loss: 568259.7541\n",
      "Epoch [1/5], step [144000/476702], loss: 1.9186, total loss: 571684.4752\n",
      "Epoch [1/5], step [145000/476702], loss: 0.5691, total loss: 575094.9370\n",
      "Epoch [1/5], step [146000/476702], loss: 0.2468, total loss: 578448.9951\n",
      "Epoch [1/5], step [147000/476702], loss: 3.0414, total loss: 581702.5442\n",
      "Epoch [1/5], step [148000/476702], loss: 2.9564, total loss: 585276.4498\n",
      "Epoch [1/5], step [149000/476702], loss: 0.1405, total loss: 588394.9249\n",
      "Epoch [1/5], step [150000/476702], loss: 1.8083, total loss: 591433.4187\n",
      "Epoch [1/5], step [151000/476702], loss: 5.6356, total loss: 595039.1734\n",
      "Epoch [1/5], step [152000/476702], loss: 1.5295, total loss: 598627.8830\n",
      "Epoch [1/5], step [153000/476702], loss: 0.0248, total loss: 602227.4976\n",
      "Epoch [1/5], step [154000/476702], loss: 6.2382, total loss: 605775.0429\n",
      "Epoch [1/5], step [155000/476702], loss: 2.2627, total loss: 609194.1222\n",
      "Epoch [1/5], step [156000/476702], loss: 0.6351, total loss: 612289.2363\n",
      "Epoch [1/5], step [157000/476702], loss: 1.5926, total loss: 615704.9166\n",
      "Epoch [1/5], step [158000/476702], loss: 1.2915, total loss: 619343.4863\n",
      "Epoch [1/5], step [159000/476702], loss: 1.1734, total loss: 622926.7518\n",
      "Epoch [1/5], step [160000/476702], loss: 2.1798, total loss: 626425.5380\n",
      "Epoch [1/5], step [161000/476702], loss: 0.0198, total loss: 629768.6703\n",
      "Epoch [1/5], step [162000/476702], loss: 0.0159, total loss: 633070.5685\n",
      "Epoch [1/5], step [163000/476702], loss: 12.3395, total loss: 636629.2102\n",
      "Epoch [1/5], step [164000/476702], loss: 0.2132, total loss: 640265.1247\n",
      "Epoch [1/5], step [165000/476702], loss: 4.4971, total loss: 643491.7858\n",
      "Epoch [1/5], step [166000/476702], loss: 0.8733, total loss: 646471.8457\n",
      "Epoch [1/5], step [167000/476702], loss: 0.0263, total loss: 649960.9047\n",
      "Epoch [1/5], step [168000/476702], loss: 6.0919, total loss: 653432.3518\n",
      "Epoch [1/5], step [169000/476702], loss: 2.3124, total loss: 657022.8125\n",
      "Epoch [1/5], step [170000/476702], loss: 2.1503, total loss: 660368.0622\n",
      "Epoch [1/5], step [171000/476702], loss: 6.5888, total loss: 663392.7603\n",
      "Epoch [1/5], step [172000/476702], loss: 0.2763, total loss: 666984.3551\n",
      "Epoch [1/5], step [173000/476702], loss: 5.2554, total loss: 670336.3901\n",
      "Epoch [1/5], step [174000/476702], loss: 1.1095, total loss: 673755.5434\n",
      "Epoch [1/5], step [175000/476702], loss: 0.0572, total loss: 676814.5058\n",
      "Epoch [1/5], step [176000/476702], loss: 3.4289, total loss: 680020.7519\n",
      "Epoch [1/5], step [177000/476702], loss: 4.2262, total loss: 683227.4130\n",
      "Epoch [1/5], step [178000/476702], loss: 0.1171, total loss: 686507.3706\n",
      "Epoch [1/5], step [179000/476702], loss: 1.7862, total loss: 690192.5754\n",
      "Epoch [1/5], step [180000/476702], loss: 5.8768, total loss: 693458.5447\n",
      "Epoch [1/5], step [181000/476702], loss: 3.3882, total loss: 696842.3890\n",
      "Epoch [1/5], step [182000/476702], loss: 0.2239, total loss: 700150.2739\n",
      "Epoch [1/5], step [183000/476702], loss: 10.7214, total loss: 703391.1468\n",
      "Epoch [1/5], step [184000/476702], loss: 6.4394, total loss: 706426.2204\n",
      "Epoch [1/5], step [185000/476702], loss: 0.0339, total loss: 709425.8535\n",
      "Epoch [1/5], step [186000/476702], loss: 6.1431, total loss: 712576.4856\n",
      "Epoch [1/5], step [187000/476702], loss: 3.5204, total loss: 715786.2339\n",
      "Epoch [1/5], step [188000/476702], loss: 4.1013, total loss: 719371.7309\n",
      "Epoch [1/5], step [189000/476702], loss: 5.3122, total loss: 722833.7910\n",
      "Epoch [1/5], step [190000/476702], loss: 5.7039, total loss: 725763.5509\n",
      "Epoch [1/5], step [191000/476702], loss: 7.8020, total loss: 729419.8376\n",
      "Epoch [1/5], step [192000/476702], loss: 5.4751, total loss: 732856.0110\n",
      "Epoch [1/5], step [193000/476702], loss: 0.8956, total loss: 736306.0089\n",
      "Epoch [1/5], step [194000/476702], loss: 10.6113, total loss: 739344.6184\n",
      "Epoch [1/5], step [195000/476702], loss: 3.9621, total loss: 742645.8929\n",
      "Epoch [1/5], step [196000/476702], loss: 5.1517, total loss: 745984.1326\n",
      "Epoch [1/5], step [197000/476702], loss: 0.4049, total loss: 749058.6571\n",
      "Epoch [1/5], step [198000/476702], loss: 1.3273, total loss: 752688.5641\n",
      "Epoch [1/5], step [199000/476702], loss: 4.9409, total loss: 756269.6418\n",
      "Epoch [1/5], step [200000/476702], loss: 7.3901, total loss: 759907.0551\n",
      "Epoch [1/5], step [201000/476702], loss: 1.4815, total loss: 763481.0943\n",
      "Epoch [1/5], step [202000/476702], loss: 0.3850, total loss: 766884.1316\n",
      "Epoch [1/5], step [203000/476702], loss: 4.5897, total loss: 770582.5749\n",
      "Epoch [1/5], step [204000/476702], loss: 0.7274, total loss: 773859.0135\n",
      "Epoch [1/5], step [205000/476702], loss: 7.2532, total loss: 777324.3254\n",
      "Epoch [1/5], step [206000/476702], loss: 0.2829, total loss: 780389.6742\n",
      "Epoch [1/5], step [207000/476702], loss: 10.2366, total loss: 783738.0432\n",
      "Epoch [1/5], step [208000/476702], loss: 1.2590, total loss: 787296.2299\n",
      "Epoch [1/5], step [209000/476702], loss: 5.5481, total loss: 790674.3884\n",
      "Epoch [1/5], step [210000/476702], loss: 0.5430, total loss: 794156.0045\n",
      "Epoch [1/5], step [211000/476702], loss: 0.7353, total loss: 797600.8345\n",
      "Epoch [1/5], step [212000/476702], loss: 11.9501, total loss: 801022.0848\n",
      "Epoch [1/5], step [213000/476702], loss: 1.2077, total loss: 804190.1855\n",
      "Epoch [1/5], step [214000/476702], loss: 2.6473, total loss: 807666.1496\n",
      "Epoch [1/5], step [215000/476702], loss: 1.5988, total loss: 811148.1903\n",
      "Epoch [1/5], step [216000/476702], loss: 3.7690, total loss: 815026.4514\n",
      "Epoch [1/5], step [217000/476702], loss: 0.0383, total loss: 819013.8952\n",
      "Epoch [1/5], step [218000/476702], loss: 1.7261, total loss: 822840.3046\n",
      "Epoch [1/5], step [219000/476702], loss: 0.4598, total loss: 826724.1324\n",
      "Epoch [1/5], step [220000/476702], loss: 0.2954, total loss: 830261.7109\n",
      "Epoch [1/5], step [221000/476702], loss: 1.8663, total loss: 833375.1391\n",
      "Epoch [1/5], step [222000/476702], loss: 0.0349, total loss: 836682.2716\n",
      "Epoch [1/5], step [223000/476702], loss: 0.4631, total loss: 839834.8643\n",
      "Epoch [1/5], step [224000/476702], loss: 1.5476, total loss: 842960.9299\n",
      "Epoch [1/5], step [225000/476702], loss: 0.0706, total loss: 846318.9357\n",
      "Epoch [1/5], step [226000/476702], loss: 0.0467, total loss: 849629.8011\n",
      "Epoch [1/5], step [227000/476702], loss: 0.0193, total loss: 853033.5682\n",
      "Epoch [1/5], step [228000/476702], loss: 7.4140, total loss: 856875.3430\n",
      "Epoch [1/5], step [229000/476702], loss: 4.4119, total loss: 860428.6258\n",
      "Epoch [1/5], step [230000/476702], loss: 0.2830, total loss: 863829.2157\n",
      "Epoch [1/5], step [231000/476702], loss: 4.0967, total loss: 867038.5995\n",
      "Epoch [1/5], step [232000/476702], loss: 1.8212, total loss: 870015.2614\n",
      "Epoch [1/5], step [233000/476702], loss: 0.0303, total loss: 873337.7822\n",
      "Epoch [1/5], step [234000/476702], loss: 1.5898, total loss: 876547.6154\n",
      "Epoch [1/5], step [235000/476702], loss: 0.2969, total loss: 880044.7742\n",
      "Epoch [1/5], step [236000/476702], loss: 0.0150, total loss: 883373.4782\n",
      "Epoch [1/5], step [237000/476702], loss: 2.9901, total loss: 887168.7427\n",
      "Epoch [1/5], step [238000/476702], loss: 9.8437, total loss: 890968.2646\n",
      "Epoch [1/5], step [239000/476702], loss: 1.2659, total loss: 894423.4435\n",
      "Epoch [1/5], step [240000/476702], loss: 1.4629, total loss: 897868.6086\n",
      "Epoch [1/5], step [241000/476702], loss: 2.7711, total loss: 900926.9831\n",
      "Epoch [1/5], step [242000/476702], loss: 0.4322, total loss: 904676.1553\n",
      "Epoch [1/5], step [243000/476702], loss: 0.0037, total loss: 907904.4325\n",
      "Epoch [1/5], step [244000/476702], loss: 0.0118, total loss: 910875.9655\n",
      "Epoch [1/5], step [245000/476702], loss: 1.1662, total loss: 914510.0500\n",
      "Epoch [1/5], step [246000/476702], loss: 13.3758, total loss: 917893.5008\n",
      "Epoch [1/5], step [247000/476702], loss: 6.4960, total loss: 921210.4531\n",
      "Epoch [1/5], step [248000/476702], loss: 5.0298, total loss: 924244.9834\n",
      "Epoch [1/5], step [249000/476702], loss: 5.2550, total loss: 927550.8401\n",
      "Epoch [1/5], step [250000/476702], loss: 7.1184, total loss: 930553.2130\n",
      "Epoch [1/5], step [251000/476702], loss: 6.8808, total loss: 933608.9427\n",
      "Epoch [1/5], step [252000/476702], loss: 3.4444, total loss: 936682.9217\n",
      "Epoch [1/5], step [253000/476702], loss: 5.0001, total loss: 940062.7427\n",
      "Epoch [1/5], step [254000/476702], loss: 8.5720, total loss: 943354.9321\n",
      "Epoch [1/5], step [255000/476702], loss: 2.7283, total loss: 946597.7511\n",
      "Epoch [1/5], step [256000/476702], loss: 0.0613, total loss: 949835.0312\n",
      "Epoch [1/5], step [257000/476702], loss: 0.5247, total loss: 953096.9947\n",
      "Epoch [1/5], step [258000/476702], loss: 10.1815, total loss: 956467.9106\n",
      "Epoch [1/5], step [259000/476702], loss: 5.6014, total loss: 960068.5700\n",
      "Epoch [1/5], step [260000/476702], loss: 3.7700, total loss: 963360.0127\n",
      "Epoch [1/5], step [261000/476702], loss: 2.1995, total loss: 966523.2330\n",
      "Epoch [1/5], step [262000/476702], loss: 2.2163, total loss: 969640.8411\n",
      "Epoch [1/5], step [263000/476702], loss: 0.2620, total loss: 973221.3613\n",
      "Epoch [1/5], step [264000/476702], loss: 0.0026, total loss: 976333.0388\n",
      "Epoch [1/5], step [265000/476702], loss: 0.1478, total loss: 979902.5000\n",
      "Epoch [1/5], step [266000/476702], loss: 0.0697, total loss: 983675.6420\n",
      "Epoch [1/5], step [267000/476702], loss: 0.6214, total loss: 987336.7099\n",
      "Epoch [1/5], step [268000/476702], loss: 3.0365, total loss: 990583.6668\n",
      "Epoch [1/5], step [269000/476702], loss: 8.6802, total loss: 994018.1847\n",
      "Epoch [1/5], step [270000/476702], loss: 0.0309, total loss: 997395.4590\n",
      "Epoch [1/5], step [271000/476702], loss: 6.2462, total loss: 1000598.5496\n",
      "Epoch [1/5], step [272000/476702], loss: 2.9014, total loss: 1003753.1822\n",
      "Epoch [1/5], step [273000/476702], loss: 0.0925, total loss: 1007349.7403\n",
      "Epoch [1/5], step [274000/476702], loss: 0.0020, total loss: 1010417.5383\n",
      "Epoch [1/5], step [275000/476702], loss: 10.8899, total loss: 1013736.6784\n",
      "Epoch [1/5], step [276000/476702], loss: 0.8829, total loss: 1016820.2926\n",
      "Epoch [1/5], step [277000/476702], loss: 11.9895, total loss: 1020408.4956\n",
      "Epoch [1/5], step [278000/476702], loss: 3.4624, total loss: 1023768.3978\n",
      "Epoch [1/5], step [279000/476702], loss: 3.9596, total loss: 1026972.1568\n",
      "Epoch [1/5], step [280000/476702], loss: 12.4136, total loss: 1030446.6905\n",
      "Epoch [1/5], step [281000/476702], loss: 0.5394, total loss: 1033808.0966\n",
      "Epoch [1/5], step [282000/476702], loss: 4.6406, total loss: 1037036.8328\n",
      "Epoch [1/5], step [283000/476702], loss: 5.3014, total loss: 1040354.2654\n",
      "Epoch [1/5], step [284000/476702], loss: 1.5974, total loss: 1043580.1406\n",
      "Epoch [1/5], step [285000/476702], loss: 1.9599, total loss: 1046875.1482\n",
      "Epoch [1/5], step [286000/476702], loss: 3.9883, total loss: 1050411.8993\n",
      "Epoch [1/5], step [287000/476702], loss: 0.3871, total loss: 1053350.4315\n",
      "Epoch [1/5], step [288000/476702], loss: 1.2605, total loss: 1056912.0804\n",
      "Epoch [1/5], step [289000/476702], loss: 0.0378, total loss: 1059947.2421\n",
      "Epoch [1/5], step [290000/476702], loss: 1.3766, total loss: 1063525.5517\n",
      "Epoch [1/5], step [291000/476702], loss: 2.7249, total loss: 1066821.5983\n",
      "Epoch [1/5], step [292000/476702], loss: 0.1041, total loss: 1070455.0675\n",
      "Epoch [1/5], step [293000/476702], loss: 0.0182, total loss: 1073589.0772\n",
      "Epoch [1/5], step [294000/476702], loss: 7.9735, total loss: 1076757.1164\n",
      "Epoch [1/5], step [295000/476702], loss: 0.1183, total loss: 1080177.6542\n",
      "Epoch [1/5], step [296000/476702], loss: 0.0836, total loss: 1083243.0657\n",
      "Epoch [1/5], step [297000/476702], loss: 0.6136, total loss: 1086458.1947\n",
      "Epoch [1/5], step [298000/476702], loss: 1.0820, total loss: 1089550.1540\n",
      "Epoch [1/5], step [299000/476702], loss: 2.2140, total loss: 1092642.8405\n",
      "Epoch [1/5], step [300000/476702], loss: 0.2693, total loss: 1095747.5948\n",
      "Epoch [1/5], step [301000/476702], loss: 1.8603, total loss: 1098959.7582\n",
      "Epoch [1/5], step [302000/476702], loss: 6.9516, total loss: 1102289.6236\n",
      "Epoch [1/5], step [303000/476702], loss: 0.2042, total loss: 1105344.8437\n",
      "Epoch [1/5], step [304000/476702], loss: 0.4053, total loss: 1108453.0712\n",
      "Epoch [1/5], step [305000/476702], loss: 0.9943, total loss: 1111639.2589\n",
      "Epoch [1/5], step [306000/476702], loss: 1.9530, total loss: 1114815.5690\n",
      "Epoch [1/5], step [307000/476702], loss: 0.0581, total loss: 1118179.1365\n",
      "Epoch [1/5], step [308000/476702], loss: 1.4198, total loss: 1121626.8675\n",
      "Epoch [1/5], step [309000/476702], loss: 3.5196, total loss: 1124541.9998\n",
      "Epoch [1/5], step [310000/476702], loss: 10.1487, total loss: 1127824.2505\n",
      "Epoch [1/5], step [311000/476702], loss: 7.2216, total loss: 1130601.7233\n",
      "Epoch [1/5], step [312000/476702], loss: 8.1248, total loss: 1134167.7365\n",
      "Epoch [1/5], step [313000/476702], loss: 9.0297, total loss: 1137451.7785\n",
      "Epoch [1/5], step [314000/476702], loss: 3.3431, total loss: 1140412.7209\n",
      "Epoch [1/5], step [315000/476702], loss: 1.9683, total loss: 1143651.8053\n",
      "Epoch [1/5], step [316000/476702], loss: 1.9059, total loss: 1147001.5348\n",
      "Epoch [1/5], step [317000/476702], loss: 0.1010, total loss: 1149942.8582\n",
      "Epoch [1/5], step [318000/476702], loss: 0.0647, total loss: 1153032.3376\n",
      "Epoch [1/5], step [319000/476702], loss: 5.3020, total loss: 1155875.2772\n",
      "Epoch [1/5], step [320000/476702], loss: 4.8585, total loss: 1159236.3167\n",
      "Epoch [1/5], step [321000/476702], loss: 2.3554, total loss: 1162214.6138\n",
      "Epoch [1/5], step [322000/476702], loss: 0.0375, total loss: 1165329.2775\n",
      "Epoch [1/5], step [323000/476702], loss: 2.0587, total loss: 1168533.1108\n",
      "Epoch [1/5], step [324000/476702], loss: 1.9221, total loss: 1171795.0511\n",
      "Epoch [1/5], step [325000/476702], loss: 2.9157, total loss: 1174894.8148\n",
      "Epoch [1/5], step [326000/476702], loss: 0.3701, total loss: 1178152.8784\n",
      "Epoch [1/5], step [327000/476702], loss: 4.4421, total loss: 1181504.1962\n",
      "Epoch [1/5], step [328000/476702], loss: 0.0693, total loss: 1184451.6355\n",
      "Epoch [1/5], step [329000/476702], loss: 6.9756, total loss: 1187739.9065\n",
      "Epoch [1/5], step [330000/476702], loss: 4.8188, total loss: 1190889.9800\n",
      "Epoch [1/5], step [331000/476702], loss: 1.2102, total loss: 1194162.3391\n",
      "Epoch [1/5], step [332000/476702], loss: 0.7997, total loss: 1197199.8665\n",
      "Epoch [1/5], step [333000/476702], loss: 4.8371, total loss: 1200330.3676\n",
      "Epoch [1/5], step [334000/476702], loss: 6.4423, total loss: 1203742.6691\n",
      "Epoch [1/5], step [335000/476702], loss: 8.3315, total loss: 1206801.1598\n",
      "Epoch [1/5], step [336000/476702], loss: 0.4320, total loss: 1210176.1042\n",
      "Epoch [1/5], step [337000/476702], loss: 1.6774, total loss: 1213044.3242\n",
      "Epoch [1/5], step [338000/476702], loss: 0.4816, total loss: 1215825.0424\n",
      "Epoch [1/5], step [339000/476702], loss: 0.1053, total loss: 1219468.6564\n",
      "Epoch [1/5], step [340000/476702], loss: 0.7921, total loss: 1222293.2036\n",
      "Epoch [1/5], step [341000/476702], loss: 4.0445, total loss: 1225727.5414\n",
      "Epoch [1/5], step [342000/476702], loss: 0.8983, total loss: 1229386.2478\n",
      "Epoch [1/5], step [343000/476702], loss: 0.0005, total loss: 1232422.4679\n",
      "Epoch [1/5], step [344000/476702], loss: 10.2546, total loss: 1235255.6454\n",
      "Epoch [1/5], step [345000/476702], loss: 6.7983, total loss: 1238661.8233\n",
      "Epoch [1/5], step [346000/476702], loss: 6.0717, total loss: 1241712.8510\n",
      "Epoch [1/5], step [347000/476702], loss: 0.1806, total loss: 1245043.5098\n",
      "Epoch [1/5], step [348000/476702], loss: 0.6116, total loss: 1248100.3355\n",
      "Epoch [1/5], step [349000/476702], loss: 8.5551, total loss: 1251363.2528\n",
      "Epoch [1/5], step [350000/476702], loss: 2.2874, total loss: 1254752.9061\n",
      "Epoch [1/5], step [351000/476702], loss: 0.0443, total loss: 1258137.5830\n",
      "Epoch [1/5], step [352000/476702], loss: 9.8365, total loss: 1261420.2639\n",
      "Epoch [1/5], step [353000/476702], loss: 7.6644, total loss: 1264733.5147\n",
      "Epoch [1/5], step [354000/476702], loss: 1.3939, total loss: 1268230.7923\n",
      "Epoch [1/5], step [355000/476702], loss: 3.1171, total loss: 1271199.9007\n",
      "Epoch [1/5], step [356000/476702], loss: 0.5469, total loss: 1274400.3963\n",
      "Epoch [1/5], step [357000/476702], loss: 0.4429, total loss: 1277514.9109\n",
      "Epoch [1/5], step [358000/476702], loss: 1.0895, total loss: 1280475.1416\n",
      "Epoch [1/5], step [359000/476702], loss: 0.7526, total loss: 1283322.2452\n",
      "Epoch [1/5], step [360000/476702], loss: 0.0112, total loss: 1286656.8825\n",
      "Epoch [1/5], step [361000/476702], loss: 2.2957, total loss: 1290000.2678\n",
      "Epoch [1/5], step [362000/476702], loss: 3.4734, total loss: 1293197.4858\n",
      "Epoch [1/5], step [363000/476702], loss: 1.0091, total loss: 1296010.8000\n",
      "Epoch [1/5], step [364000/476702], loss: 0.1274, total loss: 1298960.6574\n",
      "Epoch [1/5], step [365000/476702], loss: 0.4845, total loss: 1302011.9193\n",
      "Epoch [1/5], step [366000/476702], loss: 5.9705, total loss: 1305101.6212\n",
      "Epoch [1/5], step [367000/476702], loss: 0.9717, total loss: 1308404.1629\n",
      "Epoch [1/5], step [368000/476702], loss: 2.0912, total loss: 1311438.6425\n",
      "Epoch [1/5], step [369000/476702], loss: 6.0969, total loss: 1314537.2037\n",
      "Epoch [1/5], step [370000/476702], loss: 3.5061, total loss: 1317906.2485\n",
      "Epoch [1/5], step [371000/476702], loss: 5.5170, total loss: 1321226.2351\n",
      "Epoch [1/5], step [372000/476702], loss: 4.1078, total loss: 1324605.4747\n",
      "Epoch [1/5], step [373000/476702], loss: 0.0211, total loss: 1327411.6746\n",
      "Epoch [1/5], step [374000/476702], loss: 12.4073, total loss: 1330541.1621\n",
      "Epoch [1/5], step [375000/476702], loss: 1.3548, total loss: 1333710.3977\n",
      "Epoch [1/5], step [376000/476702], loss: 0.0101, total loss: 1337178.5295\n",
      "Epoch [1/5], step [377000/476702], loss: 8.0313, total loss: 1340660.0867\n",
      "Epoch [1/5], step [378000/476702], loss: 7.6767, total loss: 1344269.6597\n",
      "Epoch [1/5], step [379000/476702], loss: 2.7096, total loss: 1347675.0617\n",
      "Epoch [1/5], step [380000/476702], loss: 0.0452, total loss: 1350884.1450\n",
      "Epoch [1/5], step [381000/476702], loss: 11.0910, total loss: 1354343.6102\n",
      "Epoch [1/5], step [382000/476702], loss: 2.1504, total loss: 1357512.3504\n",
      "Epoch [1/5], step [383000/476702], loss: 0.0845, total loss: 1360336.0037\n",
      "Epoch [1/5], step [384000/476702], loss: 1.4412, total loss: 1363606.6326\n",
      "Epoch [1/5], step [385000/476702], loss: 0.0275, total loss: 1366764.8401\n",
      "Epoch [1/5], step [386000/476702], loss: 6.8276, total loss: 1370065.1033\n",
      "Epoch [1/5], step [387000/476702], loss: 4.9479, total loss: 1372973.1955\n",
      "Epoch [1/5], step [388000/476702], loss: 4.7085, total loss: 1375677.1828\n",
      "Epoch [1/5], step [389000/476702], loss: 0.0854, total loss: 1378760.0434\n",
      "Epoch [1/5], step [390000/476702], loss: 0.1886, total loss: 1382172.6636\n",
      "Epoch [1/5], step [391000/476702], loss: 5.1954, total loss: 1385270.6246\n",
      "Epoch [1/5], step [392000/476702], loss: 0.0034, total loss: 1388407.3138\n",
      "Epoch [1/5], step [393000/476702], loss: 0.0056, total loss: 1391618.3182\n",
      "Epoch [1/5], step [394000/476702], loss: 0.0141, total loss: 1394819.5409\n",
      "Epoch [1/5], step [395000/476702], loss: 4.4287, total loss: 1397884.7392\n",
      "Epoch [1/5], step [396000/476702], loss: 1.8943, total loss: 1400961.9681\n",
      "Epoch [1/5], step [397000/476702], loss: 5.2374, total loss: 1404049.7577\n",
      "Epoch [1/5], step [398000/476702], loss: 2.0712, total loss: 1407271.3982\n",
      "Epoch [1/5], step [399000/476702], loss: 3.1812, total loss: 1410383.3089\n",
      "Epoch [1/5], step [400000/476702], loss: 0.1327, total loss: 1413302.2867\n",
      "Epoch [1/5], step [401000/476702], loss: 7.2172, total loss: 1416818.3104\n",
      "Epoch [1/5], step [402000/476702], loss: 1.7346, total loss: 1419905.4676\n",
      "Epoch [1/5], step [403000/476702], loss: 0.7800, total loss: 1423433.0576\n",
      "Epoch [1/5], step [404000/476702], loss: 0.1337, total loss: 1426948.5200\n",
      "Epoch [1/5], step [405000/476702], loss: 5.3425, total loss: 1429995.6150\n",
      "Epoch [1/5], step [406000/476702], loss: 4.9612, total loss: 1433350.7363\n",
      "Epoch [1/5], step [407000/476702], loss: 0.1014, total loss: 1436511.4161\n",
      "Epoch [1/5], step [408000/476702], loss: 0.1434, total loss: 1439984.7541\n",
      "Epoch [1/5], step [409000/476702], loss: 2.3096, total loss: 1443148.4699\n",
      "Epoch [1/5], step [410000/476702], loss: 8.3928, total loss: 1446182.0949\n",
      "Epoch [1/5], step [411000/476702], loss: 3.1120, total loss: 1449111.2515\n",
      "Epoch [1/5], step [412000/476702], loss: 2.1209, total loss: 1452342.6984\n",
      "Epoch [1/5], step [413000/476702], loss: 3.9005, total loss: 1455354.2829\n",
      "Epoch [1/5], step [414000/476702], loss: 9.9431, total loss: 1458411.6599\n",
      "Epoch [1/5], step [415000/476702], loss: 1.4233, total loss: 1461565.8095\n",
      "Epoch [1/5], step [416000/476702], loss: 0.1722, total loss: 1464844.1885\n",
      "Epoch [1/5], step [417000/476702], loss: 0.8301, total loss: 1467833.8465\n",
      "Epoch [1/5], step [418000/476702], loss: 0.6290, total loss: 1471242.5326\n",
      "Epoch [1/5], step [419000/476702], loss: 7.0942, total loss: 1473932.6013\n",
      "Epoch [1/5], step [420000/476702], loss: 4.2085, total loss: 1477077.1197\n",
      "Epoch [1/5], step [421000/476702], loss: 0.0407, total loss: 1480238.3275\n",
      "Epoch [1/5], step [422000/476702], loss: 0.9931, total loss: 1483032.0513\n",
      "Epoch [1/5], step [423000/476702], loss: 0.0047, total loss: 1485876.1624\n",
      "Epoch [1/5], step [424000/476702], loss: 0.1145, total loss: 1489037.6822\n",
      "Epoch [1/5], step [425000/476702], loss: 8.9153, total loss: 1492084.5228\n",
      "Epoch [1/5], step [426000/476702], loss: 0.2221, total loss: 1494981.4739\n",
      "Epoch [1/5], step [427000/476702], loss: 0.4726, total loss: 1497834.4550\n",
      "Epoch [1/5], step [428000/476702], loss: 2.2955, total loss: 1500534.5927\n",
      "Epoch [1/5], step [429000/476702], loss: 0.2165, total loss: 1503568.7255\n",
      "Epoch [1/5], step [430000/476702], loss: 5.8330, total loss: 1506560.4759\n",
      "Epoch [1/5], step [431000/476702], loss: 1.0033, total loss: 1509390.2280\n",
      "Epoch [1/5], step [432000/476702], loss: 9.7808, total loss: 1512025.8641\n",
      "Epoch [1/5], step [433000/476702], loss: 0.2399, total loss: 1514728.1125\n",
      "Epoch [1/5], step [434000/476702], loss: 0.0007, total loss: 1517422.5394\n",
      "Epoch [1/5], step [435000/476702], loss: 4.4241, total loss: 1520427.0160\n",
      "Epoch [1/5], step [436000/476702], loss: 0.1307, total loss: 1523732.7061\n",
      "Epoch [1/5], step [437000/476702], loss: 1.4469, total loss: 1526638.8495\n",
      "Epoch [1/5], step [438000/476702], loss: 2.7110, total loss: 1529708.0874\n",
      "Epoch [1/5], step [439000/476702], loss: 7.3600, total loss: 1532609.5899\n",
      "Epoch [1/5], step [440000/476702], loss: 5.4241, total loss: 1535386.3048\n",
      "Epoch [1/5], step [441000/476702], loss: 14.3379, total loss: 1538279.1935\n",
      "Epoch [1/5], step [442000/476702], loss: 0.0107, total loss: 1541418.9482\n",
      "Epoch [1/5], step [443000/476702], loss: 1.8300, total loss: 1544560.0435\n",
      "Epoch [1/5], step [444000/476702], loss: 0.0360, total loss: 1547303.4854\n",
      "Epoch [1/5], step [445000/476702], loss: 3.4345, total loss: 1550477.4196\n",
      "Epoch [1/5], step [446000/476702], loss: 9.3252, total loss: 1553367.0784\n",
      "Epoch [1/5], step [447000/476702], loss: 5.9262, total loss: 1556592.3560\n",
      "Epoch [1/5], step [448000/476702], loss: 0.2042, total loss: 1559348.9393\n",
      "Epoch [1/5], step [449000/476702], loss: 2.4669, total loss: 1562775.7694\n",
      "Epoch [1/5], step [450000/476702], loss: 2.5796, total loss: 1565708.7348\n",
      "Epoch [1/5], step [451000/476702], loss: 1.3401, total loss: 1568769.9086\n",
      "Epoch [1/5], step [452000/476702], loss: 0.0908, total loss: 1572098.6691\n",
      "Epoch [1/5], step [453000/476702], loss: 2.2764, total loss: 1574980.3522\n",
      "Epoch [1/5], step [454000/476702], loss: 12.3838, total loss: 1577813.7617\n",
      "Epoch [1/5], step [455000/476702], loss: 5.1872, total loss: 1581028.3522\n",
      "Epoch [1/5], step [456000/476702], loss: 0.0296, total loss: 1583971.4003\n",
      "Epoch [1/5], step [457000/476702], loss: 0.5848, total loss: 1587007.7030\n",
      "Epoch [1/5], step [458000/476702], loss: 1.4853, total loss: 1589700.2158\n",
      "Epoch [1/5], step [459000/476702], loss: 0.1076, total loss: 1593023.1570\n",
      "Epoch [1/5], step [460000/476702], loss: 9.0121, total loss: 1595989.3530\n",
      "Epoch [1/5], step [461000/476702], loss: 0.2096, total loss: 1598830.1118\n",
      "Epoch [1/5], step [462000/476702], loss: 5.6110, total loss: 1601719.1815\n",
      "Epoch [1/5], step [463000/476702], loss: 1.3421, total loss: 1604534.9357\n",
      "Epoch [1/5], step [464000/476702], loss: 0.4180, total loss: 1607317.9792\n",
      "Epoch [1/5], step [465000/476702], loss: 3.7506, total loss: 1610203.5607\n",
      "Epoch [1/5], step [466000/476702], loss: 0.0872, total loss: 1613008.1055\n",
      "Epoch [1/5], step [467000/476702], loss: 4.2292, total loss: 1616211.0864\n",
      "Epoch [1/5], step [468000/476702], loss: 6.6475, total loss: 1619164.6590\n",
      "Epoch [1/5], step [469000/476702], loss: 0.3353, total loss: 1622021.3460\n",
      "Epoch [1/5], step [470000/476702], loss: 2.1166, total loss: 1624803.9038\n",
      "Epoch [1/5], step [471000/476702], loss: 0.2223, total loss: 1628024.6730\n",
      "Epoch [1/5], step [472000/476702], loss: 5.8019, total loss: 1630974.5261\n",
      "Epoch [1/5], step [473000/476702], loss: 0.4991, total loss: 1634037.6773\n",
      "Epoch [1/5], step [474000/476702], loss: 5.8158, total loss: 1636947.6858\n",
      "Epoch [1/5], step [475000/476702], loss: 6.6074, total loss: 1640372.5352\n",
      "Epoch [1/5], step [476000/476702], loss: 6.1740, total loss: 1643520.6127\n",
      "Epoch [2/5], step [1000/476702], loss: 1.7857, total loss: 2486.1335\n",
      "Epoch [2/5], step [2000/476702], loss: 0.4578, total loss: 5553.1880\n",
      "Epoch [2/5], step [3000/476702], loss: 1.9614, total loss: 8393.5134\n",
      "Epoch [2/5], step [4000/476702], loss: 1.9944, total loss: 11058.2063\n",
      "Epoch [2/5], step [5000/476702], loss: 4.1228, total loss: 14076.5545\n",
      "Epoch [2/5], step [6000/476702], loss: 1.1120, total loss: 16998.0830\n",
      "Epoch [2/5], step [7000/476702], loss: 0.7439, total loss: 19649.8903\n",
      "Epoch [2/5], step [8000/476702], loss: 2.1779, total loss: 22550.0867\n",
      "Epoch [2/5], step [9000/476702], loss: 1.1211, total loss: 25687.1855\n",
      "Epoch [2/5], step [10000/476702], loss: 2.3315, total loss: 28361.0616\n",
      "Epoch [2/5], step [11000/476702], loss: 1.6590, total loss: 30969.4484\n",
      "Epoch [2/5], step [12000/476702], loss: 0.0793, total loss: 33654.6545\n",
      "Epoch [2/5], step [13000/476702], loss: 0.0431, total loss: 36642.4297\n",
      "Epoch [2/5], step [14000/476702], loss: 1.4824, total loss: 39516.2076\n",
      "Epoch [2/5], step [15000/476702], loss: 0.0818, total loss: 42420.7933\n",
      "Epoch [2/5], step [16000/476702], loss: 0.9768, total loss: 45211.7025\n",
      "Epoch [2/5], step [17000/476702], loss: 2.4850, total loss: 48218.1606\n",
      "Epoch [2/5], step [18000/476702], loss: 1.7135, total loss: 51200.6970\n",
      "Epoch [2/5], step [19000/476702], loss: 5.1225, total loss: 54388.0945\n",
      "Epoch [2/5], step [20000/476702], loss: 3.3927, total loss: 57313.5867\n",
      "Epoch [2/5], step [21000/476702], loss: 0.2096, total loss: 60072.2262\n",
      "Epoch [2/5], step [22000/476702], loss: 3.1963, total loss: 62961.4812\n",
      "Epoch [2/5], step [23000/476702], loss: 1.3222, total loss: 65902.0984\n",
      "Epoch [2/5], step [24000/476702], loss: 3.0870, total loss: 68857.4439\n",
      "Epoch [2/5], step [25000/476702], loss: 8.7664, total loss: 71864.7481\n",
      "Epoch [2/5], step [26000/476702], loss: 3.3771, total loss: 75091.1234\n",
      "Epoch [2/5], step [27000/476702], loss: 6.2534, total loss: 77774.3080\n",
      "Epoch [2/5], step [28000/476702], loss: 10.7829, total loss: 80612.6104\n",
      "Epoch [2/5], step [29000/476702], loss: 4.7932, total loss: 83392.2359\n",
      "Epoch [2/5], step [30000/476702], loss: 0.0095, total loss: 86365.3488\n",
      "Epoch [2/5], step [31000/476702], loss: 0.4686, total loss: 89359.0764\n",
      "Epoch [2/5], step [32000/476702], loss: 12.3054, total loss: 92240.1676\n",
      "Epoch [2/5], step [33000/476702], loss: 3.1243, total loss: 95621.0999\n",
      "Epoch [2/5], step [34000/476702], loss: 4.8214, total loss: 98354.5489\n",
      "Epoch [2/5], step [35000/476702], loss: 0.2462, total loss: 101155.7196\n",
      "Epoch [2/5], step [36000/476702], loss: 1.0114, total loss: 104345.8567\n",
      "Epoch [2/5], step [37000/476702], loss: 2.3530, total loss: 107384.8743\n",
      "Epoch [2/5], step [38000/476702], loss: 0.2456, total loss: 110355.0244\n",
      "Epoch [2/5], step [39000/476702], loss: 0.0796, total loss: 113171.3698\n",
      "Epoch [2/5], step [40000/476702], loss: 3.2480, total loss: 115939.9987\n",
      "Epoch [2/5], step [41000/476702], loss: 0.4824, total loss: 118495.0265\n",
      "Epoch [2/5], step [42000/476702], loss: 2.6930, total loss: 121310.4993\n",
      "Epoch [2/5], step [43000/476702], loss: 3.3630, total loss: 124407.5181\n",
      "Epoch [2/5], step [44000/476702], loss: 0.3109, total loss: 127288.8719\n",
      "Epoch [2/5], step [45000/476702], loss: 0.1229, total loss: 130559.7314\n",
      "Epoch [2/5], step [46000/476702], loss: 4.6967, total loss: 133258.9211\n",
      "Epoch [2/5], step [47000/476702], loss: 1.5957, total loss: 135964.8258\n",
      "Epoch [2/5], step [48000/476702], loss: 3.3691, total loss: 138812.3287\n",
      "Epoch [2/5], step [49000/476702], loss: 0.2977, total loss: 142175.8611\n",
      "Epoch [2/5], step [50000/476702], loss: 0.1321, total loss: 145545.8511\n",
      "Epoch [2/5], step [51000/476702], loss: 0.6532, total loss: 148783.7313\n",
      "Epoch [2/5], step [52000/476702], loss: 0.0898, total loss: 151578.9479\n",
      "Epoch [2/5], step [53000/476702], loss: 0.5023, total loss: 154680.2036\n",
      "Epoch [2/5], step [54000/476702], loss: 2.0810, total loss: 157340.1795\n",
      "Epoch [2/5], step [55000/476702], loss: 2.4569, total loss: 160087.6134\n",
      "Epoch [2/5], step [56000/476702], loss: 2.7657, total loss: 163220.7937\n",
      "Epoch [2/5], step [57000/476702], loss: 0.5440, total loss: 165778.5812\n",
      "Epoch [2/5], step [58000/476702], loss: 0.0452, total loss: 169084.5472\n",
      "Epoch [2/5], step [59000/476702], loss: 0.2005, total loss: 172034.6880\n",
      "Epoch [2/5], step [60000/476702], loss: 3.8779, total loss: 174907.3912\n",
      "Epoch [2/5], step [61000/476702], loss: 1.2642, total loss: 177652.9104\n",
      "Epoch [2/5], step [62000/476702], loss: 0.2743, total loss: 180359.4670\n",
      "Epoch [2/5], step [63000/476702], loss: 0.7344, total loss: 183306.5031\n",
      "Epoch [2/5], step [64000/476702], loss: 0.1635, total loss: 186239.0843\n",
      "Epoch [2/5], step [65000/476702], loss: 0.2756, total loss: 189135.6986\n",
      "Epoch [2/5], step [66000/476702], loss: 0.1663, total loss: 191804.0855\n",
      "Epoch [2/5], step [67000/476702], loss: 4.2460, total loss: 194632.7645\n",
      "Epoch [2/5], step [68000/476702], loss: 1.0054, total loss: 197363.6604\n",
      "Epoch [2/5], step [69000/476702], loss: 0.4112, total loss: 200455.8141\n",
      "Epoch [2/5], step [70000/476702], loss: 6.7906, total loss: 203549.6594\n",
      "Epoch [2/5], step [71000/476702], loss: 13.9101, total loss: 206555.7019\n",
      "Epoch [2/5], step [72000/476702], loss: 3.3510, total loss: 209483.5675\n",
      "Epoch [2/5], step [73000/476702], loss: 0.4426, total loss: 212432.8926\n",
      "Epoch [2/5], step [74000/476702], loss: 0.3554, total loss: 215290.2982\n",
      "Epoch [2/5], step [75000/476702], loss: 0.1321, total loss: 217947.6297\n",
      "Epoch [2/5], step [76000/476702], loss: 0.5847, total loss: 220667.6362\n",
      "Epoch [2/5], step [77000/476702], loss: 0.0614, total loss: 223760.7407\n",
      "Epoch [2/5], step [78000/476702], loss: 4.2777, total loss: 226583.4481\n",
      "Epoch [2/5], step [79000/476702], loss: 10.6187, total loss: 229528.2581\n",
      "Epoch [2/5], step [80000/476702], loss: 9.3894, total loss: 232557.6340\n",
      "Epoch [2/5], step [81000/476702], loss: 1.8671, total loss: 235449.5040\n",
      "Epoch [2/5], step [82000/476702], loss: 0.2867, total loss: 238234.9990\n",
      "Epoch [2/5], step [83000/476702], loss: 2.0414, total loss: 241122.7586\n",
      "Epoch [2/5], step [84000/476702], loss: 2.3152, total loss: 243855.1676\n",
      "Epoch [2/5], step [85000/476702], loss: 2.4635, total loss: 246588.7086\n",
      "Epoch [2/5], step [86000/476702], loss: 0.0277, total loss: 249376.2582\n",
      "Epoch [2/5], step [87000/476702], loss: 6.0026, total loss: 252383.9342\n",
      "Epoch [2/5], step [88000/476702], loss: 2.9543, total loss: 254797.5719\n",
      "Epoch [2/5], step [89000/476702], loss: 0.4055, total loss: 257022.6163\n",
      "Epoch [2/5], step [90000/476702], loss: 5.3686, total loss: 259730.3717\n",
      "Epoch [2/5], step [91000/476702], loss: 6.5867, total loss: 262325.7116\n",
      "Epoch [2/5], step [92000/476702], loss: 0.5647, total loss: 265060.6543\n",
      "Epoch [2/5], step [93000/476702], loss: 0.0243, total loss: 267593.4085\n",
      "Epoch [2/5], step [94000/476702], loss: 5.4970, total loss: 270378.6654\n",
      "Epoch [2/5], step [95000/476702], loss: 5.3790, total loss: 273309.8973\n",
      "Epoch [2/5], step [96000/476702], loss: 3.5146, total loss: 275949.4879\n",
      "Epoch [2/5], step [97000/476702], loss: 8.0088, total loss: 279011.6630\n",
      "Epoch [2/5], step [98000/476702], loss: 2.8403, total loss: 281846.9057\n",
      "Epoch [2/5], step [99000/476702], loss: 0.8745, total loss: 284427.4881\n",
      "Epoch [2/5], step [100000/476702], loss: 4.1707, total loss: 287266.9957\n",
      "Epoch [2/5], step [101000/476702], loss: 0.3871, total loss: 290001.7972\n",
      "Epoch [2/5], step [102000/476702], loss: 3.7458, total loss: 292953.1715\n",
      "Epoch [2/5], step [103000/476702], loss: 2.2726, total loss: 295846.0187\n",
      "Epoch [2/5], step [104000/476702], loss: 3.3554, total loss: 298510.8574\n",
      "Epoch [2/5], step [105000/476702], loss: 1.5873, total loss: 301571.7088\n",
      "Epoch [2/5], step [106000/476702], loss: 0.9875, total loss: 304980.6650\n",
      "Epoch [2/5], step [107000/476702], loss: 10.7074, total loss: 307906.4943\n",
      "Epoch [2/5], step [108000/476702], loss: 0.0224, total loss: 310649.0894\n",
      "Epoch [2/5], step [109000/476702], loss: 1.3363, total loss: 313132.5192\n",
      "Epoch [2/5], step [110000/476702], loss: 0.7576, total loss: 315897.4185\n",
      "Epoch [2/5], step [111000/476702], loss: 0.1040, total loss: 318451.9977\n",
      "Epoch [2/5], step [112000/476702], loss: 2.9371, total loss: 321092.7973\n",
      "Epoch [2/5], step [113000/476702], loss: 0.5304, total loss: 323802.1539\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "num_epoch = 5\n",
    "step_count = len(data)\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(CONTEXT_SIZE, EMBEDDING_DIM, vocab_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for i, sample in enumerate(data):\n",
    "\n",
    "        context, target = sample\n",
    "        # Prepare the inputs to be passed to the model\n",
    "        context_idxs = make_context_vector(context, word_to_ix)\n",
    "\n",
    "        # Reset grad\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Run forward and get log probabilities over the word that matches the context\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long).to(device))\n",
    "\n",
    "        # Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if((i+1) % 1000 == 0):\n",
    "            print(\n",
    "                        f\"Epoch [{epoch + 1}/{num_epoch}]\"\n",
    "                        f\", step [{i + 1}/{step_count}]\"\n",
    "                        f\", loss: {loss.item():.4f}\"\n",
    "                        f\", total loss: {total_loss:.4f}\"\n",
    "                    )\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "print(losses)  # The loss decreased every iteration over the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model \n",
    "torch.save(model.state_dict(), './models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for test\n",
    "model = CBOW(CONTEXT_SIZE, EMBEDDING_DIM, vocab_size).to(device)\n",
    "model.load_state_dict(torch.load('./models/...'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
